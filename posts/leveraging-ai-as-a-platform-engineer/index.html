<!doctype html><html lang=en><head><title>Leveraging AI as a Platform Engineer: What Actually Works :: Git Push and Run</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="I have spent the last year building AI into the core of my daily engineering work, not as a chat assistant, but as an active participant in systems that move real infrastructure and interact with production APIs. This post is an attempt to write down what I have learned: the patterns that work, the ones that looked good in demos but failed in practice, and the architectural decisions that turned out to matter.
"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://manuelfedele.github.io/posts/leveraging-ai-as-a-platform-engineer/><script async src="https://www.googletagmanager.com/gtag/js?id=G-K9P1DJX238"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-K9P1DJX238")}</script><link rel=stylesheet href=https://manuelfedele.github.io/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://manuelfedele.github.io/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://manuelfedele.github.io/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://manuelfedele.github.io/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://manuelfedele.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://manuelfedele.github.io/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://manuelfedele.github.io/css/main.min.775ac2af004d44c22a6d000fbd1d9af529642f5cef27399d0280d180af2c2e9b.css><link rel=stylesheet href=https://manuelfedele.github.io/css/menu.min.310d32205bdedd6f43144e3c3273c9deecd238eba5f9108db5ea96ca0cfbe377.css><link rel=stylesheet href=https://manuelfedele.github.io/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://manuelfedele.github.io/css/post.min.ad50c7f4d00e7975918f37fc74c6029e1959a40d66fb5b2c6564a8715e985573.css><link rel=stylesheet href=https://manuelfedele.github.io/css/syntax.min.e9ab635cf918bc84b901eb65c0b2caa74c9544245e3647c1af5c129896ef276e.css><link rel=stylesheet href=https://manuelfedele.github.io/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://manuelfedele.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel=stylesheet href=https://manuelfedele.github.io/style.css><link rel="shortcut icon" href=https://manuelfedele.github.io/favicon.png><link rel=apple-touch-icon href=https://manuelfedele.github.io/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Leveraging AI as a Platform Engineer: What Actually Works"><meta property="og:description" content="I have spent the last year building AI into the core of my daily engineering work, not as a chat assistant, but as an active participant in systems that move real infrastructure and interact with production APIs. This post is an attempt to write down what I have learned: the patterns that work, the ones that looked good in demos but failed in practice, and the architectural decisions that turned out to matter.
"><meta property="og:url" content="https://manuelfedele.github.io/posts/leveraging-ai-as-a-platform-engineer/"><meta property="og:site_name" content="Git Push and Run"><meta property="og:image" content="https://manuelfedele.github.io/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2026-03-01 12:00:00 +0100 +0100"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>git push && run</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/about>About</a></li><li><a href=/posts>Blog</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/about>About</a></li><li><a href=/posts>Blog</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://manuelfedele.github.io/posts/leveraging-ai-as-a-platform-engineer/>Leveraging AI as a Platform Engineer: What Actually Works</a></h1><div class=post-meta><time class=post-date>2026-03-01</time><span class=post-reading-time>10 min read (2058 words)</span></div><span class=post-tags>#<a href=https://manuelfedele.github.io/tags/ai/>AI</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/llm/>LLM</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/python/>python</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/aws/>AWS</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/devops/>DevOps</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/automation/>automation</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/architecture/>architecture</a>&nbsp;</span><div class=table-of-contents><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#the-mindset-shift-from-copilot-to-collaborator>The Mindset Shift: From Copilot to Collaborator</a></li><li><a href=#building-the-right-tool-interface>Building the Right Tool Interface</a></li><li><a href=#streaming-is-not-optional>Streaming is Not Optional</a></li><li><a href=#the-llm-gateway-why-you-need-one>The LLM Gateway: Why You Need One</a></li><li><a href=#approval-gates-when-to-keep-humans-in-the-loop>Approval Gates: When to Keep Humans in the Loop</a></li><li><a href=#dry-run-mode-test-without-side-effects>Dry-Run Mode: Test Without Side Effects</a></li><li><a href=#exponential-backoff-is-not-optional>Exponential Backoff Is Not Optional</a></li><li><a href=#context-window-management>Context Window Management</a></li><li><a href=#multi-model-pipelines>Multi-Model Pipelines</a></li><li><a href=#performance-testing-ai-systems>Performance Testing AI Systems</a></li><li><a href=#what-i-wish-i-had-known-earlier>What I Wish I Had Known Earlier</a></li></ul></nav></div><div class=post-content><div><p>I have spent the last year building AI into the core of my daily engineering work, not as a chat assistant, but as an active participant in systems that move real infrastructure and interact with production APIs. This post is an attempt to write down what I have learned: the patterns that work, the ones that looked good in demos but failed in practice, and the architectural decisions that turned out to matter.</p><h2 id=the-mindset-shift-from-copilot-to-collaborator>The Mindset Shift: From Copilot to Collaborator<a href=#the-mindset-shift-from-copilot-to-collaborator class=hanchor arialabel=Anchor>#</a></h2><p>The first and most important shift is conceptual. Most people start with AI as an accelerant for their own work: you write code faster, you draft emails in half the time, you ask questions instead of reading documentation. That is all real and valuable.</p><p>But the more interesting question is: what can AI do autonomously, without you in the loop at all?</p><p>When you start asking that question, the architecture looks completely different. You are no longer building a chat interface. You are building a system that ingests requests, reasons about them, executes tools, handles errors, and produces outcomes, with a human only at the entry and exit points.</p><p>I have built several systems in this space. The pattern that has proven most reliable is what I call the <strong>dispatch-execute loop</strong>:</p><pre tabindex=0><code>Input (ticket / request / event)
  └─&gt; Route to appropriate handler
        └─&gt; Agent reasons about required steps
              └─&gt; Agent calls tools (APIs, CLIs, databases)
                    └─&gt; Output (comment, notification, artifact)
</code></pre><p>The key insight is that the agent does not need to know how to do everything. It needs to know what tools exist, what their inputs and outputs are, and how to sequence them. The actual execution is deterministic code. Only the reasoning is probabilistic.</p><h2 id=building-the-right-tool-interface>Building the Right Tool Interface<a href=#building-the-right-tool-interface class=hanchor arialabel=Anchor>#</a></h2><p>If the agent is the brain, tools are the hands. The quality of your tool interface determines almost everything about agent reliability.</p><p>I learned this the hard way. Early versions of my agents had tools that returned verbose, unstructured text. The agent would parse that text to extract information for the next step. This created a fragile pipeline where LLM parsing errors compounded across steps.</p><p>The fix was obvious in retrospect: tools should return structured data, and the schema should be exactly what the agent needs for the next decision.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=n>dataclass</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Optional</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>RotationResult</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>success</span><span class=p>:</span> <span class=nb>bool</span>
</span></span><span class=line><span class=cl>    <span class=n>new_key_id</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>old_key_id</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>expiry_timestamp</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>error</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
</span></span></code></pre></div><p>When a tool returns a <code>RotationResult</code>, the agent does not need to parse text. It reads <code>.success</code>, checks <code>.error</code>, and decides what to do next. Reliability improved dramatically after this change.</p><p>The second lesson: <strong>tool names and docstrings are part of your prompt</strong>. The agent reads them to decide which tool to call. Vague names like <code>do_thing()</code> or <code>process()</code> produce unpredictable tool selection. Names like <code>rotate_api_key(service_id: str, notify_owner: bool)</code> produce consistent behavior.</p><h2 id=streaming-is-not-optional>Streaming is Not Optional<a href=#streaming-is-not-optional class=hanchor arialabel=Anchor>#</a></h2><p>When an agent is executing a multi-step task that takes 30-90 seconds, silence is alarming. Users assume things are broken. They click again, creating duplicate requests. They abandon the session.</p><p>The solution is streaming updates, but not the naive kind. I do not mean streaming the LLM token output (though that helps too). I mean streaming <strong>tool execution events</strong> as they happen.</p><p>The architecture I settled on uses Server-Sent Events (SSE) from the backend to the frontend:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>async</span> <span class=k>def</span> <span class=nf>stream_agent_execution</span><span class=p>(</span><span class=n>task_id</span><span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>async</span> <span class=k>for</span> <span class=n>event</span> <span class=ow>in</span> <span class=n>agent</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>task_id</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>event</span><span class=o>.</span><span class=n>type</span> <span class=o>==</span> <span class=s2>&#34;tool_start&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>yield</span> <span class=sa>f</span><span class=s2>&#34;data: </span><span class=si>{</span><span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>({</span><span class=s1>&#39;status&#39;</span><span class=p>:</span> <span class=s1>&#39;running&#39;</span><span class=p>,</span> <span class=s1>&#39;tool&#39;</span><span class=p>:</span> <span class=n>event</span><span class=o>.</span><span class=n>tool_name</span><span class=p>})</span><span class=si>}</span><span class=se>\n\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>event</span><span class=o>.</span><span class=n>type</span> <span class=o>==</span> <span class=s2>&#34;tool_end&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>yield</span> <span class=sa>f</span><span class=s2>&#34;data: </span><span class=si>{</span><span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>({</span><span class=s1>&#39;status&#39;</span><span class=p>:</span> <span class=s1>&#39;done&#39;</span><span class=p>,</span> <span class=s1>&#39;tool&#39;</span><span class=p>:</span> <span class=n>event</span><span class=o>.</span><span class=n>tool_name</span><span class=p>,</span> <span class=s1>&#39;result&#39;</span><span class=p>:</span> <span class=n>event</span><span class=o>.</span><span class=n>summary</span><span class=p>})</span><span class=si>}</span><span class=se>\n\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>event</span><span class=o>.</span><span class=n>type</span> <span class=o>==</span> <span class=s2>&#34;message&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>yield</span> <span class=sa>f</span><span class=s2>&#34;data: </span><span class=si>{</span><span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>({</span><span class=s1>&#39;status&#39;</span><span class=p>:</span> <span class=s1>&#39;message&#39;</span><span class=p>,</span> <span class=s1>&#39;content&#39;</span><span class=p>:</span> <span class=n>event</span><span class=o>.</span><span class=n>delta</span><span class=p>})</span><span class=si>}</span><span class=se>\n\n</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>On the frontend, each tool execution becomes a visible step. The user sees &ldquo;checking permissions&mldr; creating resource&mldr; sending notification&mldr;&rdquo; in real time. This changes the user experience completely. People are patient when they can see progress.</p><p>One gotcha: do not use <code>axios</code> or similar libraries for SSE consumption on the client. They buffer responses. Use the native <code>fetch</code> API with <code>ReadableStream</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-typescript data-lang=typescript><span class=line><span class=cl><span class=kr>const</span> <span class=nx>response</span> <span class=o>=</span> <span class=k>await</span> <span class=nx>fetch</span><span class=p>(</span><span class=s1>&#39;/api/agent/stream&#39;</span><span class=p>,</span> <span class=p>{</span> <span class=nx>method</span><span class=o>:</span> <span class=s1>&#39;POST&#39;</span><span class=p>,</span> <span class=nx>body</span>: <span class=kt>payload</span> <span class=p>});</span>
</span></span><span class=line><span class=cl><span class=kr>const</span> <span class=nx>reader</span> <span class=o>=</span> <span class=nx>response</span><span class=p>.</span><span class=nx>body</span><span class=o>!</span><span class=p>.</span><span class=nx>getReader</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=kr>const</span> <span class=nx>decoder</span> <span class=o>=</span> <span class=k>new</span> <span class=nx>TextDecoder</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>while</span> <span class=p>(</span><span class=kc>true</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=p>{</span> <span class=nx>done</span><span class=p>,</span> <span class=nx>value</span> <span class=p>}</span> <span class=o>=</span> <span class=k>await</span> <span class=nx>reader</span><span class=p>.</span><span class=nx>read</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=nx>done</span><span class=p>)</span> <span class=k>break</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>chunk</span> <span class=o>=</span> <span class=nx>decoder</span><span class=p>.</span><span class=nx>decode</span><span class=p>(</span><span class=nx>value</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=c1>// parse SSE events and update UI state
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><h2 id=the-llm-gateway-why-you-need-one>The LLM Gateway: Why You Need One<a href=#the-llm-gateway-why-you-need-one class=hanchor arialabel=Anchor>#</a></h2><p>If you are running more than one AI-powered service, you need a centralized LLM gateway. This is not optional at scale. Without one, you have:</p><ul><li>API keys scattered across services</li><li>No visibility into who is calling what model with what prompts</li><li>No rate limiting, so one runaway process can burn your quota</li><li>No cost attribution</li></ul><p>A gateway is an HTTP proxy that sits between your services and the model provider. All requests go through it. The gateway handles authentication (your services authenticate to the gateway, the gateway authenticates to the upstream), enforces per-service quotas, logs every request with token counts, and exposes a unified API regardless of the underlying model.</p><pre tabindex=0><code>Service A ──┐
Service B ──┤──&gt; LLM Gateway ──&gt; AWS Bedrock / OpenAI / Azure OpenAI
Service C ──┘         │
                  Auth, rate limiting,
                  logging, cost tracking
</code></pre><p>I built mine using ECS Fargate with Aurora Serverless v2 for persistence. The gateway issues OAuth2 access tokens to services via Keycloak. Each service has a client ID, a rate limit in tokens-per-minute, and a model allowlist. When a service calls the gateway, the gateway validates the token, checks the quota, forwards the request, and records the usage.</p><p>The operational benefit is enormous. When a new model comes out, you update the gateway, not every service. When a service is misbehaving (token spikes at 3am), you see it immediately in the logs. When finance asks how much AI is costing, you can answer by service, by model, by day.</p><h2 id=approval-gates-when-to-keep-humans-in-the-loop>Approval Gates: When to Keep Humans in the Loop<a href=#approval-gates-when-to-keep-humans-in-the-loop class=hanchor arialabel=Anchor>#</a></h2><p>The hardest architectural decision in autonomous agent design is not technical. It is: what operations require human approval?</p><p>I have a simple heuristic: if the operation is reversible and narrow in scope, let the agent do it. If it is irreversible or has broad blast radius, require approval.</p><p>In practice, this translates to an approval flow for destructive operations. The agent identifies that approval is needed, sends a notification (email, ticket comment), and suspends. A human approves or rejects. The agent resumes.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>requires_approval</span><span class=p>(</span><span class=n>operation</span><span class=p>:</span> <span class=n>Operation</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>bool</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>HIGH_RISK</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>OperationType</span><span class=o>.</span><span class=n>DELETE_RESOURCE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>OperationType</span><span class=o>.</span><span class=n>ROTATE_PRODUCTION_KEY</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>OperationType</span><span class=o>.</span><span class=n>MODIFY_IAM_POLICY</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>operation</span><span class=o>.</span><span class=n>type</span> <span class=ow>in</span> <span class=n>HIGH_RISK</span> <span class=ow>or</span> <span class=n>operation</span><span class=o>.</span><span class=n>affects_production</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>async</span> <span class=k>def</span> <span class=nf>execute_with_gate</span><span class=p>(</span><span class=n>agent</span><span class=p>,</span> <span class=n>operation</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>requires_approval</span><span class=p>(</span><span class=n>operation</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>approval_id</span> <span class=o>=</span> <span class=k>await</span> <span class=n>send_approval_request</span><span class=p>(</span><span class=n>operation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>await</span> <span class=n>wait_for_approval</span><span class=p>(</span><span class=n>approval_id</span><span class=p>)</span>  <span class=c1># polls or uses webhook</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=k>await</span> <span class=n>agent</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span><span class=n>operation</span><span class=p>)</span>
</span></span></code></pre></div><p>This pattern also gives you an audit trail. Every approval is a record: who approved, when, for what operation. In regulated environments, this is table stakes.</p><h2 id=dry-run-mode-test-without-side-effects>Dry-Run Mode: Test Without Side Effects<a href=#dry-run-mode-test-without-side-effects class=hanchor arialabel=Anchor>#</a></h2><p>Every autonomous agent I build has a dry-run mode. In dry-run, the agent reasons through the full plan but does not execute any mutations. It describes what it would do in a structured format that a human can review.</p><p>This is implemented at the system prompt level:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>build_system_prompt</span><span class=p>(</span><span class=n>dry_run</span><span class=p>:</span> <span class=nb>bool</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>mode_instruction</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;You are in PLAN mode. Describe what actions you would take, &#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;but do not call any tools that create, modify, or delete resources.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>dry_run</span> <span class=k>else</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;You are in EXECUTE mode. You may call all available tools.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>BASE_PROMPT</span><span class=si>}</span><span class=se>\n\n</span><span class=si>{</span><span class=n>mode_instruction</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>Dry-run mode is invaluable for:</p><ul><li>Onboarding users who are not yet confident in the agent</li><li>Testing changes to agent behavior without side effects</li><li>Debugging unexpected agent decisions (&ldquo;why would it have done that?&rdquo;)</li><li>Pre-flight checks before running in production</li></ul><h2 id=exponential-backoff-is-not-optional>Exponential Backoff Is Not Optional<a href=#exponential-backoff-is-not-optional class=hanchor arialabel=Anchor>#</a></h2><p>LLM APIs are rate-limited. External APIs are rate-limited. Everything you call from an agent is rate-limited. If your agent makes parallel tool calls without backoff, it will fail under load in ways that are hard to debug.</p><p>The pattern I use everywhere:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>asyncio</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>TypeVar</span><span class=p>,</span> <span class=n>Callable</span><span class=p>,</span> <span class=n>Awaitable</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>T</span> <span class=o>=</span> <span class=n>TypeVar</span><span class=p>(</span><span class=s2>&#34;T&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>async</span> <span class=k>def</span> <span class=nf>retry_with_backoff</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>fn</span><span class=p>:</span> <span class=n>Callable</span><span class=p>[[],</span> <span class=n>Awaitable</span><span class=p>[</span><span class=n>T</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>    <span class=n>max_retries</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>base_delay</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_delay</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>30.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>T</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>attempt</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_retries</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=k>await</span> <span class=n>fn</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>except</span> <span class=n>RateLimitError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>attempt</span> <span class=o>==</span> <span class=n>max_retries</span> <span class=o>-</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>raise</span>
</span></span><span class=line><span class=cl>            <span class=n>delay</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=n>base_delay</span> <span class=o>*</span> <span class=p>(</span><span class=mi>2</span> <span class=o>**</span> <span class=n>attempt</span><span class=p>)</span> <span class=o>+</span> <span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>max_delay</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>await</span> <span class=n>asyncio</span><span class=o>.</span><span class=n>sleep</span><span class=p>(</span><span class=n>delay</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=s2>&#34;unreachable&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>The jitter (<code>random.uniform(0, 1)</code>) is important. Without it, all parallel requests back off to the same point and produce a thundering herd when they retry together.</p><h2 id=context-window-management>Context Window Management<a href=#context-window-management class=hanchor arialabel=Anchor>#</a></h2><p>LLMs have a finite context window. In long-running agent sessions with many tool calls, you will hit it. When you do, the agent starts forgetting the beginning of the conversation, including the original task, and produces increasingly incoherent behavior.</p><p>There are several strategies:</p><p><strong>Stateless agents with external memory.</strong> Each invocation gets only what it needs: the current task description, the relevant context retrieved from a database, and the tools. No conversation history. This is the most scalable approach.</p><p><strong>Summarization.</strong> Periodically summarize the conversation and replace the history with the summary. The agent loses granular detail but retains the important context.</p><p><strong>Tool-based memory.</strong> Give the agent a <code>remember(key, value)</code> and <code>recall(key)</code> tool. It manages its own persistent state. This is elegant but requires the agent to be disciplined about what it chooses to store.</p><p>I use the stateless approach for production workloads. The agent receives a task ID, fetches the task state from the database, executes a step, writes the result back, and terminates. The next invocation picks up from where it left off. No context window concerns.</p><h2 id=multi-model-pipelines>Multi-Model Pipelines<a href=#multi-model-pipelines class=hanchor arialabel=Anchor>#</a></h2><p>Not every task needs the most capable (and expensive) model. A pipeline that runs a large model for every classification, extraction, and summarization step will be 10x more expensive than one that routes intelligently.</p><p>The pattern:</p><pre tabindex=0><code>Input
  └─&gt; Classifier (small/fast model)
        ├─&gt; Simple extraction → Cheap model
        ├─&gt; Complex reasoning → Capable model
        └─&gt; Creative generation → Specialized model
</code></pre><p>For document processing pipelines, I route based on document type and complexity score. Short, structured inputs go to cheaper models. Long, ambiguous inputs with high stakes go to the most capable model available.</p><p>The gateway makes this easy: the service declares the task type, and the gateway applies the routing policy. Services do not need to know which model they are actually calling.</p><h2 id=performance-testing-ai-systems>Performance Testing AI Systems<a href=#performance-testing-ai-systems class=hanchor arialabel=Anchor>#</a></h2><p>Standard load testing tools are not well-suited for LLM workloads. The response time is variable by design, highly dependent on output length, and the interesting failure modes (model errors, context window overflow, rate limit cascades) do not show up in simple throughput tests.</p><p>What I test instead:</p><ul><li><strong>Token throughput</strong> (input + output tokens per second at various concurrency levels)</li><li><strong>Time to first token</strong> (TTFT), which matters for streaming UX</li><li><strong>Error rate by error type</strong> (rate limit vs model error vs timeout are operationally different)</li><li><strong>Cost per operation</strong> at various load levels</li></ul><p>For load generation I use a custom harness that replays a representative sample of real production requests rather than synthetic inputs. Real requests have the right token distribution and exercise real failure modes.</p><h2 id=what-i-wish-i-had-known-earlier>What I Wish I Had Known Earlier<a href=#what-i-wish-i-had-known-earlier class=hanchor arialabel=Anchor>#</a></h2><p>A few things I learned later than I should have:</p><p><strong>Do not prompt-engineer yourself into a corner.</strong> Long, brittle system prompts that encode business logic are hard to maintain and hard to test. Push logic into tools and code. The prompt should describe behavior, not implement it.</p><p><strong>Log everything.</strong> Every tool call, every model response, every token count. You will need it when something goes wrong, and something will go wrong. Storage is cheap; debugging without logs is not.</p><p><strong>Start with the smallest scope that proves the value.</strong> The first agent I built tried to handle every possible request type. It was complex, untestable, and failed in confusing ways. The second version handled one request type perfectly. From there, expansion was incremental and each addition was testable in isolation.</p><p><strong>The AI is not the hard part.</strong> The hard part is the integrations, the approval flows, the error handling, the observability. The model call is two lines of code. Everything around it is the engineering challenge.</p><hr><p>This is an evolving area. The patterns I use today will look dated in a year. But the underlying principles (structured tool interfaces, streaming for UX, approval gates for risky operations, observability for everything) feel stable. They are good engineering, with or without AI.</p><p>If you are working on similar problems or want to discuss any of this, reach out at <a href=mailto:manuel.fedele+website@gmail.com>manuel.fedele+website@gmail.com</a>.</p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://manuelfedele.github.io/posts/wasm-vert-cloudfront-zero-cost/ class="button inline next">[<span class=button__text>WASM in the Browser: Deploying VERT on CloudFront for Free</span>] ></a></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2026 Powered by <a href=https://gohugo.io>Hugo</a></span>
<span>:: <a href=https://github.com/panr/hugo-theme-terminal target=_blank>Theme</a> made by <a href=https://github.com/panr target=_blank>panr</a></span></div></div></footer><script type=text/javascript src=/bundle.min.js></script></div></body></html>