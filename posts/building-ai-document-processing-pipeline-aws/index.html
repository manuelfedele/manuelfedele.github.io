<!doctype html><html lang=en><head><title>Building an AI-Powered Document Processing Pipeline on AWS :: Git Push and Run</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Building an AI-Powered Document Processing Pipeline on AWS Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.
In this post I&rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.
"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/><script async src="https://www.googletagmanager.com/gtag/js?id=G-K9P1DJX238"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-K9P1DJX238")}</script><link rel=stylesheet href=https://manuelfedele.github.io/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://manuelfedele.github.io/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://manuelfedele.github.io/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://manuelfedele.github.io/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://manuelfedele.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://manuelfedele.github.io/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://manuelfedele.github.io/css/main.min.775ac2af004d44c22a6d000fbd1d9af529642f5cef27399d0280d180af2c2e9b.css><link rel=stylesheet href=https://manuelfedele.github.io/css/menu.min.310d32205bdedd6f43144e3c3273c9deecd238eba5f9108db5ea96ca0cfbe377.css><link rel=stylesheet href=https://manuelfedele.github.io/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://manuelfedele.github.io/css/post.min.ad50c7f4d00e7975918f37fc74c6029e1959a40d66fb5b2c6564a8715e985573.css><link rel=stylesheet href=https://manuelfedele.github.io/css/syntax.min.e9ab635cf918bc84b901eb65c0b2caa74c9544245e3647c1af5c129896ef276e.css><link rel=stylesheet href=https://manuelfedele.github.io/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://manuelfedele.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel="shortcut icon" href=https://manuelfedele.github.io/favicon.png><link rel=apple-touch-icon href=https://manuelfedele.github.io/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Building an AI-Powered Document Processing Pipeline on AWS"><meta property="og:description" content="Building an AI-Powered Document Processing Pipeline on AWS Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.
In this post I&rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.
"><meta property="og:url" content="https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/"><meta property="og:site_name" content="Git Push and Run"><meta property="og:image" content="https://manuelfedele.github.io/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2025-12-03 16:45:00 +0100 +0100"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>git push && run</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/about>About</a></li><li><a href=/posts>Blog</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/about>About</a></li><li><a href=/posts>Blog</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/>Building an AI-Powered Document Processing Pipeline on AWS</a></h1><div class=post-meta><time class=post-date>2025-12-03</time><span class=post-reading-time>10 min read (2120 words)</span></div><span class=post-tags>#<a href=https://manuelfedele.github.io/tags/aws/>AWS</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/ai/>AI</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/llm/>LLM</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/step-functions/>Step Functions</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/lambda/>Lambda</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/serverless/>serverless</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/python/>python</a>&nbsp;
#<a href=https://manuelfedele.github.io/tags/architecture/>architecture</a>&nbsp;</span><div class=table-of-contents><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#the-problem-space>The Problem Space</a></li><li><a href=#architecture-overview>Architecture Overview</a></li><li><a href=#the-ingestion-flow>The Ingestion Flow</a></li><li><a href=#the-step-functions-pipeline>The Step Functions Pipeline</a><ul><li><a href=#linear-phase-parse-and-extract-text>Linear Phase: Parse and Extract Text</a></li><li><a href=#upper-branch-embedding-pipeline>Upper Branch: Embedding Pipeline</a></li><li><a href=#lower-branch-classification-and-extraction-pipeline>Lower Branch: Classification and Extraction Pipeline</a></li></ul></li><li><a href=#the-multi-model-llm-strategy>The Multi-Model LLM Strategy</a></li><li><a href=#concurrency-and-error-handling>Concurrency and Error Handling</a></li><li><a href=#the-claims-service>The Claims Service</a></li><li><a href=#performance-and-cost>Performance and Cost</a></li><li><a href=#lessons-learned>Lessons Learned</a></li><li><a href=#whats-next>What&rsquo;s Next</a></li><li><a href=#references>References</a></li></ul></nav></div><div class=post-content><div><h1 id=building-an-ai-powered-document-processing-pipeline-on-aws>Building an AI-Powered Document Processing Pipeline on AWS<a href=#building-an-ai-powered-document-processing-pipeline-on-aws class=hanchor arialabel=Anchor>#</a></h1><p>Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.</p><p>In this post I&rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.</p><h2 id=the-problem-space>The Problem Space<a href=#the-problem-space class=hanchor arialabel=Anchor>#</a></h2><p>A typical insurance claim arrives as a ZIP archive containing multiple documents: a police report, medical certificates, repair estimates, photos, correspondence. Each document needs to be:</p><ol><li><strong>Parsed</strong> into machine-readable text (many are scanned PDFs or photos)</li><li><strong>Classified</strong> by type (police report, medical report, invoice, etc.)</li><li><strong>Structured</strong> by extracting specific fields per document type (e.g., license plate from a police report, diagnosis code from a medical report)</li><li><strong>Indexed</strong> for semantic search so case managers can query across all documents in natural language</li><li><strong>Summarized</strong> so the case manager gets an overview without reading every page</li></ol><p>The system needs to handle multiple lines of business (motor, health, accident), support concurrent processing of hundreds of claims, and recover gracefully from LLM failures.</p><h2 id=architecture-overview>Architecture Overview<a href=#architecture-overview class=hanchor arialabel=Anchor>#</a></h2><p>The architecture follows an event-driven, fully serverless pattern:</p><pre tabindex=0><code>ZIP Upload → S3 → EventBridge → Step Functions → [Lambda Pipeline] → DynamoDB + Aurora
                                                                          ↓
                                              Claims Service (ECS) ← ALB ← Users
</code></pre><p>The key components:</p><ul><li><strong>S3 Ingestion Bucket</strong>: receives ZIP uploads via presigned URLs</li><li><strong>EventBridge Rule</strong>: triggers the processing pipeline when a file lands</li><li><strong>Step Functions</strong>: orchestrates the multi-stage processing workflow</li><li><strong>Lambda Functions</strong>: execute each processing stage (stateless, parallel)</li><li><strong>DynamoDB</strong>: tracks processing state and stores extraction results</li><li><strong>Aurora PostgreSQL</strong>: stores vector embeddings for RAG</li><li><strong>ECS Service (Django)</strong>: serves the web UI and API for case managers</li><li><strong>Keycloak + Entra ID</strong>: authentication and group-based access control</li></ul><h2 id=the-ingestion-flow>The Ingestion Flow<a href=#the-ingestion-flow class=hanchor arialabel=Anchor>#</a></h2><p>Documents enter the system through a secure upload flow:</p><ol><li>The external claims management system calls our API to register a claim (line of business, permissions, metadata)</li><li>It requests a presigned S3 URL from a lightweight Lambda function</li><li>It uploads the ZIP archive directly to S3 using the presigned URL</li><li>An EventBridge rule detects the new object and triggers the Step Functions workflow</li></ol><p>The presigned URL approach avoids routing large files through our API. The Lambda generates short-lived URLs (10-60 seconds), so the upload window is narrow enough to prevent misuse.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>boto3</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datetime</span> <span class=kn>import</span> <span class=n>datetime</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>handler</span><span class=p>(</span><span class=n>event</span><span class=p>,</span> <span class=n>context</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>s3</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>client</span><span class=p>(</span><span class=s2>&#34;s3&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>claim_name</span> <span class=o>=</span> <span class=n>event</span><span class=p>[</span><span class=s2>&#34;claim_name&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>document_code</span> <span class=o>=</span> <span class=n>event</span><span class=p>[</span><span class=s2>&#34;document_code&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>key</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;landing/</span><span class=si>{</span><span class=n>claim_name</span><span class=si>}</span><span class=s2>#</span><span class=si>{</span><span class=n>document_code</span><span class=si>}</span><span class=s2>.zip&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>url</span> <span class=o>=</span> <span class=n>s3</span><span class=o>.</span><span class=n>generate_presigned_url</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;put_object&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>Params</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;Bucket&#34;</span><span class=p>:</span> <span class=n>INGESTION_BUCKET</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;Key&#34;</span><span class=p>:</span> <span class=n>key</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;ContentType&#34;</span><span class=p>:</span> <span class=s2>&#34;application/zip&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=n>ExpiresIn</span><span class=o>=</span><span class=mi>60</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;upload_url&#34;</span><span class=p>:</span> <span class=n>url</span><span class=p>,</span> <span class=s2>&#34;key&#34;</span><span class=p>:</span> <span class=n>key</span><span class=p>}</span>
</span></span></code></pre></div><p>Processing runs in periodic batches (every 10 minutes) rather than on each individual upload. This is a deliberate tradeoff: we accept a few minutes of latency in exchange for better throughput management and cost control when handling bursts of uploads.</p><h2 id=the-step-functions-pipeline>The Step Functions Pipeline<a href=#the-step-functions-pipeline class=hanchor arialabel=Anchor>#</a></h2><p>The core of the system is an AWS Step Functions state machine that orchestrates document processing through a linear phase followed by two parallel branches.</p><h3 id=linear-phase-parse-and-extract-text>Linear Phase: Parse and Extract Text<a href=#linear-phase-parse-and-extract-text class=hanchor arialabel=Anchor>#</a></h3><p><strong>Stage 1: Start (claim-level)</strong></p><p>The first Lambda validates the uploaded ZIP, extracts files, uploads individual documents to a support bucket, and creates tracking records in DynamoDB. It also deduplicates: if a document with the same name already exists in the claim and is being processed, it&rsquo;s skipped.</p><p><strong>Stage 2: Split (document-level)</strong></p><p>Each document is converted to PDF (if it&rsquo;s a DOCX), then split into individual pages saved as PNG images. We use an internal PDF splitting service for this, which handles edge cases like encrypted PDFs, malformed page trees, and oversized documents.</p><p><strong>Stage 3: Parse (page-level)</strong></p><p>This is where the AI kicks in. Each page image is sent to <strong>Claude 3.5 Sonnet</strong> (via Amazon Bedrock) for vision-based text extraction. The LLM reads the image and produces clean Markdown text, handling handwritten notes, stamps, tables, and mixed layouts that traditional OCR tools struggle with.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>base64</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>boto3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>parse_page</span><span class=p>(</span><span class=n>image_bytes</span><span class=p>:</span> <span class=nb>bytes</span><span class=p>,</span> <span class=n>page_number</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>bedrock</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>client</span><span class=p>(</span><span class=s2>&#34;bedrock-runtime&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>bedrock</span><span class=o>.</span><span class=n>invoke_model</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>modelId</span><span class=o>=</span><span class=s2>&#34;anthropic.claude-3-5-sonnet-20240620-v1:0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>body</span><span class=o>=</span><span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>({</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;anthropic_version&#34;</span><span class=p>:</span> <span class=s2>&#34;bedrock-2023-05-31&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;max_tokens&#34;</span><span class=p>:</span> <span class=mi>4096</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;messages&#34;</span><span class=p>:</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                    <span class=p>{</span>
</span></span><span class=line><span class=cl>                        <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;image&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=s2>&#34;source&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                            <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;base64&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=s2>&#34;media_type&#34;</span><span class=p>:</span> <span class=s2>&#34;image/png&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=s2>&#34;data&#34;</span><span class=p>:</span> <span class=n>base64</span><span class=o>.</span><span class=n>b64encode</span><span class=p>(</span><span class=n>image_bytes</span><span class=p>)</span><span class=o>.</span><span class=n>decode</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                        <span class=p>},</span>
</span></span><span class=line><span class=cl>                    <span class=p>},</span>
</span></span><span class=line><span class=cl>                    <span class=p>{</span>
</span></span><span class=line><span class=cl>                        <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;Extract all text from this document page. &#34;</span>
</span></span><span class=line><span class=cl>                                <span class=s2>&#34;Preserve the structure using Markdown formatting. &#34;</span>
</span></span><span class=line><span class=cl>                                <span class=s2>&#34;Include tables, headers, and any handwritten text.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=p>},</span>
</span></span><span class=line><span class=cl>                <span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=p>}],</span>
</span></span><span class=line><span class=cl>        <span class=p>}),</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>result</span> <span class=o>=</span> <span class=n>json</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>response</span><span class=p>[</span><span class=s2>&#34;body&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>read</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>result</span><span class=p>[</span><span class=s2>&#34;content&#34;</span><span class=p>][</span><span class=mi>0</span><span class=p>][</span><span class=s2>&#34;text&#34;</span><span class=p>]</span>
</span></span></code></pre></div><p>Why Claude 3.5 Sonnet for OCR instead of Amazon Textract? Two reasons: (1) vision LLMs handle messy real-world documents (stamps, handwriting, mixed layouts) significantly better than traditional OCR, and (2) the output is already structured Markdown, which downstream stages can work with directly without an intermediate parsing step.</p><p><strong>Stage 4: Page Checker (document-level)</strong></p><p>LLM calls fail. Rate limits, timeouts, transient errors. The Page Checker implements retry logic: it collects results from all page parsing calls, identifies failures, and re-dispatches failed pages up to 3 times. This is essential when processing documents with dozens of pages, where even a 1% failure rate means most documents would have at least one failed page.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>check_pages</span><span class=p>(</span><span class=n>document_id</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>page_results</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>dict</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>successful</span> <span class=o>=</span> <span class=p>[</span><span class=n>p</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>page_results</span> <span class=k>if</span> <span class=n>p</span><span class=p>[</span><span class=s2>&#34;status&#34;</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;success&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>failed</span> <span class=o>=</span> <span class=p>[</span><span class=n>p</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>page_results</span> <span class=k>if</span> <span class=n>p</span><span class=p>[</span><span class=s2>&#34;status&#34;</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;error&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>retryable</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>p</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>failed</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;retry_count&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>MAX_RETRIES</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>retryable</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Re-dispatch failed pages for another attempt</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>page</span> <span class=ow>in</span> <span class=n>retryable</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>page</span><span class=p>[</span><span class=s2>&#34;retry_count&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>page</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;retry_count&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;retry&#34;</span><span class=p>,</span> <span class=s2>&#34;pages&#34;</span><span class=p>:</span> <span class=n>retryable</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=n>successful</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;error&#34;</span><span class=p>,</span> <span class=s2>&#34;message&#34;</span><span class=p>:</span> <span class=s2>&#34;All pages failed parsing&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Proceed with whatever pages succeeded</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;success&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;parsed_pages&#34;</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>successful</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;failed_pages&#34;</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>failed</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><h3 id=upper-branch-embedding-pipeline>Upper Branch: Embedding Pipeline<a href=#upper-branch-embedding-pipeline class=hanchor arialabel=Anchor>#</a></h3><p>After text extraction, the pipeline forks into two parallel branches. The upper branch creates vector embeddings for semantic search.</p><p><strong>Chunker</strong>: Combines all page texts into a single document, then splits it into overlapping chunks with metadata (page numbers, positions). Each chunk is saved to S3.</p><p><strong>Embedder</strong>: Each chunk is embedded using <strong>OpenAI&rsquo;s text-embedding-ada-002</strong> model. We chose ada-002 for its good balance of quality and cost at scale. Embeddings are stored in Aurora PostgreSQL using the <code>pgvector</code> extension, enabling similarity search across all documents in a claim.</p><p>This powers a RAG (Retrieval-Augmented Generation) interface where case managers can ask questions like &ldquo;What was the estimated repair cost?&rdquo; and get answers grounded in the actual documents.</p><h3 id=lower-branch-classification-and-extraction-pipeline>Lower Branch: Classification and Extraction Pipeline<a href=#lower-branch-classification-and-extraction-pipeline class=hanchor arialabel=Anchor>#</a></h3><p>The lower branch handles structured information extraction.</p><p><strong>Clusterization</strong>: A single insurance document PDF often contains multiple logical sections (a police report followed by a medical certificate followed by repair photos). The clusterization stage identifies contiguous page ranges that belong to the same topic. We use <strong>Gemini 2.0 Flash</strong> for this because it&rsquo;s fast, cheap, and performs well on the classification-style reasoning this step requires.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>clusterize_document</span><span class=p>(</span><span class=n>pages</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>dict</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=nb>dict</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Identify clusters of pages about the same topic.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Build context from all pages</span>
</span></span><span class=line><span class=cl>    <span class=n>page_summaries</span> <span class=o>=</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=sa>f</span><span class=s2>&#34;Page </span><span class=si>{</span><span class=n>p</span><span class=p>[</span><span class=s1>&#39;number&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>p</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>][:</span><span class=mi>200</span><span class=p>]</span><span class=si>}</span><span class=s2>...&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>pages</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;&#34;&#34;Analyze these document pages and identify clusters
</span></span></span><span class=line><span class=cl><span class=s2>    of consecutive pages that discuss the same topic.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Pages:
</span></span></span><span class=line><span class=cl><span class=s2>    </span><span class=si>{</span><span class=n>page_summaries</span><span class=si>}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Return a JSON array of clusters, each with:
</span></span></span><span class=line><span class=cl><span class=s2>    - start_page: first page number
</span></span></span><span class=line><span class=cl><span class=s2>    - end_page: last page number
</span></span></span><span class=line><span class=cl><span class=s2>    - topic: brief description of what this section covers
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Call Gemini 2.0 Flash via the LLM Gateway</span>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>llm_gateway</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>provider</span><span class=o>=</span><span class=s2>&#34;vertex&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>=</span><span class=s2>&#34;gemini-2.0-flash&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>prompt</span><span class=o>=</span><span class=n>prompt</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>json</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Classification</strong>: Each cluster is labeled with a document type (police report, medical certificate, invoice, repair estimate, etc.). The classification model uses the cluster&rsquo;s text content and the claim&rsquo;s line of business to assign the most appropriate label.</p><p><strong>Extraction</strong>: This is where it gets domain-specific. Based on the classification label, the extraction stage applies a tailored extraction strategy. A police report gets license plate, driver name, date, and location extracted. A medical report gets diagnosis, treatment, and provider extracted. An invoice gets line items and totals extracted.</p><p>The results are stored as structured JSON in DynamoDB, making them queryable and displayable in the web UI.</p><p><strong>Cluster Aggregator</strong>: Collects results from all clusters, validates completeness, and updates the document&rsquo;s processing status.</p><h2 id=the-multi-model-llm-strategy>The Multi-Model LLM Strategy<a href=#the-multi-model-llm-strategy class=hanchor arialabel=Anchor>#</a></h2><p>One of the most interesting architectural decisions was using three different LLM providers, each selected for a specific task:</p><table><thead><tr><th>Task</th><th>Model</th><th>Why</th></tr></thead><tbody><tr><td>Text extraction (OCR)</td><td>Claude 3.5 Sonnet (Bedrock)</td><td>Best vision capabilities for messy documents</td></tr><tr><td>Embeddings</td><td>text-embedding-ada-002 (OpenAI)</td><td>Cost-effective, high-quality embeddings at scale</td></tr><tr><td>Clusterization/Classification</td><td>Gemini 2.0 Flash (Vertex AI)</td><td>Fast and cheap for reasoning tasks</td></tr></tbody></table><p>All LLM calls go through an internal gateway service that abstracts the provider differences. The gateway handles authentication, rate limiting, usage tracking, and fallback logic. From the pipeline&rsquo;s perspective, it&rsquo;s just calling an API with a provider and model name.</p><p>This multi-model approach lets us optimize for cost and quality per task rather than being locked into a single provider. The text extraction stage is the most expensive (vision + large context), so we use the best model available. The clusterization stage processes much less data and needs speed more than depth, so we use a fast, cheap model.</p><h2 id=concurrency-and-error-handling>Concurrency and Error Handling<a href=#concurrency-and-error-handling class=hanchor arialabel=Anchor>#</a></h2><p>The Step Functions state machine uses Map states to process documents and pages in parallel. A single claim might contain 20 documents, each with 50 pages, resulting in 1,000 parallel page-processing Lambda invocations.</p><p>Key patterns:</p><p><strong>Idempotency</strong>: Every Lambda function is idempotent. If a Step Functions execution is retried (due to a transient error), re-processing the same input produces the same result without side effects. We use DynamoDB conditional writes to prevent duplicate processing.</p><p><strong>Graceful degradation</strong>: If the embedding branch fails for a document, the classification branch still completes (and vice versa). A document with failed embeddings can still have its structured data extracted. The system tracks partial success at every level.</p><p><strong>Correlation tracking</strong>: Every request gets a correlation ID that flows through all Lambda invocations, S3 objects, and DynamoDB records. When something fails, you can trace the entire processing chain from upload to the specific failed step.</p><h2 id=the-claims-service>The Claims Service<a href=#the-claims-service class=hanchor arialabel=Anchor>#</a></h2><p>The web UI is a Django application running on ECS Fargate, integrated as a microfrontend into the larger analytics platform. Case managers can:</p><ul><li>Browse claims and their documents</li><li>View extracted text alongside the original document images</li><li>See structured extraction results (fields, values, confidence)</li><li>Search across all documents using natural language (RAG)</li><li>Generate claim summaries on demand</li><li>Track document processing status in real time</li></ul><p>Authentication uses Keycloak with JWT tokens. Authorization uses Entra ID groups: each claim is associated with a visibility group, and only members of that group can access the claim&rsquo;s documents. A daily sync job keeps group membership current.</p><h2 id=performance-and-cost>Performance and Cost<a href=#performance-and-cost class=hanchor arialabel=Anchor>#</a></h2><p>Some numbers from production:</p><ul><li><strong>Processing time</strong>: a 30-page document takes approximately 3-5 minutes end-to-end (dominated by LLM call latency)</li><li><strong>Throughput</strong>: the system handles 500+ documents per hour during peak periods</li><li><strong>Cost per document</strong>: roughly EUR 0.15-0.30, depending on page count and complexity (the bulk of the cost is LLM inference)</li><li><strong>Infrastructure cost when idle</strong>: near zero (serverless)</li></ul><p>The batch processing approach (every 10 minutes) means we can process multiple documents from the same claim together, which is more efficient than processing each upload individually.</p><h2 id=lessons-learned>Lessons Learned<a href=#lessons-learned class=hanchor arialabel=Anchor>#</a></h2><p><strong>Vision LLMs are production-ready for OCR.</strong> Claude 3.5 Sonnet handles real-world insurance documents (stamps, handwriting, poor scans, mixed languages) far better than traditional OCR. The quality improvement justified the higher per-page cost.</p><p><strong>Step Functions are the right tool for document pipelines.</strong> The built-in retry logic, parallel Map states, error handling, and visual debugging make Step Functions ideal for multi-stage document processing. We tried orchestrating with SQS queues initially, but the complexity of tracking state across stages was not worth it.</p><p><strong>Multi-model is the way forward.</strong> No single LLM is best at everything. Using Claude for vision, OpenAI for embeddings, and Gemini for fast classification gave us the best cost-quality tradeoff at each stage.</p><p><strong>Retry logic is not optional.</strong> LLM APIs fail more often than traditional APIs. Rate limits, timeouts, model overload. The Page Checker retry pattern (up to 3 attempts per page) is what makes the pipeline reliable enough for production.</p><p><strong>Batch over real-time when you can.</strong> Processing uploads every 10 minutes instead of immediately simplified the architecture significantly and reduced costs. For insurance claims processing, a few minutes of latency is perfectly acceptable.</p><h2 id=whats-next>What&rsquo;s Next<a href=#whats-next class=hanchor arialabel=Anchor>#</a></h2><p>The natural evolution is closing the loop: having the AI agent not just extract and classify documents, but also suggest claim decisions based on the extracted data and historical patterns. This moves from &ldquo;AI assists the human&rdquo; to &ldquo;AI proposes, human approves,&rdquo; which is the next frontier for insurance automation.</p><h2 id=references>References<a href=#references class=hanchor arialabel=Anchor>#</a></h2><ul><li><a href=https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html>AWS Step Functions</a></li><li><a href=https://docs.anthropic.com/en/docs/build-with-claude/vision>Claude Vision Capabilities</a></li><li><a href=https://github.com/pgvector/pgvector>pgvector for PostgreSQL</a></li><li><a href=https://aws.amazon.com/bedrock/>Amazon Bedrock</a></li></ul></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://manuelfedele.github.io/posts/ecs-fargate-production-patterns/ class="button inline prev">&lt; [<span class=button__text>ECS Fargate Production Patterns That Actually Work</span>]
</a>::
<a href=https://manuelfedele.github.io/posts/building-interactive-cli-tools-in-go-with-bubbletea/ class="button inline next">[<span class=button__text>Building Interactive CLI Tools in Go with Bubbletea</span>] ></a></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2026 Powered by <a href=https://gohugo.io>Hugo</a></span>
<span>:: <a href=https://github.com/panr/hugo-theme-terminal target=_blank>Theme</a> made by <a href=https://github.com/panr target=_blank>panr</a></span></div></div></footer><script type=text/javascript src=/bundle.min.js></script></div></body></html>