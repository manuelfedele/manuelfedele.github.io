<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building an AI-Powered Document Processing Pipeline on AWS | Git Push and Run</title><meta name=keywords content="AWS,AI,LLM,Step Functions,Lambda,serverless,python,architecture"><meta name=description content="Building an AI-Powered Document Processing Pipeline on AWS
Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.
In this post I&rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs."><meta name=author content><link rel=canonical href=https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/><link crossorigin=anonymous href=/assets/css/stylesheet.c2189e7d36d72767de292f8b9d897e4a464aa5bc7eb692cda50064f5eb9eeece.css integrity="sha256-whiefTbXJ2feKS+LnYl+SkZKpbx+tpLNpQBk9eue7s4=" rel="preload stylesheet" as=style><link rel=icon href=https://manuelfedele.github.io/img/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://manuelfedele.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://manuelfedele.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://manuelfedele.github.io/img/apple-touch-icon-144-precomposed.png><link rel=mask-icon href=https://manuelfedele.github.io/img/favicon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta name=google-site-verification content="bNHRtbwz1iqFFQ60i1jOBm2U7WHWMc8LgejU7wLaDSs"><script async src="https://www.googletagmanager.com/gtag/js?id=G-K9P1DJX238"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-K9P1DJX238")}</script><meta property="og:url" content="https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/"><meta property="og:site_name" content="Git Push and Run"><meta property="og:title" content="Building an AI-Powered Document Processing Pipeline on AWS"><meta property="og:description" content="Building an AI-Powered Document Processing Pipeline on AWS Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.
In this post I’ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-03T16:45:00+01:00"><meta property="article:modified_time" content="2025-12-03T16:45:00+01:00"><meta property="article:tag" content="AWS"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Step Functions"><meta property="article:tag" content="Lambda"><meta property="article:tag" content="Serverless"><meta property="og:image" content="https://manuelfedele.github.io/img/card-pillow.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://manuelfedele.github.io/img/card-pillow.jpg"><meta name=twitter:title content="Building an AI-Powered Document Processing Pipeline on AWS"><meta name=twitter:description content="Building an AI-Powered Document Processing Pipeline on AWS
Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.
In this post I&rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://manuelfedele.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Building an AI-Powered Document Processing Pipeline on AWS","item":"https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building an AI-Powered Document Processing Pipeline on AWS","name":"Building an AI-Powered Document Processing Pipeline on AWS","description":"Building an AI-Powered Document Processing Pipeline on AWS Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.\nIn this post I\u0026rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.\n","keywords":["AWS","AI","LLM","Step Functions","Lambda","serverless","python","architecture"],"articleBody":"Building an AI-Powered Document Processing Pipeline on AWS Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.\nIn this post I’ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.\nThe Problem Space A typical insurance claim arrives as a ZIP archive containing multiple documents: a police report, medical certificates, repair estimates, photos, correspondence. Each document needs to be:\nParsed into machine-readable text (many are scanned PDFs or photos) Classified by type (police report, medical report, invoice, etc.) Structured by extracting specific fields per document type (e.g., license plate from a police report, diagnosis code from a medical report) Indexed for semantic search so case managers can query across all documents in natural language Summarized so the case manager gets an overview without reading every page The system needs to handle multiple lines of business (motor, health, accident), support concurrent processing of hundreds of claims, and recover gracefully from LLM failures.\nArchitecture Overview The architecture follows an event-driven, fully serverless pattern:\nZIP Upload → S3 → EventBridge → Step Functions → [Lambda Pipeline] → DynamoDB + Aurora ↓ Claims Service (ECS) ← ALB ← Users The key components:\nS3 Ingestion Bucket: receives ZIP uploads via presigned URLs EventBridge Rule: triggers the processing pipeline when a file lands Step Functions: orchestrates the multi-stage processing workflow Lambda Functions: execute each processing stage (stateless, parallel) DynamoDB: tracks processing state and stores extraction results Aurora PostgreSQL: stores vector embeddings for RAG ECS Service (Django): serves the web UI and API for case managers Keycloak + Entra ID: authentication and group-based access control The Ingestion Flow Documents enter the system through a secure upload flow:\nThe external claims management system calls our API to register a claim (line of business, permissions, metadata) It requests a presigned S3 URL from a lightweight Lambda function It uploads the ZIP archive directly to S3 using the presigned URL An EventBridge rule detects the new object and triggers the Step Functions workflow The presigned URL approach avoids routing large files through our API. The Lambda generates short-lived URLs (10-60 seconds), so the upload window is narrow enough to prevent misuse.\nimport boto3 from datetime import datetime def handler(event, context): s3 = boto3.client(\"s3\") claim_name = event[\"claim_name\"] document_code = event[\"document_code\"] key = f\"landing/{claim_name}#{document_code}.zip\" url = s3.generate_presigned_url( \"put_object\", Params={ \"Bucket\": INGESTION_BUCKET, \"Key\": key, \"ContentType\": \"application/zip\", }, ExpiresIn=60, ) return {\"upload_url\": url, \"key\": key} Processing runs in periodic batches (every 10 minutes) rather than on each individual upload. This is a deliberate tradeoff: we accept a few minutes of latency in exchange for better throughput management and cost control when handling bursts of uploads.\nThe Step Functions Pipeline The core of the system is an AWS Step Functions state machine that orchestrates document processing through a linear phase followed by two parallel branches.\nLinear Phase: Parse and Extract Text Stage 1: Start (claim-level)\nThe first Lambda validates the uploaded ZIP, extracts files, uploads individual documents to a support bucket, and creates tracking records in DynamoDB. It also deduplicates: if a document with the same name already exists in the claim and is being processed, it’s skipped.\nStage 2: Split (document-level)\nEach document is converted to PDF (if it’s a DOCX), then split into individual pages saved as PNG images. We use an internal PDF splitting service for this, which handles edge cases like encrypted PDFs, malformed page trees, and oversized documents.\nStage 3: Parse (page-level)\nThis is where the AI kicks in. Each page image is sent to Claude 3.5 Sonnet (via Amazon Bedrock) for vision-based text extraction. The LLM reads the image and produces clean Markdown text, handling handwritten notes, stamps, tables, and mixed layouts that traditional OCR tools struggle with.\nimport json import base64 import boto3 def parse_page(image_bytes: bytes, page_number: int) -\u003e str: bedrock = boto3.client(\"bedrock-runtime\") response = bedrock.invoke_model( modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\", body=json.dumps({ \"anthropic_version\": \"bedrock-2023-05-31\", \"max_tokens\": 4096, \"messages\": [{ \"role\": \"user\", \"content\": [ { \"type\": \"image\", \"source\": { \"type\": \"base64\", \"media_type\": \"image/png\", \"data\": base64.b64encode(image_bytes).decode(), }, }, { \"type\": \"text\", \"text\": \"Extract all text from this document page. \" \"Preserve the structure using Markdown formatting. \" \"Include tables, headers, and any handwritten text.\", }, ], }], }), ) result = json.loads(response[\"body\"].read()) return result[\"content\"][0][\"text\"] Why Claude 3.5 Sonnet for OCR instead of Amazon Textract? Two reasons: (1) vision LLMs handle messy real-world documents (stamps, handwriting, mixed layouts) significantly better than traditional OCR, and (2) the output is already structured Markdown, which downstream stages can work with directly without an intermediate parsing step.\nStage 4: Page Checker (document-level)\nLLM calls fail. Rate limits, timeouts, transient errors. The Page Checker implements retry logic: it collects results from all page parsing calls, identifies failures, and re-dispatches failed pages up to 3 times. This is essential when processing documents with dozens of pages, where even a 1% failure rate means most documents would have at least one failed page.\ndef check_pages(document_id: str, page_results: list[dict]) -\u003e dict: successful = [p for p in page_results if p[\"status\"] == \"success\"] failed = [p for p in page_results if p[\"status\"] == \"error\"] retryable = [ p for p in failed if p.get(\"retry_count\", 0) \u003c MAX_RETRIES ] if retryable: # Re-dispatch failed pages for another attempt for page in retryable: page[\"retry_count\"] = page.get(\"retry_count\", 0) + 1 return {\"status\": \"retry\", \"pages\": retryable} if not successful: return {\"status\": \"error\", \"message\": \"All pages failed parsing\"} # Proceed with whatever pages succeeded return { \"status\": \"success\", \"parsed_pages\": len(successful), \"failed_pages\": len(failed), } Upper Branch: Embedding Pipeline After text extraction, the pipeline forks into two parallel branches. The upper branch creates vector embeddings for semantic search.\nChunker: Combines all page texts into a single document, then splits it into overlapping chunks with metadata (page numbers, positions). Each chunk is saved to S3.\nEmbedder: Each chunk is embedded using OpenAI’s text-embedding-ada-002 model. We chose ada-002 for its good balance of quality and cost at scale. Embeddings are stored in Aurora PostgreSQL using the pgvector extension, enabling similarity search across all documents in a claim.\nThis powers a RAG (Retrieval-Augmented Generation) interface where case managers can ask questions like “What was the estimated repair cost?” and get answers grounded in the actual documents.\nLower Branch: Classification and Extraction Pipeline The lower branch handles structured information extraction.\nClusterization: A single insurance document PDF often contains multiple logical sections (a police report followed by a medical certificate followed by repair photos). The clusterization stage identifies contiguous page ranges that belong to the same topic. We use Gemini 2.0 Flash for this because it’s fast, cheap, and performs well on the classification-style reasoning this step requires.\ndef clusterize_document(pages: list[dict]) -\u003e list[dict]: \"\"\"Identify clusters of pages about the same topic.\"\"\" # Build context from all pages page_summaries = \"\\n\".join( f\"Page {p['number']}: {p['text'][:200]}...\" for p in pages ) prompt = f\"\"\"Analyze these document pages and identify clusters of consecutive pages that discuss the same topic. Pages: {page_summaries} Return a JSON array of clusters, each with: - start_page: first page number - end_page: last page number - topic: brief description of what this section covers \"\"\" # Call Gemini 2.0 Flash via the LLM Gateway response = llm_gateway.invoke( provider=\"vertex\", model=\"gemini-2.0-flash\", prompt=prompt, ) return json.loads(response) Classification: Each cluster is labeled with a document type (police report, medical certificate, invoice, repair estimate, etc.). The classification model uses the cluster’s text content and the claim’s line of business to assign the most appropriate label.\nExtraction: This is where it gets domain-specific. Based on the classification label, the extraction stage applies a tailored extraction strategy. A police report gets license plate, driver name, date, and location extracted. A medical report gets diagnosis, treatment, and provider extracted. An invoice gets line items and totals extracted.\nThe results are stored as structured JSON in DynamoDB, making them queryable and displayable in the web UI.\nCluster Aggregator: Collects results from all clusters, validates completeness, and updates the document’s processing status.\nThe Multi-Model LLM Strategy One of the most interesting architectural decisions was using three different LLM providers, each selected for a specific task:\nTask Model Why Text extraction (OCR) Claude 3.5 Sonnet (Bedrock) Best vision capabilities for messy documents Embeddings text-embedding-ada-002 (OpenAI) Cost-effective, high-quality embeddings at scale Clusterization/Classification Gemini 2.0 Flash (Vertex AI) Fast and cheap for reasoning tasks All LLM calls go through an internal gateway service that abstracts the provider differences. The gateway handles authentication, rate limiting, usage tracking, and fallback logic. From the pipeline’s perspective, it’s just calling an API with a provider and model name.\nThis multi-model approach lets us optimize for cost and quality per task rather than being locked into a single provider. The text extraction stage is the most expensive (vision + large context), so we use the best model available. The clusterization stage processes much less data and needs speed more than depth, so we use a fast, cheap model.\nConcurrency and Error Handling The Step Functions state machine uses Map states to process documents and pages in parallel. A single claim might contain 20 documents, each with 50 pages, resulting in 1,000 parallel page-processing Lambda invocations.\nKey patterns:\nIdempotency: Every Lambda function is idempotent. If a Step Functions execution is retried (due to a transient error), re-processing the same input produces the same result without side effects. We use DynamoDB conditional writes to prevent duplicate processing.\nGraceful degradation: If the embedding branch fails for a document, the classification branch still completes (and vice versa). A document with failed embeddings can still have its structured data extracted. The system tracks partial success at every level.\nCorrelation tracking: Every request gets a correlation ID that flows through all Lambda invocations, S3 objects, and DynamoDB records. When something fails, you can trace the entire processing chain from upload to the specific failed step.\nThe Claims Service The web UI is a Django application running on ECS Fargate, integrated as a microfrontend into the larger analytics platform. Case managers can:\nBrowse claims and their documents View extracted text alongside the original document images See structured extraction results (fields, values, confidence) Search across all documents using natural language (RAG) Generate claim summaries on demand Track document processing status in real time Authentication uses Keycloak with JWT tokens. Authorization uses Entra ID groups: each claim is associated with a visibility group, and only members of that group can access the claim’s documents. A daily sync job keeps group membership current.\nPerformance and Cost Some numbers from production:\nProcessing time: a 30-page document takes approximately 3-5 minutes end-to-end (dominated by LLM call latency) Throughput: the system handles 500+ documents per hour during peak periods Cost per document: roughly EUR 0.15-0.30, depending on page count and complexity (the bulk of the cost is LLM inference) Infrastructure cost when idle: near zero (serverless) The batch processing approach (every 10 minutes) means we can process multiple documents from the same claim together, which is more efficient than processing each upload individually.\nLessons Learned Vision LLMs are production-ready for OCR. Claude 3.5 Sonnet handles real-world insurance documents (stamps, handwriting, poor scans, mixed languages) far better than traditional OCR. The quality improvement justified the higher per-page cost.\nStep Functions are the right tool for document pipelines. The built-in retry logic, parallel Map states, error handling, and visual debugging make Step Functions ideal for multi-stage document processing. We tried orchestrating with SQS queues initially, but the complexity of tracking state across stages was not worth it.\nMulti-model is the way forward. No single LLM is best at everything. Using Claude for vision, OpenAI for embeddings, and Gemini for fast classification gave us the best cost-quality tradeoff at each stage.\nRetry logic is not optional. LLM APIs fail more often than traditional APIs. Rate limits, timeouts, model overload. The Page Checker retry pattern (up to 3 attempts per page) is what makes the pipeline reliable enough for production.\nBatch over real-time when you can. Processing uploads every 10 minutes instead of immediately simplified the architecture significantly and reduced costs. For insurance claims processing, a few minutes of latency is perfectly acceptable.\nWhat’s Next The natural evolution is closing the loop: having the AI agent not just extract and classify documents, but also suggest claim decisions based on the extracted data and historical patterns. This moves from “AI assists the human” to “AI proposes, human approves,” which is the next frontier for insurance automation.\nReferences AWS Step Functions Claude Vision Capabilities pgvector for PostgreSQL Amazon Bedrock ","wordCount":"2120","inLanguage":"en","image":"https://manuelfedele.github.io/img/card-pillow.jpg","datePublished":"2025-12-03T16:45:00+01:00","dateModified":"2025-12-03T16:45:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/"},"publisher":{"@type":"Organization","name":"Git Push and Run","logo":{"@type":"ImageObject","url":"https://manuelfedele.github.io/img/favicon.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://manuelfedele.github.io/ accesskey=h title="Git Push and Run (Alt + H)"><img src=https://manuelfedele.github.io/img/apple-touch-icon-144-precomposed.png alt aria-label=logo height=25>Git Push and Run</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://manuelfedele.github.io/ title=Home><span>Home</span></a></li><li><a href=https://manuelfedele.github.io/search/ title=Search><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://manuelfedele.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://manuelfedele.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Building an AI-Powered Document Processing Pipeline on AWS</h1><div class=post-meta><span title='2025-12-03 16:45:00 +0100 +0100'>December 3, 2025</span>&nbsp;·&nbsp;<span>10 min</span>&nbsp;·&nbsp;<span>2120 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#the-problem-space>The Problem Space</a></li><li><a href=#architecture-overview>Architecture Overview</a></li><li><a href=#the-ingestion-flow>The Ingestion Flow</a></li><li><a href=#the-step-functions-pipeline>The Step Functions Pipeline</a><ul><li><a href=#linear-phase-parse-and-extract-text>Linear Phase: Parse and Extract Text</a></li><li><a href=#upper-branch-embedding-pipeline>Upper Branch: Embedding Pipeline</a></li><li><a href=#lower-branch-classification-and-extraction-pipeline>Lower Branch: Classification and Extraction Pipeline</a></li></ul></li><li><a href=#the-multi-model-llm-strategy>The Multi-Model LLM Strategy</a></li><li><a href=#concurrency-and-error-handling>Concurrency and Error Handling</a></li><li><a href=#the-claims-service>The Claims Service</a></li><li><a href=#performance-and-cost>Performance and Cost</a></li><li><a href=#lessons-learned>Lessons Learned</a></li><li><a href=#whats-next>What&rsquo;s Next</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><h1 id=building-an-ai-powered-document-processing-pipeline-on-aws>Building an AI-Powered Document Processing Pipeline on AWS<a hidden class=anchor aria-hidden=true href=#building-an-ai-powered-document-processing-pipeline-on-aws>#</a></h1><p>Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.</p><p>In this post I&rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.</p><h2 id=the-problem-space>The Problem Space<a hidden class=anchor aria-hidden=true href=#the-problem-space>#</a></h2><p>A typical insurance claim arrives as a ZIP archive containing multiple documents: a police report, medical certificates, repair estimates, photos, correspondence. Each document needs to be:</p><ol><li><strong>Parsed</strong> into machine-readable text (many are scanned PDFs or photos)</li><li><strong>Classified</strong> by type (police report, medical report, invoice, etc.)</li><li><strong>Structured</strong> by extracting specific fields per document type (e.g., license plate from a police report, diagnosis code from a medical report)</li><li><strong>Indexed</strong> for semantic search so case managers can query across all documents in natural language</li><li><strong>Summarized</strong> so the case manager gets an overview without reading every page</li></ol><p>The system needs to handle multiple lines of business (motor, health, accident), support concurrent processing of hundreds of claims, and recover gracefully from LLM failures.</p><h2 id=architecture-overview>Architecture Overview<a hidden class=anchor aria-hidden=true href=#architecture-overview>#</a></h2><p>The architecture follows an event-driven, fully serverless pattern:</p><pre tabindex=0><code>ZIP Upload → S3 → EventBridge → Step Functions → [Lambda Pipeline] → DynamoDB + Aurora
                                                                          ↓
                                              Claims Service (ECS) ← ALB ← Users
</code></pre><p>The key components:</p><ul><li><strong>S3 Ingestion Bucket</strong>: receives ZIP uploads via presigned URLs</li><li><strong>EventBridge Rule</strong>: triggers the processing pipeline when a file lands</li><li><strong>Step Functions</strong>: orchestrates the multi-stage processing workflow</li><li><strong>Lambda Functions</strong>: execute each processing stage (stateless, parallel)</li><li><strong>DynamoDB</strong>: tracks processing state and stores extraction results</li><li><strong>Aurora PostgreSQL</strong>: stores vector embeddings for RAG</li><li><strong>ECS Service (Django)</strong>: serves the web UI and API for case managers</li><li><strong>Keycloak + Entra ID</strong>: authentication and group-based access control</li></ul><h2 id=the-ingestion-flow>The Ingestion Flow<a hidden class=anchor aria-hidden=true href=#the-ingestion-flow>#</a></h2><p>Documents enter the system through a secure upload flow:</p><ol><li>The external claims management system calls our API to register a claim (line of business, permissions, metadata)</li><li>It requests a presigned S3 URL from a lightweight Lambda function</li><li>It uploads the ZIP archive directly to S3 using the presigned URL</li><li>An EventBridge rule detects the new object and triggers the Step Functions workflow</li></ol><p>The presigned URL approach avoids routing large files through our API. The Lambda generates short-lived URLs (10-60 seconds), so the upload window is narrow enough to prevent misuse.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>boto3</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datetime</span> <span class=kn>import</span> <span class=n>datetime</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>handler</span><span class=p>(</span><span class=n>event</span><span class=p>,</span> <span class=n>context</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>s3</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>client</span><span class=p>(</span><span class=s2>&#34;s3&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>claim_name</span> <span class=o>=</span> <span class=n>event</span><span class=p>[</span><span class=s2>&#34;claim_name&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>document_code</span> <span class=o>=</span> <span class=n>event</span><span class=p>[</span><span class=s2>&#34;document_code&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>key</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;landing/</span><span class=si>{</span><span class=n>claim_name</span><span class=si>}</span><span class=s2>#</span><span class=si>{</span><span class=n>document_code</span><span class=si>}</span><span class=s2>.zip&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>url</span> <span class=o>=</span> <span class=n>s3</span><span class=o>.</span><span class=n>generate_presigned_url</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;put_object&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>Params</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;Bucket&#34;</span><span class=p>:</span> <span class=n>INGESTION_BUCKET</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;Key&#34;</span><span class=p>:</span> <span class=n>key</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;ContentType&#34;</span><span class=p>:</span> <span class=s2>&#34;application/zip&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=n>ExpiresIn</span><span class=o>=</span><span class=mi>60</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;upload_url&#34;</span><span class=p>:</span> <span class=n>url</span><span class=p>,</span> <span class=s2>&#34;key&#34;</span><span class=p>:</span> <span class=n>key</span><span class=p>}</span>
</span></span></code></pre></div><p>Processing runs in periodic batches (every 10 minutes) rather than on each individual upload. This is a deliberate tradeoff: we accept a few minutes of latency in exchange for better throughput management and cost control when handling bursts of uploads.</p><h2 id=the-step-functions-pipeline>The Step Functions Pipeline<a hidden class=anchor aria-hidden=true href=#the-step-functions-pipeline>#</a></h2><p>The core of the system is an AWS Step Functions state machine that orchestrates document processing through a linear phase followed by two parallel branches.</p><h3 id=linear-phase-parse-and-extract-text>Linear Phase: Parse and Extract Text<a hidden class=anchor aria-hidden=true href=#linear-phase-parse-and-extract-text>#</a></h3><p><strong>Stage 1: Start (claim-level)</strong></p><p>The first Lambda validates the uploaded ZIP, extracts files, uploads individual documents to a support bucket, and creates tracking records in DynamoDB. It also deduplicates: if a document with the same name already exists in the claim and is being processed, it&rsquo;s skipped.</p><p><strong>Stage 2: Split (document-level)</strong></p><p>Each document is converted to PDF (if it&rsquo;s a DOCX), then split into individual pages saved as PNG images. We use an internal PDF splitting service for this, which handles edge cases like encrypted PDFs, malformed page trees, and oversized documents.</p><p><strong>Stage 3: Parse (page-level)</strong></p><p>This is where the AI kicks in. Each page image is sent to <strong>Claude 3.5 Sonnet</strong> (via Amazon Bedrock) for vision-based text extraction. The LLM reads the image and produces clean Markdown text, handling handwritten notes, stamps, tables, and mixed layouts that traditional OCR tools struggle with.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>base64</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>boto3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>parse_page</span><span class=p>(</span><span class=n>image_bytes</span><span class=p>:</span> <span class=nb>bytes</span><span class=p>,</span> <span class=n>page_number</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>bedrock</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>client</span><span class=p>(</span><span class=s2>&#34;bedrock-runtime&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>bedrock</span><span class=o>.</span><span class=n>invoke_model</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>modelId</span><span class=o>=</span><span class=s2>&#34;anthropic.claude-3-5-sonnet-20240620-v1:0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>body</span><span class=o>=</span><span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>({</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;anthropic_version&#34;</span><span class=p>:</span> <span class=s2>&#34;bedrock-2023-05-31&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;max_tokens&#34;</span><span class=p>:</span> <span class=mi>4096</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;messages&#34;</span><span class=p>:</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                    <span class=p>{</span>
</span></span><span class=line><span class=cl>                        <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;image&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=s2>&#34;source&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                            <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;base64&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=s2>&#34;media_type&#34;</span><span class=p>:</span> <span class=s2>&#34;image/png&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=s2>&#34;data&#34;</span><span class=p>:</span> <span class=n>base64</span><span class=o>.</span><span class=n>b64encode</span><span class=p>(</span><span class=n>image_bytes</span><span class=p>)</span><span class=o>.</span><span class=n>decode</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                        <span class=p>},</span>
</span></span><span class=line><span class=cl>                    <span class=p>},</span>
</span></span><span class=line><span class=cl>                    <span class=p>{</span>
</span></span><span class=line><span class=cl>                        <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;Extract all text from this document page. &#34;</span>
</span></span><span class=line><span class=cl>                                <span class=s2>&#34;Preserve the structure using Markdown formatting. &#34;</span>
</span></span><span class=line><span class=cl>                                <span class=s2>&#34;Include tables, headers, and any handwritten text.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=p>},</span>
</span></span><span class=line><span class=cl>                <span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=p>}],</span>
</span></span><span class=line><span class=cl>        <span class=p>}),</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>result</span> <span class=o>=</span> <span class=n>json</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>response</span><span class=p>[</span><span class=s2>&#34;body&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>read</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>result</span><span class=p>[</span><span class=s2>&#34;content&#34;</span><span class=p>][</span><span class=mi>0</span><span class=p>][</span><span class=s2>&#34;text&#34;</span><span class=p>]</span>
</span></span></code></pre></div><p>Why Claude 3.5 Sonnet for OCR instead of Amazon Textract? Two reasons: (1) vision LLMs handle messy real-world documents (stamps, handwriting, mixed layouts) significantly better than traditional OCR, and (2) the output is already structured Markdown, which downstream stages can work with directly without an intermediate parsing step.</p><p><strong>Stage 4: Page Checker (document-level)</strong></p><p>LLM calls fail. Rate limits, timeouts, transient errors. The Page Checker implements retry logic: it collects results from all page parsing calls, identifies failures, and re-dispatches failed pages up to 3 times. This is essential when processing documents with dozens of pages, where even a 1% failure rate means most documents would have at least one failed page.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>check_pages</span><span class=p>(</span><span class=n>document_id</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>page_results</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>dict</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>successful</span> <span class=o>=</span> <span class=p>[</span><span class=n>p</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>page_results</span> <span class=k>if</span> <span class=n>p</span><span class=p>[</span><span class=s2>&#34;status&#34;</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;success&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>failed</span> <span class=o>=</span> <span class=p>[</span><span class=n>p</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>page_results</span> <span class=k>if</span> <span class=n>p</span><span class=p>[</span><span class=s2>&#34;status&#34;</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;error&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>retryable</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>p</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>failed</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;retry_count&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>MAX_RETRIES</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>retryable</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Re-dispatch failed pages for another attempt</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>page</span> <span class=ow>in</span> <span class=n>retryable</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>page</span><span class=p>[</span><span class=s2>&#34;retry_count&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>page</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;retry_count&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;retry&#34;</span><span class=p>,</span> <span class=s2>&#34;pages&#34;</span><span class=p>:</span> <span class=n>retryable</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=n>successful</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;error&#34;</span><span class=p>,</span> <span class=s2>&#34;message&#34;</span><span class=p>:</span> <span class=s2>&#34;All pages failed parsing&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Proceed with whatever pages succeeded</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;success&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;parsed_pages&#34;</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>successful</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;failed_pages&#34;</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>failed</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><h3 id=upper-branch-embedding-pipeline>Upper Branch: Embedding Pipeline<a hidden class=anchor aria-hidden=true href=#upper-branch-embedding-pipeline>#</a></h3><p>After text extraction, the pipeline forks into two parallel branches. The upper branch creates vector embeddings for semantic search.</p><p><strong>Chunker</strong>: Combines all page texts into a single document, then splits it into overlapping chunks with metadata (page numbers, positions). Each chunk is saved to S3.</p><p><strong>Embedder</strong>: Each chunk is embedded using <strong>OpenAI&rsquo;s text-embedding-ada-002</strong> model. We chose ada-002 for its good balance of quality and cost at scale. Embeddings are stored in Aurora PostgreSQL using the <code>pgvector</code> extension, enabling similarity search across all documents in a claim.</p><p>This powers a RAG (Retrieval-Augmented Generation) interface where case managers can ask questions like &ldquo;What was the estimated repair cost?&rdquo; and get answers grounded in the actual documents.</p><h3 id=lower-branch-classification-and-extraction-pipeline>Lower Branch: Classification and Extraction Pipeline<a hidden class=anchor aria-hidden=true href=#lower-branch-classification-and-extraction-pipeline>#</a></h3><p>The lower branch handles structured information extraction.</p><p><strong>Clusterization</strong>: A single insurance document PDF often contains multiple logical sections (a police report followed by a medical certificate followed by repair photos). The clusterization stage identifies contiguous page ranges that belong to the same topic. We use <strong>Gemini 2.0 Flash</strong> for this because it&rsquo;s fast, cheap, and performs well on the classification-style reasoning this step requires.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>clusterize_document</span><span class=p>(</span><span class=n>pages</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>dict</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=nb>dict</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Identify clusters of pages about the same topic.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Build context from all pages</span>
</span></span><span class=line><span class=cl>    <span class=n>page_summaries</span> <span class=o>=</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=sa>f</span><span class=s2>&#34;Page </span><span class=si>{</span><span class=n>p</span><span class=p>[</span><span class=s1>&#39;number&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>p</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>][:</span><span class=mi>200</span><span class=p>]</span><span class=si>}</span><span class=s2>...&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>pages</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;&#34;&#34;Analyze these document pages and identify clusters
</span></span></span><span class=line><span class=cl><span class=s2>    of consecutive pages that discuss the same topic.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Pages:
</span></span></span><span class=line><span class=cl><span class=s2>    </span><span class=si>{</span><span class=n>page_summaries</span><span class=si>}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Return a JSON array of clusters, each with:
</span></span></span><span class=line><span class=cl><span class=s2>    - start_page: first page number
</span></span></span><span class=line><span class=cl><span class=s2>    - end_page: last page number
</span></span></span><span class=line><span class=cl><span class=s2>    - topic: brief description of what this section covers
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Call Gemini 2.0 Flash via the LLM Gateway</span>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>llm_gateway</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>provider</span><span class=o>=</span><span class=s2>&#34;vertex&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>=</span><span class=s2>&#34;gemini-2.0-flash&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>prompt</span><span class=o>=</span><span class=n>prompt</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>json</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Classification</strong>: Each cluster is labeled with a document type (police report, medical certificate, invoice, repair estimate, etc.). The classification model uses the cluster&rsquo;s text content and the claim&rsquo;s line of business to assign the most appropriate label.</p><p><strong>Extraction</strong>: This is where it gets domain-specific. Based on the classification label, the extraction stage applies a tailored extraction strategy. A police report gets license plate, driver name, date, and location extracted. A medical report gets diagnosis, treatment, and provider extracted. An invoice gets line items and totals extracted.</p><p>The results are stored as structured JSON in DynamoDB, making them queryable and displayable in the web UI.</p><p><strong>Cluster Aggregator</strong>: Collects results from all clusters, validates completeness, and updates the document&rsquo;s processing status.</p><h2 id=the-multi-model-llm-strategy>The Multi-Model LLM Strategy<a hidden class=anchor aria-hidden=true href=#the-multi-model-llm-strategy>#</a></h2><p>One of the most interesting architectural decisions was using three different LLM providers, each selected for a specific task:</p><table><thead><tr><th>Task</th><th>Model</th><th>Why</th></tr></thead><tbody><tr><td>Text extraction (OCR)</td><td>Claude 3.5 Sonnet (Bedrock)</td><td>Best vision capabilities for messy documents</td></tr><tr><td>Embeddings</td><td>text-embedding-ada-002 (OpenAI)</td><td>Cost-effective, high-quality embeddings at scale</td></tr><tr><td>Clusterization/Classification</td><td>Gemini 2.0 Flash (Vertex AI)</td><td>Fast and cheap for reasoning tasks</td></tr></tbody></table><p>All LLM calls go through an internal gateway service that abstracts the provider differences. The gateway handles authentication, rate limiting, usage tracking, and fallback logic. From the pipeline&rsquo;s perspective, it&rsquo;s just calling an API with a provider and model name.</p><p>This multi-model approach lets us optimize for cost and quality per task rather than being locked into a single provider. The text extraction stage is the most expensive (vision + large context), so we use the best model available. The clusterization stage processes much less data and needs speed more than depth, so we use a fast, cheap model.</p><h2 id=concurrency-and-error-handling>Concurrency and Error Handling<a hidden class=anchor aria-hidden=true href=#concurrency-and-error-handling>#</a></h2><p>The Step Functions state machine uses Map states to process documents and pages in parallel. A single claim might contain 20 documents, each with 50 pages, resulting in 1,000 parallel page-processing Lambda invocations.</p><p>Key patterns:</p><p><strong>Idempotency</strong>: Every Lambda function is idempotent. If a Step Functions execution is retried (due to a transient error), re-processing the same input produces the same result without side effects. We use DynamoDB conditional writes to prevent duplicate processing.</p><p><strong>Graceful degradation</strong>: If the embedding branch fails for a document, the classification branch still completes (and vice versa). A document with failed embeddings can still have its structured data extracted. The system tracks partial success at every level.</p><p><strong>Correlation tracking</strong>: Every request gets a correlation ID that flows through all Lambda invocations, S3 objects, and DynamoDB records. When something fails, you can trace the entire processing chain from upload to the specific failed step.</p><h2 id=the-claims-service>The Claims Service<a hidden class=anchor aria-hidden=true href=#the-claims-service>#</a></h2><p>The web UI is a Django application running on ECS Fargate, integrated as a microfrontend into the larger analytics platform. Case managers can:</p><ul><li>Browse claims and their documents</li><li>View extracted text alongside the original document images</li><li>See structured extraction results (fields, values, confidence)</li><li>Search across all documents using natural language (RAG)</li><li>Generate claim summaries on demand</li><li>Track document processing status in real time</li></ul><p>Authentication uses Keycloak with JWT tokens. Authorization uses Entra ID groups: each claim is associated with a visibility group, and only members of that group can access the claim&rsquo;s documents. A daily sync job keeps group membership current.</p><h2 id=performance-and-cost>Performance and Cost<a hidden class=anchor aria-hidden=true href=#performance-and-cost>#</a></h2><p>Some numbers from production:</p><ul><li><strong>Processing time</strong>: a 30-page document takes approximately 3-5 minutes end-to-end (dominated by LLM call latency)</li><li><strong>Throughput</strong>: the system handles 500+ documents per hour during peak periods</li><li><strong>Cost per document</strong>: roughly EUR 0.15-0.30, depending on page count and complexity (the bulk of the cost is LLM inference)</li><li><strong>Infrastructure cost when idle</strong>: near zero (serverless)</li></ul><p>The batch processing approach (every 10 minutes) means we can process multiple documents from the same claim together, which is more efficient than processing each upload individually.</p><h2 id=lessons-learned>Lessons Learned<a hidden class=anchor aria-hidden=true href=#lessons-learned>#</a></h2><p><strong>Vision LLMs are production-ready for OCR.</strong> Claude 3.5 Sonnet handles real-world insurance documents (stamps, handwriting, poor scans, mixed languages) far better than traditional OCR. The quality improvement justified the higher per-page cost.</p><p><strong>Step Functions are the right tool for document pipelines.</strong> The built-in retry logic, parallel Map states, error handling, and visual debugging make Step Functions ideal for multi-stage document processing. We tried orchestrating with SQS queues initially, but the complexity of tracking state across stages was not worth it.</p><p><strong>Multi-model is the way forward.</strong> No single LLM is best at everything. Using Claude for vision, OpenAI for embeddings, and Gemini for fast classification gave us the best cost-quality tradeoff at each stage.</p><p><strong>Retry logic is not optional.</strong> LLM APIs fail more often than traditional APIs. Rate limits, timeouts, model overload. The Page Checker retry pattern (up to 3 attempts per page) is what makes the pipeline reliable enough for production.</p><p><strong>Batch over real-time when you can.</strong> Processing uploads every 10 minutes instead of immediately simplified the architecture significantly and reduced costs. For insurance claims processing, a few minutes of latency is perfectly acceptable.</p><h2 id=whats-next>What&rsquo;s Next<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h2><p>The natural evolution is closing the loop: having the AI agent not just extract and classify documents, but also suggest claim decisions based on the extracted data and historical patterns. This moves from &ldquo;AI assists the human&rdquo; to &ldquo;AI proposes, human approves,&rdquo; which is the next frontier for insurance automation.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li><a href=https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html>AWS Step Functions</a></li><li><a href=https://docs.anthropic.com/en/docs/build-with-claude/vision>Claude Vision Capabilities</a></li><li><a href=https://github.com/pgvector/pgvector>pgvector for PostgreSQL</a></li><li><a href=https://aws.amazon.com/bedrock/>Amazon Bedrock</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://manuelfedele.github.io/tags/aws/>AWS</a></li><li><a href=https://manuelfedele.github.io/tags/ai/>AI</a></li><li><a href=https://manuelfedele.github.io/tags/llm/>LLM</a></li><li><a href=https://manuelfedele.github.io/tags/step-functions/>Step Functions</a></li><li><a href=https://manuelfedele.github.io/tags/lambda/>Lambda</a></li><li><a href=https://manuelfedele.github.io/tags/serverless/>Serverless</a></li><li><a href=https://manuelfedele.github.io/tags/python/>Python</a></li><li><a href=https://manuelfedele.github.io/tags/architecture/>Architecture</a></li></ul><nav class=paginav><a class=prev href=https://manuelfedele.github.io/posts/ecs-fargate-production-patterns/><span class=title>« Prev</span><br><span>ECS Fargate Production Patterns That Actually Work</span>
</a><a class=next href=https://manuelfedele.github.io/posts/building-interactive-cli-tools-in-go-with-bubbletea/><span class=title>Next »</span><br><span>Building Interactive CLI Tools in Go with Bubbletea</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI-Powered Document Processing Pipeline on AWS on x" href="https://x.com/intent/tweet/?text=Building%20an%20AI-Powered%20Document%20Processing%20Pipeline%20on%20AWS&amp;url=https%3a%2f%2fmanuelfedele.github.io%2fposts%2fbuilding-ai-document-processing-pipeline-aws%2f&amp;hashtags=AWS%2cAI%2cLLM%2cStepFunctions%2cLambda%2cserverless%2cpython%2carchitecture"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI-Powered Document Processing Pipeline on AWS on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmanuelfedele.github.io%2fposts%2fbuilding-ai-document-processing-pipeline-aws%2f&amp;title=Building%20an%20AI-Powered%20Document%20Processing%20Pipeline%20on%20AWS&amp;summary=Building%20an%20AI-Powered%20Document%20Processing%20Pipeline%20on%20AWS&amp;source=https%3a%2f%2fmanuelfedele.github.io%2fposts%2fbuilding-ai-document-processing-pipeline-aws%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI-Powered Document Processing Pipeline on AWS on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmanuelfedele.github.io%2fposts%2fbuilding-ai-document-processing-pipeline-aws%2f&title=Building%20an%20AI-Powered%20Document%20Processing%20Pipeline%20on%20AWS"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI-Powered Document Processing Pipeline on AWS on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmanuelfedele.github.io%2fposts%2fbuilding-ai-document-processing-pipeline-aws%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI-Powered Document Processing Pipeline on AWS on whatsapp" href="https://api.whatsapp.com/send?text=Building%20an%20AI-Powered%20Document%20Processing%20Pipeline%20on%20AWS%20-%20https%3a%2f%2fmanuelfedele.github.io%2fposts%2fbuilding-ai-document-processing-pipeline-aws%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI-Powered Document Processing Pipeline on AWS on telegram" href="https://telegram.me/share/url?text=Building%20an%20AI-Powered%20Document%20Processing%20Pipeline%20on%20AWS&amp;url=https%3a%2f%2fmanuelfedele.github.io%2fposts%2fbuilding-ai-document-processing-pipeline-aws%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI-Powered Document Processing Pipeline on AWS on ycombinator" href="https://news.ycombinator.com/submitlink?t=Building%20an%20AI-Powered%20Document%20Processing%20Pipeline%20on%20AWS&u=https%3a%2f%2fmanuelfedele.github.io%2fposts%2fbuilding-ai-document-processing-pipeline-aws%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//manuelfedele-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2026 <a href=https://manuelfedele.github.io/>Git Push and Run</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>