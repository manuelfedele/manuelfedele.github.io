<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Lambda on Git Push and Run</title><link>https://manuelfedele.github.io/tags/lambda/</link><description>Recent content in Lambda on Git Push and Run</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2026 Manuel Fedele</copyright><lastBuildDate>Wed, 03 Dec 2025 16:45:00 +0100</lastBuildDate><atom:link href="https://manuelfedele.github.io/tags/lambda/index.xml" rel="self" type="application/rss+xml"/><item><title>Building an AI-Powered Document Processing Pipeline on AWS</title><link>https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/</link><pubDate>Wed, 03 Dec 2025 16:45:00 +0100</pubDate><guid>https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/</guid><description>&lt;h1 class="relative group">Building an AI-Powered Document Processing Pipeline on AWS
 &lt;div id="building-an-ai-powered-document-processing-pipeline-on-aws" class="anchor">&lt;/div>
 
 &lt;span
 class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
 &lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#building-an-ai-powered-document-processing-pipeline-on-aws" aria-label="Anchor">#&lt;/a>
 &lt;/span>
 
&lt;/h1>
&lt;p>Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.&lt;/p>
&lt;p>In this post I&amp;rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.&lt;/p></description></item><item><title>Aws Opensearch as Monitoring Tool</title><link>https://manuelfedele.github.io/posts/aws-opensearch-as-monitoring-tool/</link><pubDate>Wed, 12 Jul 2023 08:42:00 +0200</pubDate><guid>https://manuelfedele.github.io/posts/aws-opensearch-as-monitoring-tool/</guid><description>&lt;h1 class="relative group">Cross-Account Logging: Shipping AWS Lambda Logs to OpenSearch
 &lt;div id="cross-account-logging-shipping-aws-lambda-logs-to-opensearch" class="anchor">&lt;/div>
 
 &lt;span
 class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
 &lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#cross-account-logging-shipping-aws-lambda-logs-to-opensearch" aria-label="Anchor">#&lt;/a>
 &lt;/span>
 
&lt;/h1>
&lt;p>In today&amp;rsquo;s distributed systems, logging and monitoring play a crucial role in detecting anomalies and ensuring system health. AWS Lambda and OpenSearch are often paired to deliver efficient, scalable logging solutions. However, complexities can arise when these resources live in separate AWS accounts. This blog post will guide you through the process of sending AWS Lambda logs from Account A to an OpenSearch cluster in Account B using Terraform as the Infrastructure as Code (IAC) tool and GitLab for CI/CD pipelines.&lt;/p></description></item></channel></rss>