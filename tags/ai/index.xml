<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI on Git Push and Run</title>
    <link>https://manuelfedele.github.io/tags/ai/</link>
    <description>Recent content in AI on Git Push and Run</description>
    <image>
      <title>Git Push and Run</title>
      <url>https://manuelfedele.github.io/img/card-pillow.jpg</url>
      <link>https://manuelfedele.github.io/img/card-pillow.jpg</link>
    </image>
    <generator>Hugo -- 0.147.6</generator>
    <language>en</language>
    <lastBuildDate>Sun, 22 Feb 2026 09:30:00 +0100</lastBuildDate>
    <atom:link href="https://manuelfedele.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Building an AI SRE Assistant From Scratch: Architecture of an Autonomous Infrastructure Investigator</title>
      <link>https://manuelfedele.github.io/posts/building-ai-sre-assistant-from-scratch/</link>
      <pubDate>Sun, 22 Feb 2026 09:30:00 +0100</pubDate>
      <guid>https://manuelfedele.github.io/posts/building-ai-sre-assistant-from-scratch/</guid>
      <description>&lt;h1 id=&#34;building-an-ai-sre-assistant-from-scratch-architecture-of-an-autonomous-infrastructure-investigator&#34;&gt;Building an AI SRE Assistant From Scratch: Architecture of an Autonomous Infrastructure Investigator&lt;/h1&gt;
&lt;p&gt;What if your on-call engineer never slept, had instant access to every repository and every AWS account, and could trace a production issue from DNS to database in under a minute?&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s the question that led me to build TARS, an AI-powered SRE assistant that autonomously investigates infrastructure issues by combining LLM reasoning with deep integrations into GitLab and AWS. Named after the robot from Interstellar (because every good internal tool needs a movie reference), TARS is a full-stack application where engineers interact with an AI agent through a chat interface. The agent doesn&amp;rsquo;t just answer questions. It investigates. It clones repos, greps code, reads CloudWatch logs, traces DNS chains, inspects ECS services, and synthesizes findings into structured reports.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building an AI-Powered Platform Operations Agent</title>
      <link>https://manuelfedele.github.io/posts/building-ai-powered-platform-operations-agent/</link>
      <pubDate>Sun, 15 Feb 2026 11:00:00 +0100</pubDate>
      <guid>https://manuelfedele.github.io/posts/building-ai-powered-platform-operations-agent/</guid>
      <description>&lt;h1 id=&#34;building-an-ai-powered-platform-operations-agent&#34;&gt;Building an AI-Powered Platform Operations Agent&lt;/h1&gt;
&lt;p&gt;Platform engineering teams handle a constant stream of repetitive requests: onboarding users, managing API keys, checking service health, rotating credentials. Most of these tasks follow well-defined procedures that a human executes step by step. What if an AI agent could handle them instead?&lt;/p&gt;
&lt;p&gt;In this post, I&amp;rsquo;ll walk through the architecture of an AI-powered operations agent that automates common platform tasks by giving an LLM access to your internal tools through a structured tool-calling interface.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building an AI-Powered Document Processing Pipeline on AWS</title>
      <link>https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/</link>
      <pubDate>Wed, 03 Dec 2025 16:45:00 +0100</pubDate>
      <guid>https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/</guid>
      <description>&lt;h1 id=&#34;building-an-ai-powered-document-processing-pipeline-on-aws&#34;&gt;Building an AI-Powered Document Processing Pipeline on AWS&lt;/h1&gt;
&lt;p&gt;Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.&lt;/p&gt;
&lt;p&gt;In this post I&amp;rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
