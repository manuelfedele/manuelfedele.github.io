<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Architecture on Git Push and Run</title><link>https://manuelfedele.github.io/tags/architecture/</link><description>Recent content in Architecture on Git Push and Run</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sun, 01 Mar 2026 12:00:00 +0100</lastBuildDate><atom:link href="https://manuelfedele.github.io/tags/architecture/index.xml" rel="self" type="application/rss+xml"/><item><title>Leveraging AI as a Platform Engineer: What Actually Works</title><link>https://manuelfedele.github.io/posts/leveraging-ai-as-a-platform-engineer/</link><pubDate>Sun, 01 Mar 2026 12:00:00 +0100</pubDate><guid>https://manuelfedele.github.io/posts/leveraging-ai-as-a-platform-engineer/</guid><description>&lt;p>I have spent the last year building AI into the core of my daily engineering work, not as a chat assistant, but as an active participant in systems that move real infrastructure and interact with production APIs. This post is an attempt to write down what I have learned: the patterns that work, the ones that looked good in demos but failed in practice, and the architectural decisions that turned out to matter.&lt;/p></description></item><item><title>Building an AI SRE Assistant From Scratch: Architecture of an Autonomous Infrastructure Investigator</title><link>https://manuelfedele.github.io/posts/building-ai-sre-assistant-from-scratch/</link><pubDate>Sun, 22 Feb 2026 09:30:00 +0100</pubDate><guid>https://manuelfedele.github.io/posts/building-ai-sre-assistant-from-scratch/</guid><description>&lt;h1 id="building-an-ai-sre-assistant-from-scratch-architecture-of-an-autonomous-infrastructure-investigator">Building an AI SRE Assistant From Scratch: Architecture of an Autonomous Infrastructure Investigator&lt;/h1>
&lt;p>What if your on-call engineer never slept, had instant access to every repository and every AWS account, and could trace a production issue from DNS to database in under a minute?&lt;/p>
&lt;p>That&amp;rsquo;s the question that led me to build TARS, an AI-powered SRE assistant that autonomously investigates infrastructure issues by combining LLM reasoning with deep integrations into GitLab and AWS. Named after the robot from Interstellar (because every good internal tool needs a movie reference), TARS is a full-stack application where engineers interact with an AI agent through a chat interface. The agent doesn&amp;rsquo;t just answer questions. It investigates. It clones repos, greps code, reads CloudWatch logs, traces DNS chains, inspects ECS services, and synthesizes findings into structured reports.&lt;/p></description></item><item><title>Building an AI-Powered Document Processing Pipeline on AWS</title><link>https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/</link><pubDate>Wed, 03 Dec 2025 16:45:00 +0100</pubDate><guid>https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/</guid><description>&lt;h1 id="building-an-ai-powered-document-processing-pipeline-on-aws">Building an AI-Powered Document Processing Pipeline on AWS&lt;/h1>
&lt;p>Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.&lt;/p>
&lt;p>In this post I&amp;rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.&lt;/p></description></item></channel></rss>