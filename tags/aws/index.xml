<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AWS on Git Push and Run</title><link>https://manuelfedele.github.io/tags/aws/</link><description>Recent content in AWS on Git Push and Run</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2026 Manuel Fedele</copyright><lastBuildDate>Sun, 01 Mar 2026 10:00:00 +0100</lastBuildDate><atom:link href="https://manuelfedele.github.io/tags/aws/index.xml" rel="self" type="application/rss+xml"/><item><title>WASM in the Browser: Deploying VERT on CloudFront for Free</title><link>https://manuelfedele.github.io/posts/wasm-vert-cloudfront-zero-cost/</link><pubDate>Sun, 01 Mar 2026 10:00:00 +0100</pubDate><guid>https://manuelfedele.github.io/posts/wasm-vert-cloudfront-zero-cost/</guid><description>&lt;h2 class="relative group">What is WebAssembly?
 &lt;div id="what-is-webassembly" class="anchor">&lt;/div>
 
 &lt;span
 class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
 &lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#what-is-webassembly" aria-label="Anchor">#&lt;/a>
 &lt;/span>
 
&lt;/h2>
&lt;p>WebAssembly (WASM) is a binary instruction format that runs inside the browser at near-native speed. Think of it as a portable compilation target: you write code in Rust, C, C++, or Go, compile it to &lt;code>.wasm&lt;/code>, and the browser executes it directly in a sandboxed VM alongside JavaScript.&lt;/p>
&lt;p>The browser has always been able to run &lt;em>computation&lt;/em>, but JavaScript is an interpreted, dynamically typed language. It is fast enough for most tasks, but there are workloads where it simply is not competitive: video encoding, image processing, cryptography, physics simulations. Before WASM, these had to live on a server. Now they can live in the browser tab.&lt;/p></description></item><item><title>Building an AI SRE Assistant From Scratch: Architecture of an Autonomous Infrastructure Investigator</title><link>https://manuelfedele.github.io/posts/building-ai-sre-assistant-from-scratch/</link><pubDate>Sun, 22 Feb 2026 09:30:00 +0100</pubDate><guid>https://manuelfedele.github.io/posts/building-ai-sre-assistant-from-scratch/</guid><description>&lt;h1 class="relative group">Building an AI SRE Assistant From Scratch: Architecture of an Autonomous Infrastructure Investigator
 &lt;div id="building-an-ai-sre-assistant-from-scratch-architecture-of-an-autonomous-infrastructure-investigator" class="anchor">&lt;/div>
 
 &lt;span
 class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
 &lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#building-an-ai-sre-assistant-from-scratch-architecture-of-an-autonomous-infrastructure-investigator" aria-label="Anchor">#&lt;/a>
 &lt;/span>
 
&lt;/h1>
&lt;p>What if your on-call engineer never slept, had instant access to every repository and every AWS account, and could trace a production issue from DNS to database in under a minute?&lt;/p>
&lt;p>That&amp;rsquo;s the question that led me to build TARS, an AI-powered SRE assistant that autonomously investigates infrastructure issues by combining LLM reasoning with deep integrations into GitLab and AWS. Named after the robot from Interstellar (because every good internal tool needs a movie reference), TARS is a full-stack application where engineers interact with an AI agent through a chat interface. The agent doesn&amp;rsquo;t just answer questions. It investigates. It clones repos, greps code, reads CloudWatch logs, traces DNS chains, inspects ECS services, and synthesizes findings into structured reports.&lt;/p></description></item><item><title>ECS Fargate Production Patterns That Actually Work</title><link>https://manuelfedele.github.io/posts/ecs-fargate-production-patterns/</link><pubDate>Thu, 08 Jan 2026 14:20:00 +0100</pubDate><guid>https://manuelfedele.github.io/posts/ecs-fargate-production-patterns/</guid><description>&lt;h1 class="relative group">ECS Fargate Production Patterns That Actually Work
 &lt;div id="ecs-fargate-production-patterns-that-actually-work" class="anchor">&lt;/div>
 
 &lt;span
 class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
 &lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#ecs-fargate-production-patterns-that-actually-work" aria-label="Anchor">#&lt;/a>
 &lt;/span>
 
&lt;/h1>
&lt;p>I&amp;rsquo;ve deployed and managed many containerized services on ECS Fargate. Over time, a set of patterns has emerged that I apply consistently to every new service. This post documents those patterns with Terraform examples, covering everything from Fargate Spot strategies to deployment circuit breakers and ARM64 migration.&lt;/p>

&lt;h2 class="relative group">The Standard Architecture
 &lt;div id="the-standard-architecture" class="anchor">&lt;/div>
 
 &lt;span
 class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
 &lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#the-standard-architecture" aria-label="Anchor">#&lt;/a>
 &lt;/span>
 
&lt;/h2>
&lt;p>Every service I deploy follows the same high-level architecture:&lt;/p></description></item><item><title>Building an AI-Powered Document Processing Pipeline on AWS</title><link>https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/</link><pubDate>Wed, 03 Dec 2025 16:45:00 +0100</pubDate><guid>https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/</guid><description>&lt;h1 class="relative group">Building an AI-Powered Document Processing Pipeline on AWS
 &lt;div id="building-an-ai-powered-document-processing-pipeline-on-aws" class="anchor">&lt;/div>
 
 &lt;span
 class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
 &lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#building-an-ai-powered-document-processing-pipeline-on-aws" aria-label="Anchor">#&lt;/a>
 &lt;/span>
 
&lt;/h1>
&lt;p>Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.&lt;/p>
&lt;p>In this post I&amp;rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.&lt;/p></description></item><item><title>Managing Multi-Account AWS Infrastructure with Terraform Workspaces</title><link>https://manuelfedele.github.io/posts/multi-account-aws-terraform-workspaces/</link><pubDate>Mon, 22 Sep 2025 09:15:00 +0200</pubDate><guid>https://manuelfedele.github.io/posts/multi-account-aws-terraform-workspaces/</guid><description>&lt;h1 class="relative group">Managing Multi-Account AWS Infrastructure with Terraform Workspaces
 &lt;div id="managing-multi-account-aws-infrastructure-with-terraform-workspaces" class="anchor">&lt;/div>
 
 &lt;span
 class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
 &lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#managing-multi-account-aws-infrastructure-with-terraform-workspaces" aria-label="Anchor">#&lt;/a>
 &lt;/span>
 
&lt;/h1>
&lt;p>When you&amp;rsquo;re managing infrastructure across dozens of AWS accounts, you need patterns that scale. In this post I&amp;rsquo;ll share the approach I use to manage multi-account, multi-environment AWS infrastructure using Terraform workspaces, modular code, and a consistent tagging strategy.&lt;/p>

&lt;h2 class="relative group">The Problem
 &lt;div id="the-problem" class="anchor">&lt;/div>
 
 &lt;span
 class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
 &lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#the-problem" aria-label="Anchor">#&lt;/a>
 &lt;/span>
 
&lt;/h2>
&lt;p>Imagine this setup: you have multiple organizational scopes (teams, business units, projects), each with their own AWS accounts for non-production and production. On top of that, your non-production account hosts multiple environments (dev, integration, certification). Multiply this by several countries or regions, and you&amp;rsquo;re looking at a lot of infrastructure to manage.&lt;/p></description></item><item><title>Aws Opensearch as Monitoring Tool</title><link>https://manuelfedele.github.io/posts/aws-opensearch-as-monitoring-tool/</link><pubDate>Wed, 12 Jul 2023 08:42:00 +0200</pubDate><guid>https://manuelfedele.github.io/posts/aws-opensearch-as-monitoring-tool/</guid><description>&lt;h1 class="relative group">Cross-Account Logging: Shipping AWS Lambda Logs to OpenSearch
 &lt;div id="cross-account-logging-shipping-aws-lambda-logs-to-opensearch" class="anchor">&lt;/div>
 
 &lt;span
 class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
 &lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#cross-account-logging-shipping-aws-lambda-logs-to-opensearch" aria-label="Anchor">#&lt;/a>
 &lt;/span>
 
&lt;/h1>
&lt;p>In today&amp;rsquo;s distributed systems, logging and monitoring play a crucial role in detecting anomalies and ensuring system health. AWS Lambda and OpenSearch are often paired to deliver efficient, scalable logging solutions. However, complexities can arise when these resources live in separate AWS accounts. This blog post will guide you through the process of sending AWS Lambda logs from Account A to an OpenSearch cluster in Account B using Terraform as the Infrastructure as Code (IAC) tool and GitLab for CI/CD pipelines.&lt;/p></description></item></channel></rss>