<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Serverless on Git Push and Run</title><link>https://manuelfedele.github.io/tags/serverless/</link><description>Recent content in Serverless on Git Push and Run</description><generator>Hugo</generator><language>en</language><lastBuildDate>Wed, 03 Dec 2025 16:45:00 +0100</lastBuildDate><atom:link href="https://manuelfedele.github.io/tags/serverless/index.xml" rel="self" type="application/rss+xml"/><item><title>Building an AI-Powered Document Processing Pipeline on AWS</title><link>https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/</link><pubDate>Wed, 03 Dec 2025 16:45:00 +0100</pubDate><guid>https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/</guid><description>&lt;h1 id="building-an-ai-powered-document-processing-pipeline-on-aws">Building an AI-Powered Document Processing Pipeline on AWS&lt;/h1>
&lt;p>Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.&lt;/p>
&lt;p>In this post I&amp;rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.&lt;/p></description></item></channel></rss>