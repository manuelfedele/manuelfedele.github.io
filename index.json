[{"content":"Building an AI-Powered Platform Operations Agent Platform engineering teams handle a constant stream of repetitive requests: onboarding users, managing API keys, checking service health, rotating credentials. Most of these tasks follow well-defined procedures that a human executes step by step. What if an AI agent could handle them instead?\nIn this post, I\u0026rsquo;ll walk through the architecture of an AI-powered operations agent that automates common platform tasks by giving an LLM access to your internal tools through a structured tool-calling interface.\nThe Problem A typical day on a platform team looks like this:\n\u0026ldquo;Can you create a GitLab repo for project X?\u0026rdquo; \u0026ldquo;I need AWS access for the new developer joining next week.\u0026rdquo; \u0026ldquo;What\u0026rsquo;s the current status of the LLM Gateway?\u0026rdquo; \u0026ldquo;Can you rotate the API key for service Y?\u0026rdquo; \u0026ldquo;Add these users to the Jira project.\u0026rdquo; Each request is straightforward, but collectively they eat hours of engineering time. The procedures are documented, the APIs exist, the permissions are in place. What\u0026rsquo;s missing is an intelligent dispatch layer that can understand the request and execute the right steps.\nArchitecture Overview The agent follows a simple loop:\nUser Request -\u0026gt; LLM (understands intent) -\u0026gt; Tool Selection -\u0026gt; Tool Execution -\u0026gt; Response The LLM acts as the \u0026ldquo;brain\u0026rdquo; that:\nUnderstands what the user wants Selects the right tool(s) Passes the correct parameters Interprets the results Decides if more steps are needed from strands import Agent from strands.models import BedrockModel from .tools import ( aws_tools, gitlab_tools, jira_tools, keycloak_tools, email_tools, ) def create_agent() -\u0026gt; Agent: model = BedrockModel( model_id=\u0026#34;anthropic.claude-sonnet-4-20250514\u0026#34;, region_name=\u0026#34;eu-central-1\u0026#34;, ) tools = [ *aws_tools, *gitlab_tools, *jira_tools, *keycloak_tools, *email_tools, ] system_prompt = \u0026#34;\u0026#34;\u0026#34;You are a platform operations assistant for the engineering team. You help with: - AWS account and user management - GitLab repository operations - Jira project administration - Keycloak user management - Service credential rotation Always confirm destructive operations before executing them. Never expose secrets in your responses. When in doubt, ask for clarification rather than guessing.\u0026#34;\u0026#34;\u0026#34; return Agent( model=model, tools=tools, system_prompt=system_prompt, ) Defining Tools Each tool is a Python function with a clear docstring that tells the LLM what it does and what parameters it needs:\nfrom strands.tools import tool @tool def dynamodb_get_item( table_name: str, key: dict, profile: str = \u0026#34;default\u0026#34;, ) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Retrieve an item from a DynamoDB table. Args: table_name: The DynamoDB table name key: The primary key of the item to retrieve (e.g., {\u0026#34;pk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;value\u0026#34;}}) profile: AWS profile to use Returns: The item if found, or an empty dict \u0026#34;\u0026#34;\u0026#34; import boto3 session = boto3.Session(profile_name=profile) client = session.client(\u0026#34;dynamodb\u0026#34;, region_name=\u0026#34;eu-central-1\u0026#34;) response = client.get_item(TableName=table_name, Key=key) return response.get(\u0026#34;Item\u0026#34;, {}) @tool def dynamodb_scan( table_name: str, filter_expression: str | None = None, expression_values: dict | None = None, profile: str = \u0026#34;default\u0026#34;, ) -\u0026gt; list[dict]: \u0026#34;\u0026#34;\u0026#34;Scan a DynamoDB table with an optional filter. Args: table_name: The DynamoDB table name filter_expression: Optional filter expression expression_values: Optional expression attribute values profile: AWS profile to use Returns: List of matching items \u0026#34;\u0026#34;\u0026#34; import boto3 session = boto3.Session(profile_name=profile) client = session.client(\u0026#34;dynamodb\u0026#34;, region_name=\u0026#34;eu-central-1\u0026#34;) params: dict = {\u0026#34;TableName\u0026#34;: table_name} if filter_expression: params[\u0026#34;FilterExpression\u0026#34;] = filter_expression if expression_values: params[\u0026#34;ExpressionAttributeValues\u0026#34;] = expression_values items = [] while True: response = client.scan(**params) items.extend(response.get(\u0026#34;Items\u0026#34;, [])) if \u0026#34;LastEvaluatedKey\u0026#34; not in response: break params[\u0026#34;ExclusiveStartKey\u0026#34;] = response[\u0026#34;LastEvaluatedKey\u0026#34;] return items Secrets Manager Integration Managing secrets is a common platform task. The agent needs to list, retrieve, and rotate secrets:\n@tool def secrets_manager_list( prefix: str | None = None, profile: str = \u0026#34;default\u0026#34;, ) -\u0026gt; list[str]: \u0026#34;\u0026#34;\u0026#34;List secrets in AWS Secrets Manager, optionally filtered by prefix. Args: prefix: Optional prefix to filter secret names profile: AWS profile to use Returns: List of secret names \u0026#34;\u0026#34;\u0026#34; import boto3 session = boto3.Session(profile_name=profile) client = session.client(\u0026#34;secretsmanager\u0026#34;, region_name=\u0026#34;eu-central-1\u0026#34;) paginator = client.get_paginator(\u0026#34;list_secrets\u0026#34;) filters = [] if prefix: filters.append({\u0026#34;Key\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;Values\u0026#34;: [prefix]}) secrets = [] for page in paginator.paginate(Filters=filters): for secret in page[\u0026#34;SecretList\u0026#34;]: secrets.append(secret[\u0026#34;Name\u0026#34;]) return secrets @tool def secrets_manager_get( secret_name: str, profile: str = \u0026#34;default\u0026#34;, ) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Retrieve a secret value from AWS Secrets Manager. Args: secret_name: The name or ARN of the secret profile: AWS profile to use Returns: The secret value as a string \u0026#34;\u0026#34;\u0026#34; import boto3 session = boto3.Session(profile_name=profile) client = session.client(\u0026#34;secretsmanager\u0026#34;, region_name=\u0026#34;eu-central-1\u0026#34;) response = client.get_secret_value(SecretId=secret_name) return response[\u0026#34;SecretString\u0026#34;] @tool def secrets_manager_put( secret_name: str, secret_value: str, description: str | None = None, profile: str = \u0026#34;default\u0026#34;, ) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Create or update a secret in AWS Secrets Manager. Args: secret_name: The name of the secret secret_value: The secret value to store description: Optional description for the secret profile: AWS profile to use Returns: The ARN of the created/updated secret \u0026#34;\u0026#34;\u0026#34; import boto3 session = boto3.Session(profile_name=profile) client = session.client(\u0026#34;secretsmanager\u0026#34;, region_name=\u0026#34;eu-central-1\u0026#34;) try: response = client.update_secret( SecretId=secret_name, SecretString=secret_value, **({\u0026#34;Description\u0026#34;: description} if description else {}), ) except client.exceptions.ResourceNotFoundException: response = client.create_secret( Name=secret_name, SecretString=secret_value, **({\u0026#34;Description\u0026#34;: description} if description else {}), ) return response[\u0026#34;ARN\u0026#34;] Jira Integration The agent handles Jira service desk requests, which is where most user requests come in:\nimport requests from strands.tools import tool @tool def jira_get_issue(issue_key: str) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Get details of a Jira issue. Args: issue_key: The issue key (e.g., GAP-123) Returns: Issue details including summary, status, assignee, and description \u0026#34;\u0026#34;\u0026#34; import os base_url = os.environ[\u0026#34;ATLASSIAN_BASE_URL\u0026#34;] auth = (os.environ[\u0026#34;ATLASSIAN_USER\u0026#34;], os.environ[\u0026#34;ATLASSIAN_TOKEN\u0026#34;]) response = requests.get( f\u0026#34;{base_url}/rest/api/2/issue/{issue_key}\u0026#34;, auth=auth, timeout=30, ) response.raise_for_status() data = response.json() return { \u0026#34;key\u0026#34;: data[\u0026#34;key\u0026#34;], \u0026#34;summary\u0026#34;: data[\u0026#34;fields\u0026#34;][\u0026#34;summary\u0026#34;], \u0026#34;status\u0026#34;: data[\u0026#34;fields\u0026#34;][\u0026#34;status\u0026#34;][\u0026#34;name\u0026#34;], \u0026#34;assignee\u0026#34;: (data[\u0026#34;fields\u0026#34;].get(\u0026#34;assignee\u0026#34;) or {}).get(\u0026#34;displayName\u0026#34;), \u0026#34;description\u0026#34;: data[\u0026#34;fields\u0026#34;].get(\u0026#34;description\u0026#34;, \u0026#34;\u0026#34;), \u0026#34;issue_type\u0026#34;: data[\u0026#34;fields\u0026#34;][\u0026#34;issuetype\u0026#34;][\u0026#34;name\u0026#34;], } @tool def jira_transition_issue(issue_key: str, status: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Transition a Jira issue to a new status. Args: issue_key: The issue key (e.g., GAP-123) status: The target status name (e.g., \u0026#34;Done\u0026#34;, \u0026#34;In Progress\u0026#34;) Returns: Confirmation message \u0026#34;\u0026#34;\u0026#34; import os base_url = os.environ[\u0026#34;ATLASSIAN_BASE_URL\u0026#34;] auth = (os.environ[\u0026#34;ATLASSIAN_USER\u0026#34;], os.environ[\u0026#34;ATLASSIAN_TOKEN\u0026#34;]) # Get available transitions response = requests.get( f\u0026#34;{base_url}/rest/api/2/issue/{issue_key}/transitions\u0026#34;, auth=auth, timeout=30, ) response.raise_for_status() transitions = response.json()[\u0026#34;transitions\u0026#34;] target = next( (t for t in transitions if t[\u0026#34;name\u0026#34;].lower() == status.lower()), None, ) if not target: available = [t[\u0026#34;name\u0026#34;] for t in transitions] return f\u0026#34;Status \u0026#39;{status}\u0026#39; not available. Options: {available}\u0026#34; response = requests.post( f\u0026#34;{base_url}/rest/api/2/issue/{issue_key}/transitions\u0026#34;, auth=auth, json={\u0026#34;transition\u0026#34;: {\u0026#34;id\u0026#34;: target[\u0026#34;id\u0026#34;]}}, timeout=30, ) response.raise_for_status() return f\u0026#34;Issue {issue_key} transitioned to {status}\u0026#34; Email Notifications After completing a request, the agent can send email notifications:\nimport smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from strands.tools import tool @tool def send_email( to: list[str], subject: str, body_html: str, cc: list[str] | None = None, ) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Send an email notification. Args: to: List of recipient email addresses subject: Email subject line body_html: Email body in HTML format cc: Optional list of CC recipients Returns: Confirmation message \u0026#34;\u0026#34;\u0026#34; import os msg = MIMEMultipart(\u0026#34;alternative\u0026#34;) msg[\u0026#34;Subject\u0026#34;] = subject msg[\u0026#34;From\u0026#34;] = os.environ[\u0026#34;SMTP_FROM\u0026#34;] msg[\u0026#34;To\u0026#34;] = \u0026#34;, \u0026#34;.join(to) if cc: msg[\u0026#34;Cc\u0026#34;] = \u0026#34;, \u0026#34;.join(cc) msg.attach(MIMEText(body_html, \u0026#34;html\u0026#34;)) with smtplib.SMTP(os.environ[\u0026#34;SMTP_HOST\u0026#34;], int(os.environ[\u0026#34;SMTP_PORT\u0026#34;])) as server: server.starttls() server.login(os.environ[\u0026#34;SMTP_USER\u0026#34;], os.environ[\u0026#34;SMTP_PASSWORD\u0026#34;]) recipients = to + (cc or []) server.sendmail(os.environ[\u0026#34;SMTP_FROM\u0026#34;], recipients, msg.as_string()) return f\u0026#34;Email sent to {\u0026#39;, \u0026#39;.join(to)}\u0026#34; Safety: Confirmation for Destructive Operations The system prompt tells the agent to confirm destructive operations, but you should also enforce this at the tool level:\n@tool def s3_delete_objects( bucket: str, prefix: str, dry_run: bool = True, profile: str = \u0026#34;default\u0026#34;, ) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Delete objects from an S3 bucket matching a prefix. Args: bucket: The S3 bucket name prefix: The key prefix to match for deletion dry_run: If True, only list objects that would be deleted without deleting them profile: AWS profile to use Returns: Dict with count of objects deleted (or that would be deleted in dry-run mode) \u0026#34;\u0026#34;\u0026#34; import boto3 session = boto3.Session(profile_name=profile) s3 = session.client(\u0026#34;s3\u0026#34;, region_name=\u0026#34;eu-central-1\u0026#34;) paginator = s3.get_paginator(\u0026#34;list_objects_v2\u0026#34;) objects = [] for page in paginator.paginate(Bucket=bucket, Prefix=prefix): for obj in page.get(\u0026#34;Contents\u0026#34;, []): objects.append({\u0026#34;Key\u0026#34;: obj[\u0026#34;Key\u0026#34;]}) if dry_run: return { \u0026#34;mode\u0026#34;: \u0026#34;dry_run\u0026#34;, \u0026#34;would_delete\u0026#34;: len(objects), \u0026#34;sample_keys\u0026#34;: [o[\u0026#34;Key\u0026#34;] for o in objects[:10]], } if not objects: return {\u0026#34;mode\u0026#34;: \u0026#34;live\u0026#34;, \u0026#34;deleted\u0026#34;: 0} # Delete in batches of 1000 (S3 API limit) deleted = 0 for i in range(0, len(objects), 1000): batch = objects[i : i + 1000] s3.delete_objects( Bucket=bucket, Delete={\u0026#34;Objects\u0026#34;: batch}, ) deleted += len(batch) return {\u0026#34;mode\u0026#34;: \u0026#34;live\u0026#34;, \u0026#34;deleted\u0026#34;: deleted} The dry_run = True default means the agent must explicitly opt into destructive behavior. The LLM will naturally do a dry run first, show the results to the user, and only proceed with dry_run=False after confirmation.\nRunning the Agent The agent can be exposed as a CLI, a Slack bot, or a web API. Here\u0026rsquo;s a simple CLI loop:\ndef main(): agent = create_agent() print(\u0026#34;Platform Operations Agent\u0026#34;) print(\u0026#34;Type \u0026#39;quit\u0026#39; to exit\\n\u0026#34;) while True: user_input = input(\u0026#34;You: \u0026#34;).strip() if user_input.lower() in (\u0026#34;quit\u0026#34;, \u0026#34;exit\u0026#34;): break response = agent(user_input) print(f\u0026#34;\\nAgent: {response}\\n\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Example interaction:\nYou: List all API keys in the LLM Gateway DynamoDB table Agent: I found 23 active API keys in the LLM Gateway table. Here\u0026#39;s a summary: - 8 keys for production services - 10 keys for non-production services - 5 keys for external consumers Would you like me to show details for any specific category? You: Show me the production keys Agent: Here are the 8 production API keys: 1. service-claims-ai-prd (last used: 2 hours ago) 2. service-document-classifier-prd (last used: 15 minutes ago) ... Lessons Learned After running this agent in practice for several months:\nTool descriptions are critical. The LLM selects tools based on their docstrings. Vague descriptions lead to wrong tool selection. Be specific about what each tool does and when to use it.\nDefault to safe. Every destructive tool should default to dry-run mode. The agent will naturally show the user what it plans to do before executing.\nKeep tools focused. One tool should do one thing. A secrets_manager_get tool is better than a secrets_manager_manage tool that tries to do everything based on an \u0026ldquo;action\u0026rdquo; parameter.\nLog everything. Every tool invocation should be logged with its parameters and result. This creates an audit trail and helps debug when the agent makes mistakes.\nStart small. Don\u0026rsquo;t try to automate everything at once. Start with the three most common requests, prove the pattern works, then add more tools over time.\nThe agent makes mistakes. It will occasionally select the wrong tool or pass wrong parameters. The dry-run defaults and confirmation steps catch most errors before they cause damage.\nWhat\u0026rsquo;s Next The natural evolution is to connect the agent to your ticketing system. When a new Jira service desk request comes in, the agent can:\nRead the request Classify it Execute the standard procedure Update the ticket with results Send a notification to the requester This turns your platform team from a ticket queue into an automated service, freeing up engineers for the work that actually requires human judgment.\nReferences Strands Agents SDK Amazon Bedrock Tool Use with Claude ","permalink":"https://manuelfedele.github.io/posts/building-ai-powered-platform-operations-agent/","summary":"\u003ch1 id=\"building-an-ai-powered-platform-operations-agent\"\u003eBuilding an AI-Powered Platform Operations Agent\u003c/h1\u003e\n\u003cp\u003ePlatform engineering teams handle a constant stream of repetitive requests: onboarding users, managing API keys, checking service health, rotating credentials. Most of these tasks follow well-defined procedures that a human executes step by step. What if an AI agent could handle them instead?\u003c/p\u003e\n\u003cp\u003eIn this post, I\u0026rsquo;ll walk through the architecture of an AI-powered operations agent that automates common platform tasks by giving an LLM access to your internal tools through a structured tool-calling interface.\u003c/p\u003e","title":"Building an AI-Powered Platform Operations Agent"},{"content":"ECS Fargate Production Patterns That Actually Work I\u0026rsquo;ve deployed and managed many containerized services on ECS Fargate. Over time, a set of patterns has emerged that I apply consistently to every new service. This post documents those patterns with Terraform examples, covering everything from Fargate Spot strategies to deployment circuit breakers and ARM64 migration.\nThe Standard Architecture Every service I deploy follows the same high-level architecture:\nInternet/VPC -\u0026gt; ALB (HTTPS, TLS 1.3) -\u0026gt; ECS Fargate -\u0026gt; Aurora PostgreSQL Serverless v2 | WAF (rate limiting + AWS managed rules) Each component has its own security group, with traffic flowing only from the layer above. The ALB sits in private subnets (no public-facing services), and Route53 private hosted zones handle internal DNS.\nFargate Spot Strategy Fargate Spot can save up to 70% on compute costs, but you need to handle interruptions. The approach: use a weighted capacity provider strategy that balances cost savings with availability.\nresource \u0026#34;aws_ecs_cluster\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;${var.service_name}-${var.environment}\u0026#34; setting { name = \u0026#34;containerInsights\u0026#34; value = \u0026#34;enabled\u0026#34; } tags = var.tags } resource \u0026#34;aws_ecs_cluster_capacity_providers\u0026#34; \u0026#34;main\u0026#34; { cluster_name = aws_ecs_cluster.main.name capacity_providers = [\u0026#34;FARGATE\u0026#34;, \u0026#34;FARGATE_SPOT\u0026#34;] default_capacity_provider_strategy { capacity_provider = \u0026#34;FARGATE_SPOT\u0026#34; weight = var.spot_weight base = 1 # At least 1 task on regular Fargate } default_capacity_provider_strategy { capacity_provider = \u0026#34;FARGATE\u0026#34; weight = var.ondemand_weight } } The base = 1 on FARGATE ensures you always have at least one task running on On-Demand capacity. This is your safety net during Spot interruptions.\nFor non-production, I use a 4:1 Spot-to-OnDemand ratio. For production, I flip it to 1:4, prioritizing stability while still getting some Spot savings.\nDeployment Circuit Breakers ECS deployment circuit breakers automatically roll back failed deployments. Combined with the right health check configuration, they prevent bad deployments from taking down your service:\nresource \u0026#34;aws_ecs_service\u0026#34; \u0026#34;main\u0026#34; { name = var.service_name cluster = aws_ecs_cluster.main.id task_definition = aws_ecs_task_definition.main.arn desired_count = var.desired_count deployment_circuit_breaker { enable = true rollback = true } deployment_configuration { maximum_percent = 200 minimum_healthy_percent = 100 } load_balancer { target_group_arn = var.target_group_arn container_name = var.service_name container_port = var.container_port } network_configuration { subnets = var.subnet_ids security_groups = [var.security_group_id] assign_public_ip = false } capacity_provider_strategy { capacity_provider = \u0026#34;FARGATE_SPOT\u0026#34; weight = var.spot_weight base = 1 } capacity_provider_strategy { capacity_provider = \u0026#34;FARGATE\u0026#34; weight = var.ondemand_weight } tags = var.tags } The maximum_percent = 200 with minimum_healthy_percent = 100 means ECS will spin up new tasks before draining old ones (rolling deployment). If the new tasks fail health checks, the circuit breaker kicks in and rolls back.\nHealth Check Configuration Getting health checks right is critical. Too aggressive and you get false positives; too lenient and failed deployments take forever to detect:\nresource \u0026#34;aws_lb_target_group\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;${var.service_name}-${var.environment}\u0026#34; port = var.container_port protocol = \u0026#34;HTTP\u0026#34; vpc_id = var.vpc_id target_type = \u0026#34;ip\u0026#34; health_check { enabled = true path = \u0026#34;/health\u0026#34; port = \u0026#34;traffic-port\u0026#34; protocol = \u0026#34;HTTP\u0026#34; healthy_threshold = 2 unhealthy_threshold = 3 timeout = 10 interval = 30 matcher = \u0026#34;200\u0026#34; } deregistration_delay = 30 tags = var.tags } A few things to note:\nderegistration_delay = 30 instead of the default 300 seconds. Most applications can drain in-flight requests in 30 seconds, and the shorter delay means faster deployments. healthy_threshold = 2 means a task needs only 2 successful health checks to be considered healthy (60 seconds with a 30-second interval). unhealthy_threshold = 3 gives tasks 90 seconds of failed health checks before being marked unhealthy. ALB with Path-Based Routing For services with separate frontend and backend containers, path-based routing on a single ALB keeps things simple:\nresource \u0026#34;aws_lb_listener\u0026#34; \u0026#34;https\u0026#34; { load_balancer_arn = aws_lb.main.arn port = 443 protocol = \u0026#34;HTTPS\u0026#34; ssl_policy = \u0026#34;ELBSecurityPolicy-TLS13-1-2-2021-06\u0026#34; certificate_arn = var.certificate_arn default_action { type = \u0026#34;forward\u0026#34; target_group_arn = aws_lb_target_group.frontend.arn } tags = var.tags } resource \u0026#34;aws_lb_listener_rule\u0026#34; \u0026#34;api\u0026#34; { listener_arn = aws_lb_listener.https.arn priority = 100 action { type = \u0026#34;forward\u0026#34; target_group_arn = aws_lb_target_group.backend.arn } condition { path_pattern { values = [\u0026#34;/api/*\u0026#34;] } } tags = var.tags } Note the TLS 1.3 policy. There\u0026rsquo;s no reason to support older TLS versions for internal services.\nAuto-Scaling ECS services should auto-scale on both CPU and memory. I use target tracking policies:\nresource \u0026#34;aws_appautoscaling_target\u0026#34; \u0026#34;ecs\u0026#34; { max_capacity = var.max_count min_capacity = var.min_count resource_id = \u0026#34;service/${aws_ecs_cluster.main.name}/${aws_ecs_service.main.name}\u0026#34; scalable_dimension = \u0026#34;ecs:service:DesiredCount\u0026#34; service_namespace = \u0026#34;ecs\u0026#34; } resource \u0026#34;aws_appautoscaling_policy\u0026#34; \u0026#34;cpu\u0026#34; { name = \u0026#34;${var.service_name}-cpu-scaling\u0026#34; policy_type = \u0026#34;TargetTrackingScaling\u0026#34; resource_id = aws_appautoscaling_target.ecs.resource_id scalable_dimension = aws_appautoscaling_target.ecs.scalable_dimension service_namespace = aws_appautoscaling_target.ecs.service_namespace target_tracking_scaling_policy_configuration { predefined_metric_specification { predefined_metric_type = \u0026#34;ECSServiceAverageCPUUtilization\u0026#34; } target_value = 70.0 scale_in_cooldown = 300 scale_out_cooldown = 60 } } resource \u0026#34;aws_appautoscaling_policy\u0026#34; \u0026#34;memory\u0026#34; { name = \u0026#34;${var.service_name}-memory-scaling\u0026#34; policy_type = \u0026#34;TargetTrackingScaling\u0026#34; resource_id = aws_appautoscaling_target.ecs.resource_id scalable_dimension = aws_appautoscaling_target.ecs.scalable_dimension service_namespace = aws_appautoscaling_target.ecs.service_namespace target_tracking_scaling_policy_configuration { predefined_metric_specification { predefined_metric_type = \u0026#34;ECSServiceAverageMemoryUtilization\u0026#34; } target_value = 80.0 scale_in_cooldown = 300 scale_out_cooldown = 60 } } The asymmetric cooldowns matter: scale_out_cooldown = 60 means the service reacts quickly to load spikes, while scale_in_cooldown = 300 prevents premature scale-down during bursty traffic.\nMigrating to ARM64 (Graviton) AWS Graviton instances offer ~20% better price-performance than x86. Migrating ECS Fargate tasks to ARM64 is straightforward if your images support it:\n# Multi-arch Dockerfile FROM --platform=$TARGETPLATFORM python:3.12-slim WORKDIR /app COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt COPY . . CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;main:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34;] Build and push multi-arch images:\ndocker buildx create --use docker buildx build \\ --platform linux/amd64,linux/arm64 \\ -t $ECR_REPO:latest \\ --push . Then update the task definition:\nresource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;main\u0026#34; { family = var.service_name requires_compatibilities = [\u0026#34;FARGATE\u0026#34;] network_mode = \u0026#34;awsvpc\u0026#34; cpu = var.cpu memory = var.memory execution_role_arn = var.execution_role_arn task_role_arn = var.task_role_arn runtime_platform { operating_system_family = \u0026#34;LINUX\u0026#34; cpu_architecture = \u0026#34;ARM64\u0026#34; } container_definitions = jsonencode([{ name = var.service_name image = var.image essential = true portMappings = [{ containerPort = var.container_port protocol = \u0026#34;tcp\u0026#34; }] logConfiguration = { logDriver = \u0026#34;awslogs\u0026#34; options = { \u0026#34;awslogs-group\u0026#34; = var.log_group_name \u0026#34;awslogs-region\u0026#34; = var.region \u0026#34;awslogs-stream-prefix\u0026#34; = var.service_name } } }]) tags = var.tags } The key line is cpu_architecture = \u0026quot;ARM64\u0026quot;. That\u0026rsquo;s it. If your Docker image is multi-arch, Fargate pulls the right architecture automatically.\nSecrets Management Never bake secrets into container images. Use AWS Secrets Manager with ECS native integration:\nresource \u0026#34;aws_secretsmanager_secret\u0026#34; \u0026#34;db_credentials\u0026#34; { name = \u0026#34;${var.service_name}/${var.environment}/db-credentials\u0026#34; tags = var.tags } # In the container definition container_definitions = jsonencode([{ name = var.service_name image = var.image secrets = [ { name = \u0026#34;DATABASE_URL\u0026#34; valueFrom = aws_secretsmanager_secret.db_credentials.arn }, { name = \u0026#34;DJANGO_SECRET_KEY\u0026#34; valueFrom = \u0026#34;${aws_secretsmanager_secret.app_secrets.arn}:django_secret_key::\u0026#34; } ] }]) ECS injects the secret values as environment variables at task startup. The execution role needs secretsmanager:GetSecretValue permission on the specific secret ARNs.\nCloudWatch Logging with Sane Retention Every service gets a log group with a retention policy. Keeping logs forever is expensive and rarely useful:\nresource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;/ecs/${var.service_name}/${var.environment}\u0026#34; retention_in_days = 14 tags = var.tags } 14 days is usually enough for debugging. If you need longer retention for compliance, ship logs to S3 or OpenSearch.\nPutting It All Together Here\u0026rsquo;s the complete pattern for a new service:\nECR repository with lifecycle policy (keep last 10 images) ECS cluster with Container Insights enabled Task definition with ARM64, proper resource limits, secrets injection ECS service with circuit breaker, Spot strategy, auto-scaling ALB with HTTPS (TLS 1.3), path-based routing WAF with rate limiting and AWS managed rules Aurora Serverless v2 with environment-appropriate scaling Route53 private hosted zone record CloudWatch log group with 14-day retention Security groups with three-tier model (ALB -\u0026gt; ECS -\u0026gt; Aurora) Once you have this as a set of Terraform modules, deploying a new service is just composing the modules with service-specific variables. The infrastructure is consistent, secure, and cost-optimized across all environments.\nReferences ECS Fargate Capacity Providers Deployment Circuit Breaker AWS Graviton Getting Started Aurora Serverless v2 ","permalink":"https://manuelfedele.github.io/posts/ecs-fargate-production-patterns/","summary":"\u003ch1 id=\"ecs-fargate-production-patterns-that-actually-work\"\u003eECS Fargate Production Patterns That Actually Work\u003c/h1\u003e\n\u003cp\u003eI\u0026rsquo;ve deployed and managed many containerized services on ECS Fargate. Over time, a set of patterns has emerged that I apply consistently to every new service. This post documents those patterns with Terraform examples, covering everything from Fargate Spot strategies to deployment circuit breakers and ARM64 migration.\u003c/p\u003e\n\u003ch2 id=\"the-standard-architecture\"\u003eThe Standard Architecture\u003c/h2\u003e\n\u003cp\u003eEvery service I deploy follows the same high-level architecture:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eInternet/VPC -\u0026gt; ALB (HTTPS, TLS 1.3) -\u0026gt; ECS Fargate -\u0026gt; Aurora PostgreSQL Serverless v2\n                 |\n                WAF (rate limiting + AWS managed rules)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eEach component has its own security group, with traffic flowing only from the layer above. The ALB sits in private subnets (no public-facing services), and Route53 private hosted zones handle internal DNS.\u003c/p\u003e","title":"ECS Fargate Production Patterns That Actually Work"},{"content":"Building an AI-Powered Document Processing Pipeline on AWS Insurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.\nIn this post I\u0026rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.\nThe Problem Space A typical insurance claim arrives as a ZIP archive containing multiple documents: a police report, medical certificates, repair estimates, photos, correspondence. Each document needs to be:\nParsed into machine-readable text (many are scanned PDFs or photos) Classified by type (police report, medical report, invoice, etc.) Structured by extracting specific fields per document type (e.g., license plate from a police report, diagnosis code from a medical report) Indexed for semantic search so case managers can query across all documents in natural language Summarized so the case manager gets an overview without reading every page The system needs to handle multiple lines of business (motor, health, accident), support concurrent processing of hundreds of claims, and recover gracefully from LLM failures.\nArchitecture Overview The architecture follows an event-driven, fully serverless pattern:\nZIP Upload → S3 → EventBridge → Step Functions → [Lambda Pipeline] → DynamoDB + Aurora ↓ Claims Service (ECS) ← ALB ← Users The key components:\nS3 Ingestion Bucket: receives ZIP uploads via presigned URLs EventBridge Rule: triggers the processing pipeline when a file lands Step Functions: orchestrates the multi-stage processing workflow Lambda Functions: execute each processing stage (stateless, parallel) DynamoDB: tracks processing state and stores extraction results Aurora PostgreSQL: stores vector embeddings for RAG ECS Service (Django): serves the web UI and API for case managers Keycloak + Entra ID: authentication and group-based access control The Ingestion Flow Documents enter the system through a secure upload flow:\nThe external claims management system calls our API to register a claim (line of business, permissions, metadata) It requests a presigned S3 URL from a lightweight Lambda function It uploads the ZIP archive directly to S3 using the presigned URL An EventBridge rule detects the new object and triggers the Step Functions workflow The presigned URL approach avoids routing large files through our API. The Lambda generates short-lived URLs (10-60 seconds), so the upload window is narrow enough to prevent misuse.\nimport boto3 from datetime import datetime def handler(event, context): s3 = boto3.client(\u0026#34;s3\u0026#34;) claim_name = event[\u0026#34;claim_name\u0026#34;] document_code = event[\u0026#34;document_code\u0026#34;] key = f\u0026#34;landing/{claim_name}#{document_code}.zip\u0026#34; url = s3.generate_presigned_url( \u0026#34;put_object\u0026#34;, Params={ \u0026#34;Bucket\u0026#34;: INGESTION_BUCKET, \u0026#34;Key\u0026#34;: key, \u0026#34;ContentType\u0026#34;: \u0026#34;application/zip\u0026#34;, }, ExpiresIn=60, ) return {\u0026#34;upload_url\u0026#34;: url, \u0026#34;key\u0026#34;: key} Processing runs in periodic batches (every 10 minutes) rather than on each individual upload. This is a deliberate tradeoff: we accept a few minutes of latency in exchange for better throughput management and cost control when handling bursts of uploads.\nThe Step Functions Pipeline The core of the system is an AWS Step Functions state machine that orchestrates document processing through a linear phase followed by two parallel branches.\nLinear Phase: Parse and Extract Text Stage 1: Start (claim-level)\nThe first Lambda validates the uploaded ZIP, extracts files, uploads individual documents to a support bucket, and creates tracking records in DynamoDB. It also deduplicates: if a document with the same name already exists in the claim and is being processed, it\u0026rsquo;s skipped.\nStage 2: Split (document-level)\nEach document is converted to PDF (if it\u0026rsquo;s a DOCX), then split into individual pages saved as PNG images. We use an internal PDF splitting service for this, which handles edge cases like encrypted PDFs, malformed page trees, and oversized documents.\nStage 3: Parse (page-level)\nThis is where the AI kicks in. Each page image is sent to Claude 3.5 Sonnet (via Amazon Bedrock) for vision-based text extraction. The LLM reads the image and produces clean Markdown text, handling handwritten notes, stamps, tables, and mixed layouts that traditional OCR tools struggle with.\nimport json import base64 import boto3 def parse_page(image_bytes: bytes, page_number: int) -\u0026gt; str: bedrock = boto3.client(\u0026#34;bedrock-runtime\u0026#34;) response = bedrock.invoke_model( modelId=\u0026#34;anthropic.claude-3-5-sonnet-20240620-v1:0\u0026#34;, body=json.dumps({ \u0026#34;anthropic_version\u0026#34;: \u0026#34;bedrock-2023-05-31\u0026#34;, \u0026#34;max_tokens\u0026#34;: 4096, \u0026#34;messages\u0026#34;: [{ \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;image\u0026#34;, \u0026#34;source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;base64\u0026#34;, \u0026#34;media_type\u0026#34;: \u0026#34;image/png\u0026#34;, \u0026#34;data\u0026#34;: base64.b64encode(image_bytes).decode(), }, }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Extract all text from this document page. \u0026#34; \u0026#34;Preserve the structure using Markdown formatting. \u0026#34; \u0026#34;Include tables, headers, and any handwritten text.\u0026#34;, }, ], }], }), ) result = json.loads(response[\u0026#34;body\u0026#34;].read()) return result[\u0026#34;content\u0026#34;][0][\u0026#34;text\u0026#34;] Why Claude 3.5 Sonnet for OCR instead of Amazon Textract? Two reasons: (1) vision LLMs handle messy real-world documents (stamps, handwriting, mixed layouts) significantly better than traditional OCR, and (2) the output is already structured Markdown, which downstream stages can work with directly without an intermediate parsing step.\nStage 4: Page Checker (document-level)\nLLM calls fail. Rate limits, timeouts, transient errors. The Page Checker implements retry logic: it collects results from all page parsing calls, identifies failures, and re-dispatches failed pages up to 3 times. This is essential when processing documents with dozens of pages, where even a 1% failure rate means most documents would have at least one failed page.\ndef check_pages(document_id: str, page_results: list[dict]) -\u0026gt; dict: successful = [p for p in page_results if p[\u0026#34;status\u0026#34;] == \u0026#34;success\u0026#34;] failed = [p for p in page_results if p[\u0026#34;status\u0026#34;] == \u0026#34;error\u0026#34;] retryable = [ p for p in failed if p.get(\u0026#34;retry_count\u0026#34;, 0) \u0026lt; MAX_RETRIES ] if retryable: # Re-dispatch failed pages for another attempt for page in retryable: page[\u0026#34;retry_count\u0026#34;] = page.get(\u0026#34;retry_count\u0026#34;, 0) + 1 return {\u0026#34;status\u0026#34;: \u0026#34;retry\u0026#34;, \u0026#34;pages\u0026#34;: retryable} if not successful: return {\u0026#34;status\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All pages failed parsing\u0026#34;} # Proceed with whatever pages succeeded return { \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;parsed_pages\u0026#34;: len(successful), \u0026#34;failed_pages\u0026#34;: len(failed), } Upper Branch: Embedding Pipeline After text extraction, the pipeline forks into two parallel branches. The upper branch creates vector embeddings for semantic search.\nChunker: Combines all page texts into a single document, then splits it into overlapping chunks with metadata (page numbers, positions). Each chunk is saved to S3.\nEmbedder: Each chunk is embedded using OpenAI\u0026rsquo;s text-embedding-ada-002 model. We chose ada-002 for its good balance of quality and cost at scale. Embeddings are stored in Aurora PostgreSQL using the pgvector extension, enabling similarity search across all documents in a claim.\nThis powers a RAG (Retrieval-Augmented Generation) interface where case managers can ask questions like \u0026ldquo;What was the estimated repair cost?\u0026rdquo; and get answers grounded in the actual documents.\nLower Branch: Classification and Extraction Pipeline The lower branch handles structured information extraction.\nClusterization: A single insurance document PDF often contains multiple logical sections (a police report followed by a medical certificate followed by repair photos). The clusterization stage identifies contiguous page ranges that belong to the same topic. We use Gemini 2.0 Flash for this because it\u0026rsquo;s fast, cheap, and performs well on the classification-style reasoning this step requires.\ndef clusterize_document(pages: list[dict]) -\u0026gt; list[dict]: \u0026#34;\u0026#34;\u0026#34;Identify clusters of pages about the same topic.\u0026#34;\u0026#34;\u0026#34; # Build context from all pages page_summaries = \u0026#34;\\n\u0026#34;.join( f\u0026#34;Page {p[\u0026#39;number\u0026#39;]}: {p[\u0026#39;text\u0026#39;][:200]}...\u0026#34; for p in pages ) prompt = f\u0026#34;\u0026#34;\u0026#34;Analyze these document pages and identify clusters of consecutive pages that discuss the same topic. Pages: {page_summaries} Return a JSON array of clusters, each with: - start_page: first page number - end_page: last page number - topic: brief description of what this section covers \u0026#34;\u0026#34;\u0026#34; # Call Gemini 2.0 Flash via the LLM Gateway response = llm_gateway.invoke( provider=\u0026#34;vertex\u0026#34;, model=\u0026#34;gemini-2.0-flash\u0026#34;, prompt=prompt, ) return json.loads(response) Classification: Each cluster is labeled with a document type (police report, medical certificate, invoice, repair estimate, etc.). The classification model uses the cluster\u0026rsquo;s text content and the claim\u0026rsquo;s line of business to assign the most appropriate label.\nExtraction: This is where it gets domain-specific. Based on the classification label, the extraction stage applies a tailored extraction strategy. A police report gets license plate, driver name, date, and location extracted. A medical report gets diagnosis, treatment, and provider extracted. An invoice gets line items and totals extracted.\nThe results are stored as structured JSON in DynamoDB, making them queryable and displayable in the web UI.\nCluster Aggregator: Collects results from all clusters, validates completeness, and updates the document\u0026rsquo;s processing status.\nThe Multi-Model LLM Strategy One of the most interesting architectural decisions was using three different LLM providers, each selected for a specific task:\nTask Model Why Text extraction (OCR) Claude 3.5 Sonnet (Bedrock) Best vision capabilities for messy documents Embeddings text-embedding-ada-002 (OpenAI) Cost-effective, high-quality embeddings at scale Clusterization/Classification Gemini 2.0 Flash (Vertex AI) Fast and cheap for reasoning tasks All LLM calls go through an internal gateway service that abstracts the provider differences. The gateway handles authentication, rate limiting, usage tracking, and fallback logic. From the pipeline\u0026rsquo;s perspective, it\u0026rsquo;s just calling an API with a provider and model name.\nThis multi-model approach lets us optimize for cost and quality per task rather than being locked into a single provider. The text extraction stage is the most expensive (vision + large context), so we use the best model available. The clusterization stage processes much less data and needs speed more than depth, so we use a fast, cheap model.\nConcurrency and Error Handling The Step Functions state machine uses Map states to process documents and pages in parallel. A single claim might contain 20 documents, each with 50 pages, resulting in 1,000 parallel page-processing Lambda invocations.\nKey patterns:\nIdempotency: Every Lambda function is idempotent. If a Step Functions execution is retried (due to a transient error), re-processing the same input produces the same result without side effects. We use DynamoDB conditional writes to prevent duplicate processing.\nGraceful degradation: If the embedding branch fails for a document, the classification branch still completes (and vice versa). A document with failed embeddings can still have its structured data extracted. The system tracks partial success at every level.\nCorrelation tracking: Every request gets a correlation ID that flows through all Lambda invocations, S3 objects, and DynamoDB records. When something fails, you can trace the entire processing chain from upload to the specific failed step.\nThe Claims Service The web UI is a Django application running on ECS Fargate, integrated as a microfrontend into the larger analytics platform. Case managers can:\nBrowse claims and their documents View extracted text alongside the original document images See structured extraction results (fields, values, confidence) Search across all documents using natural language (RAG) Generate claim summaries on demand Track document processing status in real time Authentication uses Keycloak with JWT tokens. Authorization uses Entra ID groups: each claim is associated with a visibility group, and only members of that group can access the claim\u0026rsquo;s documents. A daily sync job keeps group membership current.\nPerformance and Cost Some numbers from production:\nProcessing time: a 30-page document takes approximately 3-5 minutes end-to-end (dominated by LLM call latency) Throughput: the system handles 500+ documents per hour during peak periods Cost per document: roughly EUR 0.15-0.30, depending on page count and complexity (the bulk of the cost is LLM inference) Infrastructure cost when idle: near zero (serverless) The batch processing approach (every 10 minutes) means we can process multiple documents from the same claim together, which is more efficient than processing each upload individually.\nLessons Learned Vision LLMs are production-ready for OCR. Claude 3.5 Sonnet handles real-world insurance documents (stamps, handwriting, poor scans, mixed languages) far better than traditional OCR. The quality improvement justified the higher per-page cost.\nStep Functions are the right tool for document pipelines. The built-in retry logic, parallel Map states, error handling, and visual debugging make Step Functions ideal for multi-stage document processing. We tried orchestrating with SQS queues initially, but the complexity of tracking state across stages was not worth it.\nMulti-model is the way forward. No single LLM is best at everything. Using Claude for vision, OpenAI for embeddings, and Gemini for fast classification gave us the best cost-quality tradeoff at each stage.\nRetry logic is not optional. LLM APIs fail more often than traditional APIs. Rate limits, timeouts, model overload. The Page Checker retry pattern (up to 3 attempts per page) is what makes the pipeline reliable enough for production.\nBatch over real-time when you can. Processing uploads every 10 minutes instead of immediately simplified the architecture significantly and reduced costs. For insurance claims processing, a few minutes of latency is perfectly acceptable.\nWhat\u0026rsquo;s Next The natural evolution is closing the loop: having the AI agent not just extract and classify documents, but also suggest claim decisions based on the extracted data and historical patterns. This moves from \u0026ldquo;AI assists the human\u0026rdquo; to \u0026ldquo;AI proposes, human approves,\u0026rdquo; which is the next frontier for insurance automation.\nReferences AWS Step Functions Claude Vision Capabilities pgvector for PostgreSQL Amazon Bedrock ","permalink":"https://manuelfedele.github.io/posts/building-ai-document-processing-pipeline-aws/","summary":"\u003ch1 id=\"building-an-ai-powered-document-processing-pipeline-on-aws\"\u003eBuilding an AI-Powered Document Processing Pipeline on AWS\u003c/h1\u003e\n\u003cp\u003eInsurance companies process millions of documents every year: police reports, medical records, invoices, repair estimates. Traditionally, human operators read each document, classify it, extract the relevant fields, and enter the data into the claims system. This is slow, expensive, and error-prone.\u003c/p\u003e\n\u003cp\u003eIn this post I\u0026rsquo;ll describe the architecture of a production document processing pipeline I helped build. The system ingests claim documents, extracts text using vision-based LLMs, clusters and classifies document sections, extracts structured data, and generates vector embeddings for semantic search. All of this runs on a fully serverless AWS architecture with no idle infrastructure costs.\u003c/p\u003e","title":"Building an AI-Powered Document Processing Pipeline on AWS"},{"content":"Building Interactive CLI Tools in Go with Bubbletea If you\u0026rsquo;ve ever wanted to build a terminal application that feels more like a proper UI than a wall of text, the charmbracelet ecosystem is the way to go. I\u0026rsquo;ve been using it to build internal DevOps tools, and the developer experience is excellent. In this post, I\u0026rsquo;ll walk through building an interactive CLI tool using Bubbletea and Huh, the same libraries behind tools like gum and soft-serve.\nWhy Build TUI Tools? As a platform engineer, I spend a lot of time running repetitive operations across multiple AWS accounts: scanning for cost optimization opportunities, checking resource utilization, cleaning up unused infrastructure. A plain CLI with flags works, but when you have 100+ AWS profiles to choose from and a dozen scan categories, an interactive TUI makes the experience significantly better.\nThe alternative is a web dashboard, but that means deploying and maintaining another service. A Go binary with a TUI is a single file you can distribute to your team.\nThe Elm Architecture Bubbletea follows The Elm Architecture, which boils down to three concepts:\nModel - your application state Update - a function that handles messages and updates the model View - a function that renders the model as a string package main import ( \u0026#34;fmt\u0026#34; tea \u0026#34;github.com/charmbracelet/bubbletea\u0026#34; ) type model struct { profiles []string cursor int selected string } func (m model) Init() tea.Cmd { return nil } func (m model) Update(msg tea.Msg) (tea.Model, tea.Cmd) { switch msg := msg.(type) { case tea.KeyMsg: switch msg.String() { case \u0026#34;ctrl+c\u0026#34;, \u0026#34;q\u0026#34;: return m, tea.Quit case \u0026#34;up\u0026#34;, \u0026#34;k\u0026#34;: if m.cursor \u0026gt; 0 { m.cursor-- } case \u0026#34;down\u0026#34;, \u0026#34;j\u0026#34;: if m.cursor \u0026lt; len(m.profiles)-1 { m.cursor++ } case \u0026#34;enter\u0026#34;: m.selected = m.profiles[m.cursor] return m, tea.Quit } } return m, nil } func (m model) View() string { s := \u0026#34;Select an AWS profile:\\n\\n\u0026#34; for i, profile := range m.profiles { cursor := \u0026#34; \u0026#34; if m.cursor == i { cursor = \u0026#34;\u0026gt;\u0026#34; } s += fmt.Sprintf(\u0026#34;%s %s\\n\u0026#34;, cursor, profile) } s += \u0026#34;\\nPress q to quit.\\n\u0026#34; return s } This gives you a scrollable list with keyboard navigation. But we can do much better with the higher-level libraries.\nUsing Huh for Forms Huh provides pre-built form components (selects, multi-selects, inputs, confirms) that handle all the rendering and interaction logic for you. This is what I use for building the interactive parts of DevOps tools.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/charmbracelet/huh\u0026#34; ) func main() { var profile string var services []string var dryRun bool form := huh.NewForm( huh.NewGroup( huh.NewSelect[string](). Title(\u0026#34;AWS Profile\u0026#34;). Description(\u0026#34;Select the AWS account to scan\u0026#34;). Options( huh.NewOption(\u0026#34;production-eu\u0026#34;, \u0026#34;prod-eu\u0026#34;), huh.NewOption(\u0026#34;staging-eu\u0026#34;, \u0026#34;staging-eu\u0026#34;), huh.NewOption(\u0026#34;development-eu\u0026#34;, \u0026#34;dev-eu\u0026#34;), ). Value(\u0026amp;profile), huh.NewMultiSelect[string](). Title(\u0026#34;Services to Scan\u0026#34;). Description(\u0026#34;Choose which services to analyze\u0026#34;). Options( huh.NewOption(\u0026#34;ECS Fargate\u0026#34;, \u0026#34;ecs\u0026#34;), huh.NewOption(\u0026#34;Aurora Clusters\u0026#34;, \u0026#34;aurora\u0026#34;), huh.NewOption(\u0026#34;S3 Buckets\u0026#34;, \u0026#34;s3\u0026#34;), huh.NewOption(\u0026#34;EBS Volumes\u0026#34;, \u0026#34;ebs\u0026#34;), huh.NewOption(\u0026#34;CloudWatch Logs\u0026#34;, \u0026#34;cloudwatch\u0026#34;), huh.NewOption(\u0026#34;ElastiCache\u0026#34;, \u0026#34;elasticache\u0026#34;), ). Value(\u0026amp;services), huh.NewConfirm(). Title(\u0026#34;Dry Run?\u0026#34;). Description(\u0026#34;Preview changes without applying them\u0026#34;). Value(\u0026amp;dryRun), ), ) err := form.Run() if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } fmt.Printf(\u0026#34;Scanning %s for: %v (dry-run: %v)\\n\u0026#34;, profile, services, dryRun) } This gives you a polished, themed form with keyboard navigation, validation, and proper rendering out of the box.\nLoading AWS Profiles Dynamically In practice, you want to load profiles from ~/.aws/credentials rather than hardcoding them:\npackage main import ( \u0026#34;bufio\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/charmbracelet/huh\u0026#34; ) func loadAWSProfiles() ([]huh.Option[string], error) { home, err := os.UserHomeDir() if err != nil { return nil, err } file, err := os.Open(filepath.Join(home, \u0026#34;.aws\u0026#34;, \u0026#34;credentials\u0026#34;)) if err != nil { return nil, err } defer file.Close() var options []huh.Option[string] scanner := bufio.NewScanner(file) for scanner.Scan() { line := strings.TrimSpace(scanner.Text()) if strings.HasPrefix(line, \u0026#34;[\u0026#34;) \u0026amp;\u0026amp; strings.HasSuffix(line, \u0026#34;]\u0026#34;) { profile := line[1 : len(line)-1] options = append(options, huh.NewOption(profile, profile)) } } return options, scanner.Err() } Running Scans with a Spinner Once the user selects their options, you want to show progress. Bubbletea\u0026rsquo;s spinner component works well here:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/charmbracelet/bubbles/spinner\u0026#34; tea \u0026#34;github.com/charmbracelet/bubbletea\u0026#34; \u0026#34;github.com/charmbracelet/lipgloss\u0026#34; ) type scanModel struct { spinner spinner.Model scanning bool results []ScanResult current string } type ScanResult struct { Service string ResourceCount int EstimatedSaving float64 } type scanDoneMsg struct { results []ScanResult } func newScanModel(services []string) scanModel { s := spinner.New() s.Spinner = spinner.Dot s.Style = lipgloss.NewStyle().Foreground(lipgloss.Color(\u0026#34;205\u0026#34;)) return scanModel{ spinner: s, scanning: true, current: services[0], } } func (m scanModel) Init() tea.Cmd { return tea.Batch(m.spinner.Tick, runScans()) } func (m scanModel) Update(msg tea.Msg) (tea.Model, tea.Cmd) { switch msg := msg.(type) { case scanDoneMsg: m.scanning = false m.results = msg.results return m, tea.Quit case spinner.TickMsg: var cmd tea.Cmd m.spinner, cmd = m.spinner.Update(msg) return m, cmd } return m, nil } func (m scanModel) View() string { if m.scanning { return fmt.Sprintf(\u0026#34;%s Scanning %s...\\n\u0026#34;, m.spinner.View(), m.current) } s := \u0026#34;Scan Results:\\n\\n\u0026#34; totalSaving := 0.0 for _, r := range m.results { s += fmt.Sprintf(\u0026#34; %s: %d resources, estimated saving: EUR %.2f/month\\n\u0026#34;, r.Service, r.ResourceCount, r.EstimatedSaving) totalSaving += r.EstimatedSaving } s += fmt.Sprintf(\u0026#34;\\n Total estimated saving: EUR %.2f/month (EUR %.2f/year)\\n\u0026#34;, totalSaving, totalSaving*12) return s } func runScans() tea.Cmd { return func() tea.Msg { // Your actual AWS scanning logic goes here time.Sleep(2 * time.Second) // Simulated scan return scanDoneMsg{ results: []ScanResult{ {Service: \u0026#34;ECS Fargate\u0026#34;, ResourceCount: 12, EstimatedSaving: 340.50}, {Service: \u0026#34;EBS Volumes\u0026#34;, ResourceCount: 8, EstimatedSaving: 120.00}, {Service: \u0026#34;CloudWatch Logs\u0026#34;, ResourceCount: 45, EstimatedSaving: 85.30}, }, } } } Parallel Scanning with Goroutines For a real tool, you want to scan multiple services in parallel:\nfunc scanServices(profile string, services []string) tea.Cmd { return func() tea.Msg { results := make(chan ScanResult, len(services)) var wg sync.WaitGroup cfg, err := config.LoadDefaultConfig(context.Background(), config.WithSharedConfigProfile(profile), ) if err != nil { return scanErrorMsg{err: err} } for _, svc := range services { wg.Add(1) go func(service string) { defer wg.Done() result := scanService(cfg, service) results \u0026lt;- result }(svc) } go func() { wg.Wait() close(results) }() var allResults []ScanResult for r := range results { allResults = append(allResults, r) } return scanDoneMsg{results: allResults} } } Rendering Tables with Lipgloss For the final output, Lipgloss lets you style terminal output with a CSS-like API:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/charmbracelet/lipgloss\u0026#34; \u0026#34;github.com/charmbracelet/lipgloss/table\u0026#34; ) func renderResults(results []ScanResult) string { headerStyle := lipgloss.NewStyle(). Bold(true). Foreground(lipgloss.Color(\u0026#34;205\u0026#34;)) rows := [][]string{} totalSaving := 0.0 for _, r := range results { rows = append(rows, []string{ r.Service, fmt.Sprintf(\u0026#34;%d\u0026#34;, r.ResourceCount), fmt.Sprintf(\u0026#34;EUR %.2f\u0026#34;, r.EstimatedSaving), }) totalSaving += r.EstimatedSaving } t := table.New(). Border(lipgloss.NormalBorder()). Headers(\u0026#34;Service\u0026#34;, \u0026#34;Resources\u0026#34;, \u0026#34;Est. Saving/month\u0026#34;). Rows(rows...) summary := headerStyle.Render( fmt.Sprintf(\u0026#34;\\nTotal: EUR %.2f/month (EUR %.2f/year)\u0026#34;, totalSaving, totalSaving*12)) return t.String() + \u0026#34;\\n\u0026#34; + summary } Distributing the Binary One of the best things about Go is the single-binary output. Build for your team\u0026rsquo;s platforms:\nGOOS=linux GOARCH=amd64 go build -o tool-linux-amd64 GOOS=darwin GOARCH=arm64 go build -o tool-darwin-arm64 GOOS=windows GOARCH=amd64 go build -o tool-windows-amd64.exe Or use GoReleaser to automate this with your CI pipeline.\nTakeaways After building several internal tools with this stack, here are my recommendations:\nUse Huh for forms, Bubbletea for custom interactions. Don\u0026rsquo;t build a custom select component when Huh already has one. Keep the TUI thin. The interactive part should only collect user input. The actual business logic (AWS API calls, scanning, etc.) should be in separate packages that can also be called non-interactively via flags. Add a --json flag. Other tools and scripts will want to consume your output. Test with tea.NewProgram options. Bubbletea supports testing by passing in a custom reader/writer. The charmbracelet ecosystem makes it easy to build tools that your team will actually enjoy using, instead of yet another script with 30 flags.\nReferences Bubbletea Huh Lipgloss Bubbles (pre-built components) The Elm Architecture ","permalink":"https://manuelfedele.github.io/posts/building-interactive-cli-tools-in-go-with-bubbletea/","summary":"\u003ch1 id=\"building-interactive-cli-tools-in-go-with-bubbletea\"\u003eBuilding Interactive CLI Tools in Go with Bubbletea\u003c/h1\u003e\n\u003cp\u003eIf you\u0026rsquo;ve ever wanted to build a terminal application that feels more like a proper UI than a wall of text, the \u003ca href=\"https://github.com/charmbracelet\"\u003echarmbracelet\u003c/a\u003e ecosystem is the way to go. I\u0026rsquo;ve been using it to build internal DevOps tools, and the developer experience is excellent. In this post, I\u0026rsquo;ll walk through building an interactive CLI tool using \u003ca href=\"https://github.com/charmbracelet/bubbletea\"\u003eBubbletea\u003c/a\u003e and \u003ca href=\"https://github.com/charmbracelet/huh\"\u003eHuh\u003c/a\u003e, the same libraries behind tools like \u003ccode\u003egum\u003c/code\u003e and \u003ccode\u003esoft-serve\u003c/code\u003e.\u003c/p\u003e","title":"Building Interactive CLI Tools in Go with Bubbletea"},{"content":"Managing Multi-Account AWS Infrastructure with Terraform Workspaces When you\u0026rsquo;re managing infrastructure across dozens of AWS accounts, you need patterns that scale. In this post I\u0026rsquo;ll share the approach I use to manage multi-account, multi-environment AWS infrastructure using Terraform workspaces, modular code, and a consistent tagging strategy.\nThe Problem Imagine this setup: you have multiple organizational scopes (teams, business units, projects), each with their own AWS accounts for non-production and production. On top of that, your non-production account hosts multiple environments (dev, integration, certification). Multiply this by several countries or regions, and you\u0026rsquo;re looking at a lot of infrastructure to manage.\nThe naive approach of copy-pasting Terraform code per environment quickly becomes unmaintainable. You need a strategy that lets you define infrastructure once and deploy it consistently across all environments.\nWorkspace-Based Environment Separation Terraform workspaces are the foundation of this approach. Each workspace maps to an environment tier:\n# terraform.tf terraform { required_version = \u0026#34;\u0026gt;= 1.5\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } backend \u0026#34;s3\u0026#34; {} } We use partial backend configuration with .hcl files per environment:\n# backend-qual.hcl bucket = \u0026#34;my-scope-terraform-state-qual\u0026#34; key = \u0026#34;my-service/terraform.tfstate\u0026#34; region = \u0026#34;eu-central-1\u0026#34; dynamodb_table = \u0026#34;terraform-lock\u0026#34; encrypt = true # backend-prod.hcl bucket = \u0026#34;my-scope-terraform-state-prod\u0026#34; key = \u0026#34;my-service/terraform.tfstate\u0026#34; region = \u0026#34;eu-central-1\u0026#34; dynamodb_table = \u0026#34;terraform-lock\u0026#34; encrypt = true Initialize with the appropriate backend:\nterraform init -backend-config=backend-qual.hcl terraform workspace select qual || terraform workspace new qual terraform init -backend-config=backend-prod.hcl terraform workspace select prod || terraform workspace new prod Environment-Specific Variables with Lookup Maps Instead of separate .tfvars files, I use lookup maps keyed by terraform.workspace. This keeps everything in one place and makes differences between environments immediately visible:\n# locals.tf locals { environment = terraform.workspace # ECS configuration per environment ecs_cpu = lookup({ qual = 512 prod = 1024 }, terraform.workspace, 512) ecs_memory = lookup({ qual = 1024 prod = 2048 }, terraform.workspace, 1024) # Aurora Serverless v2 scaling aurora_min_acu = lookup({ qual = 0.5 prod = 1 }, terraform.workspace, 0.5) aurora_max_acu = lookup({ qual = 2 prod = 4 }, terraform.workspace, 2) # Fargate capacity provider strategy fargate_spot_weight = lookup({ qual = 4 prod = 1 }, terraform.workspace, 4) fargate_ondemand_weight = lookup({ qual = 1 prod = 4 }, terraform.workspace, 1) # Common tags applied to all resources common_tags = { Environment = local.environment Project = var.project_name ManagedBy = \u0026#34;terraform\u0026#34; Team = \u0026#34;platform\u0026#34; } } This pattern makes it easy to see at a glance how environments differ. Non-production gets smaller instances and more Spot capacity; production gets larger instances and more On-Demand stability.\nModular Infrastructure Each infrastructure concern lives in its own module:\nterraform/ modules/ alb/ aurora/ cloudwatch/ ecr/ ecs/ route53/ security-groups/ waf/ main.tf locals.tf terraform.tf backend-qual.hcl backend-prod.hcl The root module composes them:\n# main.tf module \u0026#34;ecr\u0026#34; { source = \u0026#34;./modules/ecr\u0026#34; service_name = var.service_name tags = local.common_tags } module \u0026#34;security_groups\u0026#34; { source = \u0026#34;./modules/security-groups\u0026#34; vpc_id = data.aws_vpc.main.id vpc_cidr = data.aws_vpc.main.cidr_block tags = local.common_tags } module \u0026#34;alb\u0026#34; { source = \u0026#34;./modules/alb\u0026#34; service_name = var.service_name vpc_id = data.aws_vpc.main.id subnet_ids = data.aws_subnets.private.ids security_group_id = module.security_groups.alb_sg_id certificate_arn = data.aws_acm_certificate.main.arn tags = local.common_tags } module \u0026#34;ecs\u0026#34; { source = \u0026#34;./modules/ecs\u0026#34; service_name = var.service_name cpu = local.ecs_cpu memory = local.ecs_memory image = \u0026#34;${module.ecr.repository_url}:latest\u0026#34; target_group_arn = module.alb.target_group_arn security_group_id = module.security_groups.ecs_sg_id subnet_ids = data.aws_subnets.private.ids spot_weight = local.fargate_spot_weight ondemand_weight = local.fargate_ondemand_weight tags = local.common_tags } module \u0026#34;aurora\u0026#34; { source = \u0026#34;./modules/aurora\u0026#34; cluster_name = \u0026#34;${var.service_name}-${local.environment}\u0026#34; engine_version = \u0026#34;16.6\u0026#34; min_acu = local.aurora_min_acu max_acu = local.aurora_max_acu vpc_id = data.aws_vpc.main.id subnet_ids = data.aws_subnets.database.ids security_group_id = module.security_groups.aurora_sg_id tags = local.common_tags } The Three-Tier Security Group Pattern Every service follows the same layered security model:\n# modules/security-groups/main.tf resource \u0026#34;aws_security_group\u0026#34; \u0026#34;alb\u0026#34; { name_prefix = \u0026#34;${var.service_name}-alb-\u0026#34; vpc_id = var.vpc_id ingress { description = \u0026#34;HTTPS from VPC\u0026#34; from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [var.vpc_cidr] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = var.tags } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;ecs\u0026#34; { name_prefix = \u0026#34;${var.service_name}-ecs-\u0026#34; vpc_id = var.vpc_id ingress { description = \u0026#34;Traffic from ALB\u0026#34; from_port = var.container_port to_port = var.container_port protocol = \u0026#34;tcp\u0026#34; security_groups = [aws_security_group.alb.id] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = var.tags } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;aurora\u0026#34; { name_prefix = \u0026#34;${var.service_name}-aurora-\u0026#34; vpc_id = var.vpc_id ingress { description = \u0026#34;PostgreSQL from ECS\u0026#34; from_port = 5432 to_port = 5432 protocol = \u0026#34;tcp\u0026#34; security_groups = [aws_security_group.ecs.id] } tags = var.tags } The key principle: each layer only accepts traffic from the layer above it. The ALB accepts HTTPS from the VPC, ECS accepts traffic only from the ALB security group, and Aurora accepts connections only from the ECS security group. No hardcoded CIDRs between tiers.\nIAM with Permissions Boundaries In an enterprise multi-account setup, you typically have a governance layer that constrains what each scope can do. Permissions boundaries are the mechanism:\nresource \u0026#34;aws_iam_role\u0026#34; \u0026#34;ecs_task\u0026#34; { name = \u0026#34;${var.service_name}-ecs-task-${local.environment}\u0026#34; assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [{ Action = \u0026#34;sts:AssumeRole\u0026#34; Effect = \u0026#34;Allow\u0026#34; Principal = { Service = \u0026#34;ecs-tasks.amazonaws.com\u0026#34; } }] }) permissions_boundary = data.aws_iam_policy.scope_boundary.arn tags = local.common_tags } Every IAM role gets the scope\u0026rsquo;s permissions boundary attached. This ensures that even if a role policy is overly permissive, it can\u0026rsquo;t exceed what the organizational scope allows. The boundary is managed by a central governance team, not by individual project teams.\nWAF for Rate Limiting Every ALB gets a WAF with at least rate limiting and the AWS managed common rule set:\n# modules/waf/main.tf resource \u0026#34;aws_wafv2_web_acl\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;${var.service_name}-waf-${var.environment}\u0026#34; scope = \u0026#34;REGIONAL\u0026#34; default_action { allow {} } rule { name = \u0026#34;rate-limit\u0026#34; priority = 1 action { block {} } statement { rate_based_statement { limit = var.rate_limit aggregate_key_type = \u0026#34;IP\u0026#34; } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;${var.service_name}-rate-limit\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;aws-common-rules\u0026#34; priority = 2 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesCommonRuleSet\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;${var.service_name}-common-rules\u0026#34; sampled_requests_enabled = true } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;${var.service_name}-waf\u0026#34; sampled_requests_enabled = true } tags = var.tags } resource \u0026#34;aws_wafv2_web_acl_association\u0026#34; \u0026#34;main\u0026#34; { resource_arn = var.alb_arn web_acl_arn = aws_wafv2_web_acl.main.arn } CI/CD Integration The GitLab CI pipeline follows a promotion flow. A commit to master triggers a plan; merging to release triggers apply:\nstages: - init - plan - apply variables: TF_PLUGIN_CACHE_DIR: \u0026#34;$CI_PROJECT_DIR/.terraform-plugins\u0026#34; cache: key: terraform-plugins paths: - .terraform-plugins/ .terraform_base: image: hashicorp/terraform:1.5 before_script: - terraform init -backend-config=backend-${ENVIRONMENT}.hcl - terraform workspace select ${ENVIRONMENT} || terraform workspace new ${ENVIRONMENT} plan:qual: extends: .terraform_base stage: plan variables: ENVIRONMENT: qual script: - terraform plan -out=plan.tfplan artifacts: paths: - plan.tfplan rules: - if: $CI_COMMIT_BRANCH == \u0026#34;master\u0026#34; apply:qual: extends: .terraform_base stage: apply variables: ENVIRONMENT: qual script: - terraform apply plan.tfplan dependencies: - plan:qual rules: - if: $CI_COMMIT_BRANCH == \u0026#34;release\u0026#34; when: manual plan:prod: extends: .terraform_base stage: plan variables: ENVIRONMENT: prod script: - terraform plan -out=plan.tfplan artifacts: paths: - plan.tfplan rules: - if: $CI_COMMIT_BRANCH == \u0026#34;release\u0026#34; apply:prod: extends: .terraform_base stage: apply variables: ENVIRONMENT: prod script: - terraform apply plan.tfplan dependencies: - plan:prod rules: - if: $CI_COMMIT_BRANCH == \u0026#34;release\u0026#34; when: manual Caching the Terraform plugin directory significantly speeds up pipeline runs when you have large provider downloads.\nLessons Learned After managing this pattern across many scopes and projects, here\u0026rsquo;s what I\u0026rsquo;ve found works well:\nWorkspaces over directories. Having separate directories per environment leads to drift. Workspaces with lookup maps keep a single source of truth.\nModules with opinions. Each module should embed best practices (deployment circuit breakers, Container Insights, log retention policies) rather than exposing every knob. If 90% of services need the same config, make it the default.\nTag everything. Consistent tagging across all resources is what makes cost allocation, compliance reporting, and automated cleanup possible at scale.\nPermissions boundaries are non-negotiable. In a multi-team enterprise, you need guardrails. Permissions boundaries let teams self-serve within safe limits.\nPlan before apply, always. Even in non-production. A Terraform plan that shows 47 resources being destroyed is a lot cheaper to review than to recover from.\nReferences Terraform Workspaces Documentation AWS Well-Architected Framework - Multi-Account Strategy Terraform Module Best Practices ","permalink":"https://manuelfedele.github.io/posts/multi-account-aws-terraform-workspaces/","summary":"\u003ch1 id=\"managing-multi-account-aws-infrastructure-with-terraform-workspaces\"\u003eManaging Multi-Account AWS Infrastructure with Terraform Workspaces\u003c/h1\u003e\n\u003cp\u003eWhen you\u0026rsquo;re managing infrastructure across dozens of AWS accounts, you need patterns that scale. In this post I\u0026rsquo;ll share the approach I use to manage multi-account, multi-environment AWS infrastructure using Terraform workspaces, modular code, and a consistent tagging strategy.\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eImagine this setup: you have multiple organizational scopes (teams, business units, projects), each with their own AWS accounts for non-production and production. On top of that, your non-production account hosts multiple environments (dev, integration, certification). Multiply this by several countries or regions, and you\u0026rsquo;re looking at a lot of infrastructure to manage.\u003c/p\u003e","title":"Managing Multi-Account AWS Infrastructure with Terraform Workspaces"},{"content":"Building a Chess Engine: From Position Evaluation to Search Techniques Chess engines are fascinating pieces of software that combine various computer science concepts: position evaluation, tree search, move generation, and optimization techniques. This guide will walk you through implementing a chess engine, with a particular focus on position evaluation and search strategies.\nPart 1: Basic Position Representation First, let\u0026rsquo;s implement a basic board representation. While FEN (Forsyth–Edwards Notation) is the standard for chess positions, we\u0026rsquo;ll use a more computation-friendly format internally.\npackage chess // Piece constants const ( Empty = iota Pawn Knight Bishop Rook Queen King ) // Color constants const ( White = 1 Black = -1 ) // Position represents a chess position type Position struct { Board [64]int // Piece type (0-6) Colors [64]int // Piece color (1 for white, -1 for black) ToMove int // Side to move CastlingRights int // Castling availability EnPassant int // En passant target square HalfMoveClock int // Halfmove clock for fifty-move rule FullMoveNumber int // Fullmove number } // Square converts file (a-h) and rank (1-8) to board index func Square(file byte, rank int) int { return (rank-1)*8 + int(file-\u0026#39;a\u0026#39;) } // NewPosition creates a starting chess position func NewPosition() *Position { pos := \u0026amp;Position{ ToMove: White, CastlingRights: 0xF, // All castling rights available } // Initialize starting position // Back rank pieces backRank := []int{Rook, Knight, Bishop, Queen, King, Bishop, Knight, Rook} for i, piece := range backRank { // White pieces pos.Board[i] = piece pos.Colors[i] = White // Black pieces pos.Board[i+56] = piece pos.Colors[i+56] = Black } // Pawns for i := 0; i \u0026lt; 8; i++ { // White pawns pos.Board[i+8] = Pawn pos.Colors[i+8] = White // Black pawns pos.Board[i+48] = Pawn pos.Colors[i+48] = Black } return pos } Part 2: Position Evaluation Position evaluation is crucial for a chess engine. We\u0026rsquo;ll implement several evaluation components:\nMaterial count Piece position tables Pawn structure evaluation King safety Mobility evaluation // Piece values in centipawns var pieceValues = map[int]int{ Pawn: 100, Knight: 320, Bishop: 330, Rook: 500, Queen: 900, King: 20000, } // Piece-Square tables for position evaluation var pawnTable = [64]int{ 0, 0, 0, 0, 0, 0, 0, 0, 50, 50, 50, 50, 50, 50, 50, 50, 10, 10, 20, 30, 30, 20, 10, 10, 5, 5, 10, 25, 25, 10, 5, 5, 0, 0, 0, 20, 20, 0, 0, 0, 5, -5,-10, 0, 0,-10, -5, 5, 5, 10, 10,-20,-20, 10, 10, 5, 0, 0, 0, 0, 0, 0, 0, 0, } // Similar tables for other pieces... type Evaluator struct { pos *Position stage GameStage } // Evaluate returns a position score in centipawns // Positive scores favor White, negative scores favor Black func (e *Evaluator) Evaluate() int { score := 0 // Material evaluation score += e.evaluateMaterial() // Piece-square table evaluation score += e.evaluatePiecePositions() // Pawn structure evaluation score += e.evaluatePawnStructure() // King safety score += e.evaluateKingSafety() // Mobility score += e.evaluateMobility() return score * e.pos.ToMove } func (e *Evaluator) evaluateMaterial() int { score := 0 for sq := 0; sq \u0026lt; 64; sq++ { if piece := e.pos.Board[sq]; piece != Empty { score += pieceValues[piece] * e.pos.Colors[sq] } } return score } func (e *Evaluator) evaluatePawnStructure() int { score := 0 // Evaluate doubled pawns for file := 0; file \u0026lt; 8; file++ { whitePawns := 0 blackPawns := 0 for rank := 0; rank \u0026lt; 8; rank++ { sq := rank*8 + file if e.pos.Board[sq] == Pawn { if e.pos.Colors[sq] == White { whitePawns++ } else { blackPawns++ } } } // Penalty for doubled pawns if whitePawns \u0026gt; 1 { score -= 20 * (whitePawns - 1) } if blackPawns \u0026gt; 1 { score += 20 * (blackPawns - 1) } } // Evaluate isolated pawns for file := 0; file \u0026lt; 8; file++ { hasWhitePawn := false hasBlackPawn := false for rank := 0; rank \u0026lt; 8; rank++ { sq := rank*8 + file if e.pos.Board[sq] == Pawn { if e.pos.Colors[sq] == White { hasWhitePawn = true } else { hasBlackPawn = true } } } if hasWhitePawn { isIsolated := true if file \u0026gt; 0 { // Check left file for pawns for rank := 0; rank \u0026lt; 8; rank++ { sq := rank*8 + (file-1) if e.pos.Board[sq] == Pawn \u0026amp;\u0026amp; e.pos.Colors[sq] == White { isIsolated = false break } } } if file \u0026lt; 7 \u0026amp;\u0026amp; isIsolated { // Check right file for pawns for rank := 0; rank \u0026lt; 8; rank++ { sq := rank*8 + (file+1) if e.pos.Board[sq] == Pawn \u0026amp;\u0026amp; e.pos.Colors[sq] == White { isIsolated = false break } } } if isIsolated { score -= 15 // Penalty for isolated pawn } } // Similar check for black pawns... } return score } func (e *Evaluator) evaluateKingSafety() int { score := 0 // Find king positions var whiteKingSq, blackKingSq int for sq := 0; sq \u0026lt; 64; sq++ { if e.pos.Board[sq] == King { if e.pos.Colors[sq] == White { whiteKingSq = sq } else { blackKingSq = sq } } } // Evaluate pawn shield score += e.evaluatePawnShield(whiteKingSq, White) score -= e.evaluatePawnShield(blackKingSq, Black) // Evaluate king tropism (enemy pieces\u0026#39; distance to king) score += e.evaluateKingTropism(whiteKingSq, blackKingSq) return score } func (e *Evaluator) evaluatePawnShield(kingSq, color int) int { score := 0 rank := kingSq / 8 file := kingSq % 8 // Check pawns in front of king pawnShieldSquares := []int{ Square(byte(\u0026#39;a\u0026#39;+file-1), rank+1), Square(byte(\u0026#39;a\u0026#39;+file), rank+1), Square(byte(\u0026#39;a\u0026#39;+file+1), rank+1), } for _, sq := range pawnShieldSquares { if sq \u0026gt;= 0 \u0026amp;\u0026amp; sq \u0026lt; 64 { if e.pos.Board[sq] == Pawn \u0026amp;\u0026amp; e.pos.Colors[sq] == color { score += 10 // Bonus for each pawn shield } } } return score } Part 3: Search Implementation Now let\u0026rsquo;s implement the search algorithm. We\u0026rsquo;ll use alpha-beta pruning with various enhancements:\nPrincipal Variation Search Quiescence Search Move Ordering Transposition Table type SearchInfo struct { Depth int NodesSearched int64 StartTime time.Time StopTime time.Time Stopped bool } type TranspositionEntry struct { Hash uint64 Depth int Score int Type int // EXACT, ALPHA, or BETA Move Move } const ( EXACT = iota ALPHA BETA ) func (e *Engine) Search(pos *Position, depth int) (bestMove Move, score int) { info := \u0026amp;SearchInfo{ Depth: depth, StartTime: time.Now(), StopTime: time.Now().Add(5 * time.Second), // 5 second time control } alpha := -infinity beta := infinity // Iterative deepening for currentDepth := 1; currentDepth \u0026lt;= depth; currentDepth++ { score = e.alphaBeta(pos, currentDepth, alpha, beta, info) // Check if search should be stopped if info.Stopped { break } // Get best move from transposition table bestMove = e.tt.GetMove(pos.Hash()) } return bestMove, score } func (e *Engine) alphaBeta(pos *Position, depth, alpha, beta int, info *SearchInfo) int { info.NodesSearched++ // Check for time if time.Now().After(info.StopTime) { info.Stopped = true return 0 } // Check transposition table if entry, found := e.tt.Get(pos.Hash()); found { if entry.Depth \u0026gt;= depth { if entry.Type == EXACT { return entry.Score } if entry.Type == ALPHA \u0026amp;\u0026amp; entry.Score \u0026lt;= alpha { return alpha } if entry.Type == BETA \u0026amp;\u0026amp; entry.Score \u0026gt;= beta { return beta } } } // Base case: evaluate position if depth == 0 { return e.quiescence(pos, alpha, beta, info) } moves := e.generateMoves(pos) moves = e.orderMoves(pos, moves) // Order moves for better pruning bestScore := -infinity for _, move := range moves { newPos := pos.MakeMove(move) score := -e.alphaBeta(newPos, depth-1, -beta, -alpha, info) if score \u0026gt; bestScore { bestScore = score // Update transposition table e.tt.Store(pos.Hash(), TranspositionEntry{ Hash: pos.Hash(), Depth: depth, Score: score, Type: EXACT, Move: move, }) } alpha = max(alpha, score) if alpha \u0026gt;= beta { break // Beta cutoff } } return bestScore } func (e *Engine) quiescence(pos *Position, alpha, beta int, info *SearchInfo) int { info.NodesSearched++ standPat := e.evaluate(pos) if standPat \u0026gt;= beta { return beta } if alpha \u0026lt; standPat { alpha = standPat } // Generate capturing moves only captures := e.generateCaptures(pos) captures = e.orderMoves(pos, captures) for _, move := range captures { newPos := pos.MakeMove(move) score := -e.quiescence(newPos, -beta, -alpha, info) if score \u0026gt;= beta { return beta } if score \u0026gt; alpha { alpha = score } } return alpha } func (e *Engine) orderMoves(pos *Position, moves []Move) []Move { // Score moves for ordering type ScoredMove struct { move Move score int } scoredMoves := make([]ScoredMove, len(moves)) for i, move := range moves { score := 0 // TTMove bonus if ttMove, found := e.tt.GetMove(pos.Hash()); found \u0026amp;\u0026amp; move == ttMove { score += 10000 } // MVV/LVA (Most Valuable Victim / Least Valuable Attacker) if move.IsCapture() { victim := pos.Board[move.To()] attacker := pos.Board[move.From()] score += pieceValues[victim] - pieceValues[attacker]/10 } // Killer move bonus if e.isKillerMove(move, pos.Ply()) { score += 900 } // History heuristic score += e.history[move.From()][move.To()] scoredMoves[i] = ScoredMove{move, score} } // Sort moves by score sort.Slice(scoredMoves, func(i, j int) bool { return scoredMoves[i].score \u0026gt; scoredMoves[j].score }) // Extract sorted moves sortedMoves := make([]Move, len(moves)) for i, sm := range scoredMoves { sortedMoves[i] = sm.move } return sortedMoves } Part 4: Advanced Evaluation Features Let\u0026rsquo;s enhance our evaluation with more sophisticated features:\nfunc (e *Evaluator) evaluateMobility() int { score := 0 // Piece mobility bonuses/penalties func (e *Evaluator) evaluateMobility() int { score := 0 // Mobility weights for each piece type mobilityWeights := map[int]int{ Knight: 4, Bishop: 5, Rook: 2, Queen: 1, } // Calculate mobility for both sides for sq := 0; sq \u0026lt; 64; sq++ { piece := e.pos.Board[sq] color := e.pos.Colors[sq] if piece == Empty || piece == Pawn || piece == King { continue } moves := e.generatePieceMoves(sq) mobilityScore := len(moves) * mobilityWeights[piece] score += mobilityScore * color } return score } // Helper function to check piece attacks func (e *Evaluator) isSquareAttacked(sq, byColor int) bool { // Check pawn attacks pawnDir := -byColor // Pawns move in opposite direction of their color for _, offset := range []int{7, 9} { attackSq := sq + pawnDir*8 + offset if attackSq \u0026gt;= 0 \u0026amp;\u0026amp; attackSq \u0026lt; 64 { if e.pos.Board[attackSq] == Pawn \u0026amp;\u0026amp; e.pos.Colors[attackSq] == byColor { return true } } } // Check knight attacks knightOffsets := []int{-17, -15, -10, -6, 6, 10, 15, 17} for _, offset := range knightOffsets { attackSq := sq + offset if attackSq \u0026gt;= 0 \u0026amp;\u0026amp; attackSq \u0026lt; 64 { if e.pos.Board[attackSq] == Knight \u0026amp;\u0026amp; e.pos.Colors[attackSq] == byColor { return true } } } // Check sliding piece attacks (Bishop, Rook, Queen) directions := [][]int{ {-1, -1}, {-1, 1}, {1, -1}, {1, 1}, // Bishop/Queen directions {-1, 0}, {1, 0}, {0, -1}, {0, 1}, // Rook/Queen directions } for _, dir := range directions { dx, dy := dir[0], dir[1] x, y := sq%8, sq/8 for i := 1; i \u0026lt; 8; i++ { newX, newY := x+dx*i, y+dy*i if newX \u0026lt; 0 || newX \u0026gt;= 8 || newY \u0026lt; 0 || newY \u0026gt;= 8 { break } attackSq := newY*8 + newX piece := e.pos.Board[attackSq] if piece != Empty { if e.pos.Colors[attackSq] == byColor { // Check if piece can move in this direction if piece == Queen || (piece == Rook \u0026amp;\u0026amp; (dx == 0 || dy == 0)) || (piece == Bishop \u0026amp;\u0026amp; dx != 0 \u0026amp;\u0026amp; dy != 0) { return true } } break } } } return false } Part 5: Advanced Search Techniques Let\u0026rsquo;s implement some advanced search techniques to improve the engine\u0026rsquo;s strength:\n// Null Move Pruning func (e *Engine) alphaBetaWithNullMove(pos *Position, depth, alpha, beta int, info *SearchInfo) int { if depth \u0026gt;= 3 \u0026amp;\u0026amp; !pos.InCheck() \u0026amp;\u0026amp; pos.HasNonPawnMaterial() { // Make null move newPos := pos.MakeNullMove() // Reduced depth for null move search R := 2 + depth/4 // Recursive search with reduced depth score := -e.alphaBeta(newPos, depth-R-1, -beta, -beta+1, info) if score \u0026gt;= beta { return beta // Null move cutoff } } // Continue with regular alpha-beta search... return e.alphaBeta(pos, depth, alpha, beta, info) } // Late Move Reduction func (e *Engine) alphaBetaWithLMR(pos *Position, depth, alpha, beta int, moveCount int, info *SearchInfo) int { if depth \u0026gt;= 3 \u0026amp;\u0026amp; moveCount \u0026gt;= 4 \u0026amp;\u0026amp; !pos.InCheck() { // Reduce depth for late moves reduction := 1 if moveCount \u0026gt;= 6 { reduction++ } // Reduced depth search score := -e.alphaBeta(pos, depth-reduction-1, -alpha-1, -alpha, info) // If reduced search fails high, do a full depth search if score \u0026gt; alpha { return -e.alphaBeta(pos, depth-1, -beta, -alpha, info) } return score } // Continue with regular search... return -e.alphaBeta(pos, depth-1, -beta, -alpha, info) } // Aspiration Windows func (e *Engine) searchWithAspirationWindows(pos *Position, depth int, info *SearchInfo) int { score := 0 alpha := -infinity beta := infinity window := 50 // Initial window size in centipawns // First search with full window score = e.alphaBeta(pos, depth, alpha, beta, info) // Subsequent searches with aspiration windows for currentDepth := 2; currentDepth \u0026lt;= depth; currentDepth++ { alpha = score - window beta = score + window score = e.alphaBeta(pos, currentDepth, alpha, beta, info) if score \u0026lt;= alpha || score \u0026gt;= beta { // Window failed, retry with larger window window *= 2 score = e.alphaBeta(pos, currentDepth, -infinity, infinity, info) } window = 50 // Reset window size for next iteration } return score } Part 6: Time Management Proper time management is crucial for tournament play:\ntype TimeManager struct { InitialTime time.Duration Increment time.Duration MovesToGo int MaxMoveTime time.Duration } func NewTimeManager(initialTime, increment time.Duration, movesToGo int) *TimeManager { return \u0026amp;TimeManager{ InitialTime: initialTime, Increment: increment, MovesToGo: movesToGo, MaxMoveTime: initialTime / 20, // Don\u0026#39;t use more than 5% of total time } } func (tm *TimeManager) allocateTime(pos *Position) time.Duration { // Basic time allocation baseTime := tm.InitialTime / time.Duration(tm.MovesToGo) // Adjust based on game phase gamePhase := evaluateGamePhase(pos) switch { case gamePhase \u0026lt; 0.3: // Opening baseTime *= 8/10 case gamePhase \u0026gt; 0.7: // Endgame baseTime *= 12/10 } // Add increment allocatedTime := baseTime + tm.Increment*8/10 // Never exceed maximum move time if allocatedTime \u0026gt; tm.MaxMoveTime { allocatedTime = tm.MaxMoveTime } return allocatedTime } func evaluateGamePhase(pos *Position) float64 { // Count material to determine game phase totalMaterial := 0 maxMaterial := 0 for sq := 0; sq \u0026lt; 64; sq++ { piece := pos.Board[sq] if piece != Empty \u0026amp;\u0026amp; piece != King { totalMaterial += pieceValues[piece] maxMaterial += pieceValues[piece] } } // Return value between 0 (opening) and 1 (endgame) return 1.0 - float64(totalMaterial)/float64(maxMaterial) } Part 7: Opening Book and Endgame Tablebases For completeness, let\u0026rsquo;s add support for opening books and endgame tablebases:\ntype OpeningBook struct { positions map[uint64][]BookMove } type BookMove struct { Move Move Weight int } func (book *OpeningBook) GetBookMove(pos *Position) (Move, bool) { moves, exists := book.positions[pos.Hash()] if !exists { return Move{}, false } // Select move based on weights totalWeight := 0 for _, m := range moves { totalWeight += m.Weight } // Random selection weighted by frequency r := rand.Intn(totalWeight) currentWeight := 0 for _, m := range moves { currentWeight += m.Weight if r \u0026lt; currentWeight { return m.Move, true } } return moves[0].Move, true } type Tablebase struct { cache map[uint64]TablebaseEntry } type TablebaseEntry struct { Score int BestMove Move DTM int // Distance to mate } func (tb *Tablebase) Probe(pos *Position) (TablebaseEntry, bool) { // Probe tablebase cache if entry, exists := tb.cache[pos.Hash()]; exists { return entry, true } // Only probe for positions with 6 or fewer pieces if pos.PieceCount() \u0026gt; 6 { return TablebaseEntry{}, false } // Here you would implement actual tablebase probing // This typically involves accessing Syzygy or Gaviota tablebases return TablebaseEntry{}, false } Conclusion Building a chess engine involves multiple complex components working together:\nBoard Representation: Efficient position storage and move generation Evaluation: Material, piece placement, pawn structure, king safety, and mobility Search: Alpha-beta pruning with enhancements (null move, LMR, aspiration windows) Time Management: Adaptive time allocation based on position and game phase Opening/Endgame: Book moves and tablebase access for perfect play Future improvements could include:\nNeural network evaluation Multi-threaded search Supervised learning from master games NNUE (Efficiently Updatable Neural Network) evaluation Advanced pruning techniques (Futility pruning, SEE, etc.) Contempt factor for tournament play Remember that chess programming is an iterative process. Start with the basics and gradually add more sophisticated features while testing against other engines to measure improvement.\nReferences Chess Programming Wiki Stockfish GitHub Repository \u0026ldquo;How Computers Play Chess\u0026rdquo; by David Levy and Monty Newborn \u0026ldquo;Chess Programming\u0026rdquo; by François-Dominic Laramée ","permalink":"https://manuelfedele.github.io/posts/evaluate-chess-position/","summary":"\u003ch1 id=\"building-a-chess-engine-from-position-evaluation-to-search-techniques\"\u003eBuilding a Chess Engine: From Position Evaluation to Search Techniques\u003c/h1\u003e\n\u003cp\u003eChess engines are fascinating pieces of software that combine various computer science concepts: position evaluation, tree search, move generation, and optimization techniques. This guide will walk you through implementing a chess engine, with a particular focus on position evaluation and search strategies.\u003c/p\u003e\n\u003ch2 id=\"part-1-basic-position-representation\"\u003ePart 1: Basic Position Representation\u003c/h2\u003e\n\u003cp\u003eFirst, let\u0026rsquo;s implement a basic board representation. While FEN (Forsyth–Edwards Notation) is the standard for chess positions, we\u0026rsquo;ll use a more computation-friendly format internally.\u003c/p\u003e","title":"Building a Chess Engine - From Position Evaluation to Search Techniques"},{"content":"Building a Secure JWT Issuer in Go: A Complete Guide JSON Web Tokens (JWT) have become the de facto standard for implementing stateless authentication in modern web applications. In this guide, we\u0026rsquo;ll implement a secure JWT issuer in Go, covering both basic implementation and advanced security considerations.\nUnderstanding JWT Basics A JWT consists of three parts: header, payload, and signature. These parts are Base64URL encoded and concatenated with dots. The signature ensures the token hasn\u0026rsquo;t been tampered with, while the payload carries the claims (data) we want to transmit securely.\nFirst, let\u0026rsquo;s create a basic JWT issuer that can generate tokens with custom claims.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/golang-jwt/jwt/v5\u0026#34; ) type Claims struct { UserID string `json:\u0026#34;uid\u0026#34;` Username string `json:\u0026#34;username\u0026#34;` Roles []string `json:\u0026#34;roles\u0026#34;` jwt.RegisteredClaims } type JWTIssuer struct { secretKey []byte duration time.Duration } func NewJWTIssuer(secretKey string, duration time.Duration) *JWTIssuer { return \u0026amp;JWTIssuer{ secretKey: []byte(secretKey), duration: duration, } } func (i *JWTIssuer) IssueToken(userID, username string, roles []string) (string, error) { now := time.Now() claims := Claims{ UserID: userID, Username: username, Roles: roles, RegisteredClaims: jwt.RegisteredClaims{ IssuedAt: jwt.NewNumericDate(now), ExpiresAt: jwt.NewNumericDate(now.Add(i.duration)), NotBefore: jwt.NewNumericDate(now), Issuer: \u0026#34;your-service-name\u0026#34;, }, } token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) return token.SignedString(i.secretKey) } Basic Usage Example Here\u0026rsquo;s how to use the basic JWT issuer:\nfunc main() { // Create a new issuer with a 24-hour token duration issuer := NewJWTIssuer(\u0026#34;your-secret-key\u0026#34;, 24*time.Hour) // Issue a token token, err := issuer.IssueToken( \u0026#34;user123\u0026#34;, \u0026#34;john.doe\u0026#34;, []string{\u0026#34;user\u0026#34;, \u0026#34;admin\u0026#34;}, ) if err != nil { panic(err) } fmt.Println(\u0026#34;Generated Token:\u0026#34;, token) } Adding Token Validation It\u0026rsquo;s crucial to implement token validation to ensure the tokens we receive are valid and haven\u0026rsquo;t been tampered with.\nfunc (i *JWTIssuer) ValidateToken(tokenString string) (*Claims, error) { token, err := jwt.ParseWithClaims(tokenString, \u0026amp;Claims{}, func(token *jwt.Token) (interface{}, error) { // Validate the signing method if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok { return nil, fmt.Errorf(\u0026#34;unexpected signing method: %v\u0026#34;, token.Header[\u0026#34;alg\u0026#34;]) } return i.secretKey, nil }) if err != nil { return nil, fmt.Errorf(\u0026#34;invalid token: %w\u0026#34;, err) } if claims, ok := token.Claims.(*Claims); ok \u0026amp;\u0026amp; token.Valid { return claims, nil } return nil, fmt.Errorf(\u0026#34;invalid token claims\u0026#34;) } Enhanced Security Features Let\u0026rsquo;s add some security enhancements to our JWT issuer.\nToken Revocation Support We\u0026rsquo;ll implement a simple token blacklist using Redis to support token revocation:\nimport ( \u0026#34;context\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) type EnhancedJWTIssuer struct { *JWTIssuer redis *redis.Client } func NewEnhancedJWTIssuer(secretKey string, duration time.Duration, redisURL string) (*EnhancedJWTIssuer, error) { opt, err := redis.ParseURL(redisURL) if err != nil { return nil, err } return \u0026amp;EnhancedJWTIssuer{ JWTIssuer: NewJWTIssuer(secretKey, duration), redis: redis.NewClient(opt), }, nil } func (i *EnhancedJWTIssuer) RevokeToken(ctx context.Context, token string) error { claims, err := i.ValidateToken(token) if err != nil { return err } // Store the token in the blacklist until its original expiration expiration := time.Until(claims.ExpiresAt.Time) return i.redis.Set(ctx, \u0026#34;blacklist:\u0026#34;+token, true, expiration).Err() } func (i *EnhancedJWTIssuer) IsTokenRevoked(ctx context.Context, token string) bool { exists, err := i.redis.Exists(ctx, \u0026#34;blacklist:\u0026#34;+token).Result() return err == nil \u0026amp;\u0026amp; exists \u0026gt; 0 } Rate Limiting Token Generation We\u0026rsquo;ll add rate limiting to prevent token generation abuse:\nimport ( \u0026#34;golang.org/x/time/rate\u0026#34; \u0026#34;sync\u0026#34; ) type RateLimitedJWTIssuer struct { *EnhancedJWTIssuer limiters map[string]*rate.Limiter limitersMu sync.RWMutex } func NewRateLimitedJWTIssuer(secretKey string, duration time.Duration, redisURL string) (*RateLimitedJWTIssuer, error) { enhanced, err := NewEnhancedJWTIssuer(secretKey, duration, redisURL) if err != nil { return nil, err } return \u0026amp;RateLimitedJWTIssuer{ EnhancedJWTIssuer: enhanced, limiters: make(map[string]*rate.Limiter), }, nil } func (i *RateLimitedJWTIssuer) getLimiter(userID string) *rate.Limiter { i.limitersMu.RLock() limiter, exists := i.limiters[userID] i.limitersMu.RUnlock() if exists { return limiter } i.limitersMu.Lock() defer i.limitersMu.Unlock() limiter = rate.NewLimiter(rate.Every(time.Minute), 10) // 10 tokens per minute i.limiters[userID] = limiter return limiter } func (i *RateLimitedJWTIssuer) IssueTokenWithRateLimit(ctx context.Context, userID, username string, roles []string) (string, error) { limiter := i.getLimiter(userID) if !limiter.Allow() { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;rate limit exceeded for user %s\u0026#34;, userID) } return i.IssueToken(userID, username, roles) } Using the Enhanced JWT Issuer in a Web Application Here\u0026rsquo;s an example of how to use our enhanced JWT issuer in a web application:\nfunc main() { issuer, err := NewRateLimitedJWTIssuer( \u0026#34;your-secret-key\u0026#34;, 24*time.Hour, \u0026#34;redis://localhost:6379/0\u0026#34;, ) if err != nil { panic(err) } http.HandleFunc(\u0026#34;/login\u0026#34;, func(w http.ResponseWriter, r *http.Request) { // Authentication logic here... userID := \u0026#34;user123\u0026#34; username := \u0026#34;john.doe\u0026#34; roles := []string{\u0026#34;user\u0026#34;, \u0026#34;admin\u0026#34;} token, err := issuer.IssueTokenWithRateLimit(r.Context(), userID, username, roles) if err != nil { http.Error(w, err.Error(), http.StatusTooManyRequests) return } json.NewEncoder(w).Encode(map[string]string{ \u0026#34;token\u0026#34;: token, }) }) http.HandleFunc(\u0026#34;/logout\u0026#34;, func(w http.ResponseWriter, r *http.Request) { token := r.Header.Get(\u0026#34;Authorization\u0026#34;) if token == \u0026#34;\u0026#34; { http.Error(w, \u0026#34;no token provided\u0026#34;, http.StatusBadRequest) return } // Remove \u0026#34;Bearer \u0026#34; prefix if present token = strings.TrimPrefix(token, \u0026#34;Bearer \u0026#34;) if err := issuer.RevokeToken(r.Context(), token); err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) return } w.WriteHeader(http.StatusOK) }) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } Best Practices and Security Considerations Secret Key Management\nNever hardcode secret keys Use environment variables or secure key management services Rotate keys periodically Token Expiration\nSet reasonable expiration times Consider using refresh tokens for long-term sessions Implement token rotation for sensitive operations Claims Security\nInclude only necessary information in claims Avoid sensitive data in tokens Use standard claims when possible Validation\nAlways validate tokens before trusting them Check signature, expiration, and issuer Implement proper error handling Conclusion Implementing a JWT issuer in Go requires careful consideration of security aspects beyond just token generation. By including features like token revocation, rate limiting, and proper validation, we can create a robust authentication system that\u0026rsquo;s both secure and scalable.\nThe implementation provided here serves as a foundation that you can build upon based on your specific requirements. Remember to always follow security best practices and keep your dependencies updated to maintain a secure system.\nReferences JWT.io golang-jwt/jwt Documentation OWASP JWT Security Cheat Sheet ","permalink":"https://manuelfedele.github.io/posts/jwt-issuer-in-go/","summary":"\u003ch1 id=\"building-a-secure-jwt-issuer-in-go-a-complete-guide\"\u003eBuilding a Secure JWT Issuer in Go: A Complete Guide\u003c/h1\u003e\n\u003cp\u003eJSON Web Tokens (JWT) have become the de facto standard for implementing stateless authentication in modern web applications. In this guide, we\u0026rsquo;ll implement a secure JWT issuer in Go, covering both basic implementation and advanced security considerations.\u003c/p\u003e\n\u003ch2 id=\"understanding-jwt-basics\"\u003eUnderstanding JWT Basics\u003c/h2\u003e\n\u003cp\u003eA JWT consists of three parts: header, payload, and signature. These parts are Base64URL encoded and concatenated with dots. The signature ensures the token hasn\u0026rsquo;t been tampered with, while the payload carries the claims (data) we want to transmit securely.\u003c/p\u003e","title":"Implementing a JWT Issuer in Go"},{"content":"A Beginner’s Guide and Optimization Techniques Graphs are fundamental data structures in computer science, representing relationships between entities. One of the most common problems involving graphs is finding the shortest path between nodes. Dijkstra’s algorithm is a classic solution to this problem for graphs with non-negative edge weights. In this guide, we’ll implement Dijkstra’s algorithm in Go and explore ways to optimize it using advanced data structures.\\\nBasic Implementation of Dijkstra’s Algorithm Let’s begin by understanding the core concept. Dijkstra’s algorithm maintains a set of nodes whose shortest distance from the source is known and repeatedly selects the node with the minimum distance to explore its neighbors.\nFirst, we’ll represent our graph. In Go, a convenient way to represent a graph is with adjacency lists using maps.\npackage main import ( \u0026#34;fmt\u0026#34; ) type Edge struct { node string weight int } type Graph struct { nodes map[string][]Edge } func NewGraph() *Graph { return \u0026amp;Graph{nodes: make(map[string][]Edge)} } func (g *Graph) AddEdge(from, to string, weight int) { g.nodes[from] = append(g.nodes[from], Edge{node: to, weight: weight}) } Implementing the Algorithm Now, let’s implement the basic version of Dijkstra’s algorithm:\nfunc Dijkstra(graph *Graph, start string) (distances map[string]int, previous map[string]string) { distances = make(map[string]int) previous = make(map[string]string) unvisited := make(map[string]bool) // Initialize distances and unvisited nodes for node := range graph.nodes { distances[node] = int(^uint(0) \u0026gt;\u0026gt; 1) // Set to infinity unvisited[node] = true } distances[start] = 0 for len(unvisited) \u0026gt; 0 { // Find the unvisited node with the smallest distance var currentNode string smallestDistance := int(^uint(0) \u0026gt;\u0026gt; 1) for node := range unvisited { if distances[node] \u0026lt;= smallestDistance { smallestDistance = distances[node] currentNode = node } } // Remove the node from unvisited set delete(unvisited, currentNode) // Update distances to neighbors for _, edge := range graph.nodes[currentNode] { alt := distances[currentNode] + edge.weight if alt \u0026lt; distances[edge.node] { distances[edge.node] = alt previous[edge.node] = currentNode } } } return distances, previous } Explanation Distances keeps track of the shortest distance from the start node to every other node. Previous helps reconstruct the shortest path. We iterate over all unvisited nodes, selecting the one with the smallest known distance. For each neighbor of the current node, we check if a shorter path exists via the current node. Example Usage func main() { graph := NewGraph() edges := []struct { from string to string weight int }{ {\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, 7}, {\u0026#34;A\u0026#34;, \u0026#34;C\u0026#34;, 9}, {\u0026#34;A\u0026#34;, \u0026#34;F\u0026#34;, 14}, {\u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, 10}, {\u0026#34;B\u0026#34;, \u0026#34;D\u0026#34;, 15}, {\u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, 11}, {\u0026#34;C\u0026#34;, \u0026#34;F\u0026#34;, 2}, {\u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, 6}, {\u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, 9}, } for _, edge := range edges { graph.AddEdge(edge.from, edge.to, edge.weight) graph.AddEdge(edge.to, edge.from, edge.weight) // Assuming undirected graph } distances, previous := Dijkstra(graph, \u0026#34;A\u0026#34;) fmt.Println(\u0026#34;Distances:\u0026#34;, distances) fmt.Println(\u0026#34;Previous nodes:\u0026#34;, previous) } This code constructs a graph and finds the shortest paths from node \u0026ldquo;A\u0026rdquo; to all other nodes.\nOptimizing Dijkstra’s Algorithm with a Priority Queue The basic implementation above has a time complexity of O(N²) due to the selection of the minimum node using a linear search. We can optimize this by using a priority queue (min-heap) to select the node with the smallest distance in logarithmic time.\nImplementing a Priority Queue\nGo’s standard library provides container/heap which can be used to implement a priority queue.\nfunc main() { graph := NewGraph() edges := []struct { from string to string weight int }{ {\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, 7}, {\u0026#34;A\u0026#34;, \u0026#34;C\u0026#34;, 9}, {\u0026#34;A\u0026#34;, \u0026#34;F\u0026#34;, 14}, {\u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, 10}, {\u0026#34;B\u0026#34;, \u0026#34;D\u0026#34;, 15}, {\u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, 11}, {\u0026#34;C\u0026#34;, \u0026#34;F\u0026#34;, 2}, {\u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, 6}, {\u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, 9}, } for _, edge := range edges { graph.AddEdge(edge.from, edge.to, edge.weight) graph.AddEdge(edge.to, edge.from, edge.weight) // Assuming undirected graph } distances, previous := Dijkstra(graph, \u0026#34;A\u0026#34;) fmt.Println(\u0026#34;Distances:\u0026#34;, distances) fmt.Println(\u0026#34;Previous nodes:\u0026#34;, previous) } This code constructs a graph and finds the shortest paths from node \u0026ldquo;A\u0026rdquo; to all other nodes.\nOptimizing Dijkstra’s Algorithm with a Priority Queue The basic implementation above has a time complexity of O(N²) due to the selection of the minimum node using a linear search. We can optimize this by using a priority queue (min-heap) to select the node with the smallest distance in logarithmic time.\nImplementing a Priority Queue Go’s standard library provides container/heap which can be used to implement a priority queue.\nimport ( \u0026#34;container/heap\u0026#34; ) type Item struct { node string priority int index int } type PriorityQueue []*Item func (pq PriorityQueue) Len() int { return len(pq) } func (pq PriorityQueue) Less(i, j int) bool { return pq[i].priority \u0026lt; pq[j].priority } func (pq PriorityQueue) Swap(i, j int) { pq[i], pq[j] = pq[j], pq[i] pq[i].index = i pq[j].index = j } func (pq *PriorityQueue) Push(x interface{}) { n := len(*pq) item := x.(*Item) item.index = n *pq = append(*pq, item) } func (pq *PriorityQueue) Pop() interface{} { old := *pq n := len(old) item := old[n-1] old[n-1] = nil // Avoid memory leak item.index = -1 // For safety *pq = old[0 : n-1] return item } func (pq *PriorityQueue) update(item *Item, priority int) { item.priority = priority heap.Fix(pq, item.index) } Optimized Dijkstra’s Algorithm func DijkstraWithHeap(graph *Graph, start string) (distances map[string]int, previous map[string]string) { distances = make(map[string]int) previous = make(map[string]string) pq := make(PriorityQueue, 0) heap.Init(\u0026amp;pq) heap.Push(\u0026amp;pq, \u0026amp;Item{node: start, priority: 0}) for pq.Len() \u0026gt; 0 { item := heap.Pop(\u0026amp;pq).(*Item) currentNode := item.node currentDistance := item.priority if _, ok := distances[currentNode]; ok { continue // Node already visited } distances[currentNode] = currentDistance for _, edge := range graph.nodes[currentNode] { if _, ok := distances[edge.node]; !ok { alt := currentDistance + edge.weight heap.Push(\u0026amp;pq, \u0026amp;Item{node: edge.node, priority: alt}) previous[edge.node] = currentNode } } } return distances, previous } Explanation We use a priority queue pq to always select the node with the smallest distance. When we pop a node from the heap, we check if it’s already visited to avoid redundant processing. The overall time complexity reduces to O((E + V) log V), where E is the number of edges and V is the number of vertices. Testing the Optimized Algorithm Let’s test our optimized function with the same graph:\nfunc main() { graph := NewGraph() edges := []struct { from string to string weight int }{ {\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, 7}, {\u0026#34;A\u0026#34;, \u0026#34;C\u0026#34;, 9}, {\u0026#34;A\u0026#34;, \u0026#34;F\u0026#34;, 14}, {\u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, 10}, {\u0026#34;B\u0026#34;, \u0026#34;D\u0026#34;, 15}, {\u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, 11}, {\u0026#34;C\u0026#34;, \u0026#34;F\u0026#34;, 2}, {\u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, 6}, {\u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, 9}, } for _, edge := range edges { graph.AddEdge(edge.from, edge.to, edge.weight) graph.AddEdge(edge.to, edge.from, edge.weight) // Assuming undirected graph } distances, previous := DijkstraWithHeap(graph, \u0026#34;A\u0026#34;) fmt.Println(\u0026#34;Distances with heap:\u0026#34;, distances) fmt.Println(\u0026#34;Previous nodes with heap:\u0026#34;, previous) } You should observe the same results as before, but with improved efficiency, especially noticeable in larger graphs.\nConclusion Implementing Dijkstra’s algorithm provides valuable insights into graph traversal and optimization techniques. Starting with a basic implementation helps understand the core logic, while optimizing with a priority queue significantly improves performance. While further optimizations are possible, the heap-based approach offers a good balance between complexity and efficiency for most practical applications.\nReferences Dijkstra’s Algorithm - Wikipedia container/heap Package Documentation ","permalink":"https://manuelfedele.github.io/posts/implementing-djikstra-algorithm-in-go/","summary":"\u003ch1 id=\"a-beginners-guide-and-optimization-techniques\"\u003eA Beginner’s Guide and Optimization Techniques\u003c/h1\u003e\n\u003cp\u003eGraphs are fundamental data structures in computer science, representing relationships between entities. One of the most common problems involving graphs is finding the shortest path between nodes. Dijkstra’s algorithm is a classic solution to this problem for graphs with non-negative edge weights. In this guide, we’ll implement Dijkstra’s algorithm in Go and explore ways to optimize it using advanced data structures.\\\u003c/p\u003e\n\u003ch2 id=\"basic-implementation-of-dijkstras-algorithm\"\u003eBasic Implementation of Dijkstra’s Algorithm\u003c/h2\u003e\n\u003cp\u003eLet’s begin by understanding the core concept. Dijkstra’s algorithm maintains a set of nodes whose shortest distance from the source is known and repeatedly selects the node with the minimum distance to explore its neighbors.\u003c/p\u003e","title":"Implementing Dijkstra’s Algorithm in Go"},{"content":"Monitoring Clipboard in Golang: A Guide to Obscuring Passwords Introduction In this article, we explore creating a Go program that monitors the system clipboard, automatically substituting passwords with asterisks. Uniquely, the program leaves the last few characters (minimum 1, maximum 3) of the password visible when the password length exceeds 8 characters.\nUnderstanding Clipboard Monitoring in Go The Clipboard Package Go lacks a built-in library for clipboard operations. We use atotto/clipboard, a third-party package offering simple clipboard interfaces.\nPolling Mechanism The program periodically checks the clipboard\u0026rsquo;s content, processing it based on predefined criteria through a polling mechanism.\nImplementing the Clipboard Watcher Setting Up the Environment Ensure Go is installed on your system. Download it from the Go website.\nInstalling the Clipboard Package Install atotto/clipboard:\ngo get github.com/atotto/clipboard Writing the Clipboard Watcher Create clipboard-watcher.go:\npackage main import ( \u0026#34;github.com/atotto/clipboard\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; ) func main() { var previousContent string for { currentContent, _ := clipboard.ReadAll() if currentContent != previousContent { const passwordPrefix = \u0026#34;password: \u0026#34; idx := strings.Index(currentContent, passwordPrefix) if idx != -1 { passwordStart := idx + len(passwordPrefix) passwordEnd := strings.Index(currentContent[passwordStart:], \u0026#34; \u0026#34;) if passwordEnd == -1 { passwordEnd = len(currentContent) } else { passwordEnd += passwordStart } password := currentContent[passwordStart:passwordEnd] var unobscuredLength int if len(password) \u0026gt; 8 { unobscuredLength = 3 if len(password) \u0026lt; 11 { unobscuredLength = len(password) - 8 } } else { unobscuredLength = 0 } obscuredPassword := strings.Repeat(\u0026#34;*\u0026#34;, len(password)-unobscuredLength) + currentContent[passwordStart+len(password)-unobscuredLength:passwordEnd] modifiedContent := currentContent[:passwordStart] + obscuredPassword + currentContent[passwordEnd:] clipboard.WriteAll(modifiedContent) } previousContent = currentContent } time.Sleep(1 * time.Second) } } This script obscures passwords with asterisks, leaving the last 1 to 3 characters visible for passwords longer than 8 characters.\nMaking the Application Installable Compiling the Go Program Compile into an executable:\ngo build clipboard-watcher.go Creating an Installer Use tools like Inno Setup (Windows) or Packages (macOS) for distribution. For Linux, distribute a shell script to copy the executable to a location like /usr/local/bin.\nConclusion This clipboard watcher in Go provides an innovative approach to obscuring passwords while maintaining a hint of their length. It\u0026rsquo;s a practical starting point for more advanced clipboard monitoring applications.\n","permalink":"https://manuelfedele.github.io/posts/clipboard-watch-remove-accidentally-typed-passwords/","summary":"\u003ch1 id=\"monitoring-clipboard-in-golang-a-guide-to-obscuring-passwords\"\u003eMonitoring Clipboard in Golang: A Guide to Obscuring Passwords\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this article, we explore creating a Go program that monitors the system clipboard, automatically substituting passwords with asterisks. Uniquely, the program leaves the last few characters (minimum 1, maximum 3) of the password visible when the password length exceeds 8 characters.\u003c/p\u003e\n\u003ch2 id=\"understanding-clipboard-monitoring-in-go\"\u003eUnderstanding Clipboard Monitoring in Go\u003c/h2\u003e\n\u003ch3 id=\"the-clipboard-package\"\u003eThe Clipboard Package\u003c/h3\u003e\n\u003cp\u003eGo lacks a built-in library for clipboard operations. We use \u003ccode\u003eatotto/clipboard\u003c/code\u003e, a third-party package offering simple clipboard interfaces.\u003c/p\u003e","title":"Clipboard Watch Remove Accidentally Typed Passwords"},{"content":"Cross-Account Logging: Shipping AWS Lambda Logs to OpenSearch In today\u0026rsquo;s distributed systems, logging and monitoring play a crucial role in detecting anomalies and ensuring system health. AWS Lambda and OpenSearch are often paired to deliver efficient, scalable logging solutions. However, complexities can arise when these resources live in separate AWS accounts. This blog post will guide you through the process of sending AWS Lambda logs from Account A to an OpenSearch cluster in Account B using Terraform as the Infrastructure as Code (IAC) tool and GitLab for CI/CD pipelines.\nPrerequisites Two AWS accounts (Account A for AWS Lambda and Account B for OpenSearch) A GitLab account with runners provisioned on AWS for CI/CD pipelines Terraform installed on your system Basic understanding of AWS Lambda, OpenSearch, AWS IAM, GitLab, and Terraform Step 1: Configuring Your AWS Accounts Before proceeding, ensure that both AWS accounts are correctly set up, and you have the necessary access permissions. We will also need an IAM role in Account B with the necessary permissions to access the OpenSearch cluster.\nIn Account A, create an AWS Lambda function. Be sure to attach the necessary IAM role and policy, enabling the function to create and write logs to AWS CloudWatch.\nStep 2: Set Up OpenSearch in Account B Next, in Account B, set up an OpenSearch cluster. The cluster should be correctly configured to receive logs from an external account. In addition, you should create an IAM role that allows the Log Stream from Account A to write to this cluster.\nStep 3: Setting Up Terraform Terraform is a powerful tool for managing infrastructure as code. With Terraform, you can define and provide data center infrastructure using a declarative configuration language.\nCreate a new Terraform project and define your AWS resources, including your Lambda function and OpenSearch cluster. Be sure to use the provider alias feature of Terraform to manage resources in multiple AWS accounts.\nRemember to include the necessary resource blocks, such as aws_lambda_function and aws_elasticsearch_domain, depending on your requirements.\nStep 4: Establishing Trust Relationship Establish a trust relationship between Account A (Lambda) and Account B (OpenSearch) using an IAM role in Account B. This role should have a trust policy that allows Account A to assume it, and a permissions policy that allows writing to OpenSearch. Use the \u0026lsquo;aws_iam_role\u0026rsquo; and \u0026lsquo;aws_iam_role_policy\u0026rsquo; Terraform resources to create this.\nStep 5: Configure AWS Lambda to Send Logs With the trust relationship set up, configure your Lambda function in Account A to send logs to CloudWatch Logs. From there, use a CloudWatch Logs subscription filter to redirect these logs to the OpenSearch cluster in Account B.\nThe subscription filter should use the ARN of the IAM role in Account B, allowing it to forward logs across accounts.\nStep 6: GitLab CI/CD The final step involves integrating our setup with GitLab\u0026rsquo;s CI/CD. We will create a pipeline that uses AWS-provisioned runners to apply our Terraform configuration, creating or updating our infrastructure.\nIn your GitLab repository, define a .gitlab-ci.yml file that describes the stages of your pipeline, including steps for \u0026ldquo;init\u0026rdquo;, \u0026ldquo;validate\u0026rdquo;, \u0026ldquo;plan\u0026rdquo;, and \u0026ldquo;apply\u0026rdquo;. Your runners will execute these steps to deploy your infrastructure.\nConclusion This post has covered a high-level guide on shipping AWS Lambda logs from one AWS account to an OpenSearch cluster in another. By integrating AWS services, Terraform, and GitLab, you can create a seamless logging solution that spans across multiple AWS accounts. Remember that while this setup works, it\u0026rsquo;s crucial to tailor it to your organization\u0026rsquo;s needs, considering factors like security, scale, and compliance. Happy logging!\nTerraform and GitLab CI/CD Configuration Below is a basic outline of Terraform and GitLab CI/CD configurations that facilitate the shipping of AWS Lambda logs from one AWS account to an OpenSearch cluster in another.\nTerraform Configuration # Account A - provider.tf provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; profile = \u0026#34;accountA\u0026#34; } # Account A - lambda.tf resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;example\u0026#34; { function_name = \u0026#34;example\u0026#34; filename = \u0026#34;lambda_function_payload.zip\u0026#34; source_code_hash = filebase64sha256(\u0026#34;lambda_function_payload.zip\u0026#34;) handler = \u0026#34;exports.test\u0026#34; role = aws_iam_role.lambda_exec.arn runtime = \u0026#34;nodejs12.x\u0026#34; publish = true } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;lambda_exec\u0026#34; { name = \u0026#34;lambda_exec_role\u0026#34; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34; } ] } EOF } resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;/aws/lambda/${aws_lambda_function.example.function_name}\u0026#34; retention_in_days = 14 } # Account B - provider.tf provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; profile = \u0026#34;accountB\u0026#34; alias = \u0026#34;accountB\u0026#34; } # Account B - opensearch.tf provider \u0026#34;aws\u0026#34; { alias = \u0026#34;accountB\u0026#34; region = \u0026#34;us-west-2\u0026#34; } resource \u0026#34;aws_elasticsearch_domain\u0026#34; \u0026#34;example\u0026#34; { provider = aws.accountB domain_name = \u0026#34;example\u0026#34; elasticsearch_version = \u0026#34;7.1\u0026#34; } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;cross_account_role\u0026#34; { name = \u0026#34;cross_account_role\u0026#34; assume_role_policy = \u0026lt;\u0026lt;POLICY { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;AccountA_ID\u0026gt;:root\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } POLICY } resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;policy\u0026#34; { name = \u0026#34;cross_account_policy\u0026#34; role = aws_iam_role.cross_account_role.id policy = \u0026lt;\u0026lt;POLICY { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;es:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;${aws_elasticsearch_domain.example.arn}/*\u0026#34; } ] } POLICY } # GitLab - .gitlab-ci.yml stages: - init - validate - plan - apply variables: AWS_DEFAULT_REGION: \u0026#34;us-west-2\u0026#34; AWS_ACCESS_KEY_ID: \u0026#34;\u0026lt;access_key\u0026gt;\u0026#34; AWS_SECRET_ACCESS_KEY: \u0026#34;\u0026lt;secret_key\u0026gt;\u0026#34; init: stage: init script: - terraform init validate: stage: validate script: - terraform validate plan: stage: plan script: - terraform plan apply: stage: apply script: - terraform apply -auto-approve ","permalink":"https://manuelfedele.github.io/posts/aws-opensearch-as-monitoring-tool/","summary":"\u003ch1 id=\"cross-account-logging-shipping-aws-lambda-logs-to-opensearch\"\u003eCross-Account Logging: Shipping AWS Lambda Logs to OpenSearch\u003c/h1\u003e\n\u003cp\u003eIn today\u0026rsquo;s distributed systems, logging and monitoring play a crucial role in detecting anomalies and ensuring system health. AWS Lambda and OpenSearch are often paired to deliver efficient, scalable logging solutions. However, complexities can arise when these resources live in separate AWS accounts. This blog post will guide you through the process of sending AWS Lambda logs from Account A to an OpenSearch cluster in Account B using Terraform as the Infrastructure as Code (IAC) tool and GitLab for CI/CD pipelines.\u003c/p\u003e","title":"Aws Opensearch as Monitoring Tool"},{"content":"The Single Responsibility Principle (SRP) is a software design principle that states that a software module or component should have only one reason to change. This means that a module or component should have a single, narrowly defined responsibility and all of its features should be related to that responsibility.\nIn Go, the SRP can be applied at both the package and the function level.\nAt the package level, it\u0026rsquo;s important to consider what a package should be responsible for. A package should contain all of the code related to a specific feature or set of features. For example, a package that handles user authentication and authorization should not also contain code related to sending email notifications. These are two distinct responsibilities and should be separated into different packages.\nAt the function level, the SRP can be applied by ensuring that each function has a specific, narrowly defined responsibility. A function should do one thing and do it well. This makes it easier to understand, test, and maintain the code.\nHere\u0026rsquo;s an example of how the SRP can be applied in Go:\npackage user // User represents a user in the system. type User struct { ID int Username string Email string } // NewUser creates a new user with the given ID, username, and email. func NewUser(id int, username, email string) *User { return \u0026amp;User{ ID: id, Username: username, Email: email, } } // SendWelcomeEmail sends a welcome email to the user. func (u *User) SendWelcomeEmail() error { // Send the welcome email. return nil } // UpdateEmail updates the user\u0026#39;s email address. func (u *User) UpdateEmail(email string) { u.Email = email } In this example, the user package is responsible for managing users in the system. The NewUser function is responsible for creating a new user, and the SendWelcomeEmail and UpdateEmail functions are responsible for performing specific actions on a user. Each of these functions has a single, narrowly defined responsibility, making it easy to understand what they do and test them individually.\nBy following the Single Responsibility Principle in Go, you can write code that is easier to understand, test, and maintain. It also helps to ensure that your code is well-organized and modular, making it easier to reuse and extend in the future.\n","permalink":"https://manuelfedele.github.io/posts/the-single-responsibility-principle/","summary":"\u003cp\u003eThe Single Responsibility Principle (SRP) is a software design principle that states that a software module or component should have only one reason to change. This means that a module or component should have a single, narrowly defined responsibility and all of its features should be related to that responsibility.\u003c/p\u003e\n\u003cp\u003eIn Go, the SRP can be applied at both the package and the function level.\u003c/p\u003e\n\u003cp\u003eAt the package level, it\u0026rsquo;s important to consider what a package should be responsible for. A package should contain all of the code related to a specific feature or set of features. For example, a package that handles user authentication and authorization should not also contain code related to sending email notifications. These are two distinct responsibilities and should be separated into different packages.\u003c/p\u003e","title":"The Single Responsibility Principle"},{"content":"Cron scheduling is a useful feature in GitHub Actions that allows you to run a workflow on a schedule. This can be useful for tasks such as running tests or deploying code at regular intervals.\nTo use cron scheduling in GitHub Actions, you will need to add a schedule key to your workflow file. The schedule key should contain a cron expression that specifies when the workflow should run.\nHere is an example of a workflow that runs every day at noon:\nname: Daily Build on: schedule: - cron: \u0026#39;0 12 * * *\u0026#39; jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Run tests run: npm test In this example, the cron expression 0 12 * * * means that the workflow will run at noon (12:00) every day. The expression consists of five fields separated by spaces:\n0: The minute field. This specifies that the workflow should run at the 0th minute of the hour (i.e., on the hour). 12: The hour field. This specifies that the workflow should run at 12:00 (noon). *: The day of the month field. The asterisk means that the workflow will run every day of the month. *: The month field. The asterisk means that the workflow will run every month. *: The day of the week field. The asterisk means that the workflow will run every day of the week. You can use a variety of different values in the fields of a cron expression to specify when the workflow should run. For example, you could use 0 0 * * 1 to run the workflow at midnight every Monday, or 0 0 1 * * to run the workflow at midnight on the first of every month.\nIt\u0026rsquo;s also possible to use more advanced cron expressions with multiple schedules. For example:\non: schedule: - cron: \u0026#39;0 12 * * 1,2,3,4,5\u0026#39; - cron: \u0026#39;0 12 * * 6\u0026#39; - cron: \u0026#39;0 0 1 * *\u0026#39; In this example, the workflow will run at noon on weekdays (Monday through Friday) and at midnight on the first of every month.\nGitHub Actions provides a helpful cron expression builder tool that you can use to create custom cron expressions. You can access the tool by clicking the \u0026ldquo;Add a new schedule\u0026rdquo; button on the Actions tab of your repository.\nI hope this helps you get started with cron scheduling in GitHub Actions! Let me know if you have any questions.\n","permalink":"https://manuelfedele.github.io/posts/schedule-github-action-with-cron/","summary":"\u003cp\u003eCron scheduling is a useful feature in GitHub Actions that allows you to run a workflow on a schedule. This can be useful for tasks such as running tests or deploying code at regular intervals.\u003c/p\u003e\n\u003cp\u003eTo use cron scheduling in GitHub Actions, you will need to add a schedule key to your workflow file. The schedule key should contain a cron expression that specifies when the workflow should run.\u003c/p\u003e\n\u003cp\u003eHere is an example of a workflow that runs every day at noon:\u003c/p\u003e","title":"Schedule Github Action With Cron"},{"content":"To delete a Git branch locally, you can use the git branch command with the -d flag, followed by the name of the branch you want to delete. For example:\ngit branch -d branch_name This will delete the specified branch if it has already been fully merged into the current branch. If the branch has not been fully merged, you can use the -D flag instead, which will force the deletion of the branch.\nTo delete a Git branch remotely, you can use the git push command followed by the name of the remote repository and the name of the branch you want to delete, prefixed with a :. For example:\ngit push origin :branch_name This will delete the specified branch from the remote repository. Note that you will not be able to delete a branch that you are currently on. To delete a branch that you are currently on, you will need to switch to a different branch first.\nIt\u0026rsquo;s generally a good idea to double-check that you really want to delete a branch before doing so, as once a branch is deleted, the changes made in that branch are lost forever.\n","permalink":"https://manuelfedele.github.io/posts/how-to-delete-git-branch-locally/","summary":"\u003cp\u003eTo delete a Git branch locally, you can use the \u003ccode\u003egit branch\u003c/code\u003e command with the \u003ccode\u003e-d\u003c/code\u003e flag, followed by the name of the branch you want to delete. For example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit branch -d branch_name\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis will delete the specified branch if it has already been fully merged into the current branch. If the branch has not been fully merged, you can use the -D flag instead, which will force the deletion of the branch.\u003c/p\u003e","title":"How to Delete Git Branch Locally"},{"content":"Git is a version control system that allows developers to track changes to their codebase and collaborate with others. One of the useful features of Git is the ability to revert changes that have been made to the codebase. In this article, we will look at how to undo the most recent local commits in Git.\nThere are several ways to undo commits in Git, and the method you choose will depend on your specific needs. Here are three common ways to undo commits in Git:\ngit reset The git reset command is used to undo commits by moving the current branch pointer to a previous commit. It has three options for where to move the pointer: --mixed (the default option), --soft, and --hard.\ngit reset --mixed HEAD~1: This will undo the most recent commit and move the changes made in that commit to the staging area. The HEAD~1 specifies the commit to be undone, where HEAD is the current commit and ~1 indicates the commit before the current commit.\ngit reset --soft HEAD~1: This will undo the most recent commit and move the changes made in that commit to the staging area, but it will not delete the changes. This is useful if you want to make additional changes before committing.\ngit reset --hard HEAD~1: This will undo the most recent commit and permanently delete the changes made in that commit. This option should be used with caution, as the changes cannot be recovered.\ngit revert The git revert command is used to undo commits by creating a new commit that undoes the changes made in a previous commit. It does not delete any commits, and the changes can be easily undone by reverting the revert commit. To undo the most recent commit using git revert, use the following command: git revert head\ngit cherry-pick The git cherry-pick command is used to apply the changes made in a specific commit to the current branch. It can be used to selectively undo commits by cherry-picking the commits that you do not want to include.\nTo undo the most recent commit using git cherry-pick, use the following command: git cherry-pick HEAD~1. This will apply the changes made in the commit before the current commit (HEAD~1) to the current branch.\n","permalink":"https://manuelfedele.github.io/posts/how-to-undo-most-recent-local-git-commit/","summary":"\u003cp\u003eGit is a version control system that allows developers to track changes to their codebase and collaborate with others. One of the useful features of Git is the ability to revert changes that have been made to the codebase. In this article, we will look at how to undo the most recent local commits in Git.\u003c/p\u003e\n\u003cp\u003eThere are several ways to undo commits in Git, and the method you choose will depend on your specific needs. Here are three common ways to undo commits in Git:\u003c/p\u003e","title":"How to Undo Most Recent Local Git Commit"},{"content":"Design patterns are reusable solutions to common software design problems. They are a way to structure and organize code in a way that makes it easier to understand, maintain, and extend. In this article, we\u0026rsquo;ll explore how to implement some popular design patterns in Go.\nSingleton pattern The singleton pattern is a creational design pattern that ensures a class has only one instance and provides a global access point to it. In Go, we can implement the singleton pattern using the sync.Once type. Here\u0026rsquo;s an example:\ntype Singleton struct { // fields } var instance *Singleton var once sync.Once func GetInstance() *Singleton { once.Do(func() { instance = \u0026amp;Singleton{} }) return instance } The sync.Once type has a Do method that ensures that a function is only called once. In this case, the function creates a new Singleton instance and assigns it to the instance variable. The GetInstance function returns the instance variable, which will always be the same instance created by the once.Do function.\nFactory pattern The factory pattern is a creational design pattern that provides an interface for creating objects in a super class, but allows subclasses to alter the type of objects that will be created. In Go, we can implement the factory pattern using an interface and concrete types that implement the interface. Here\u0026rsquo;s an example:\ntype Shape interface { Area() float64 } type Rectangle struct { width float64 height float64 } func (r Rectangle) Area() float64 { return r.width * r.height } type Circle struct { radius float64 } func (c Circle) Area() float64 { return math.Pi * c.radius * c.radius } type ShapeFactory struct{} func (f ShapeFactory) NewShape(shapeType string) Shape { switch shapeType { case \u0026#34;rectangle\u0026#34;: return Rectangle{} case \u0026#34;circle\u0026#34;: return Circle{} default: return nil } } The Shape interface defines a method for calculating the area of a shape. The Rectangle and Circle types both implement the Shape interface. The ShapeFactory type has a NewShape method that returns a new Shape of the specified type. This allows us to create different types of shapes using the same interface, without specifying the concrete type.\nObserver pattern The observer pattern is a behavioral design pattern that allows an object to be notified of changes to another object. In Go, we can implement the observer pattern using channels and goroutines. Here\u0026rsquo;s an example:\ntype Observable struct { observers []chan int } func (o *Observable) AddObserver(c chan int) { o.observers = append(o.observers, c) } func (o *Observable) Notify(n int) { for _, c := range o.observers { c \u0026lt;- n } } func main() { observable := Observable{} observer1 := make(chan int) observer2 := make(chan int) observable.AddObserver(observer1) observable.AddObserver(observer2) go func() { for n := range observer1 { fmt.Println(\u0026#34;Observer 1 received:\u0026#34;, n) } }() go func() { for n := range observer2 { fmt.Println(\u0026#34;Observer 2 received:\u0026#34;, n) } }() observable.Notify(1) observable.Notify(2) observable.Notify(3) close(observer1) close(observer2) } The Observable type has a slice of channels to hold its observers, and methods to add and notify observers. In the main function, we create two channels and add them as observers to the Observable. We then start two goroutines that listen on the channels and print the values they receive. Finally, we call the Notify method on the Observable to send values to the observers.\nBuilder pattern The builder pattern is a creational design pattern that separates the construction of a complex object from its representation, allowing the same construction process to create different representations. In Go, we can implement the builder pattern using a builder interface and concrete builders that implement the interface. Here\u0026rsquo;s an example:\ntype Builder interface { SetType(string) Builder SetSize(int) Builder SetColor(string) Builder Build() interface{} } type HouseBuilder struct { houseType string size int color string } func (h *HouseBuilder) SetType(t string) Builder { h.houseType = t return h } func (h *HouseBuilder) SetSize(s int) Builder { h.size = s return h } func (h *HouseBuilder) SetColor(c string) Builder { h.color = c return h } func (h *HouseBuilder) Build() interface{} { return House{Type: h.houseType, Size: h.size, Color: h.color} } type House struct { Type string Size int Color string } The Builder interface defines methods for setting the properties of the object being built and for building the object. The HouseBuilder type is a concrete builder that implements the Builder interface and has fields to store the properties of a House object. The Build method returns a House object with the stored properties. In the main function, we use the builder\u0026rsquo;s methods to set the properties of the House and then call the Build method to create the House.\nThese are just a few examples of how to implement design patterns in Go. There are many more design patterns that can be useful in different situations, and Go provides a flexible and powerful toolset for implementing them.\n","permalink":"https://manuelfedele.github.io/posts/implement-desing-patterns-with-golang/","summary":"\u003cp\u003eDesign patterns are reusable solutions to common software design problems. They are a way to structure and organize code in a way that makes it easier to understand, maintain, and extend. In this article, we\u0026rsquo;ll explore how to implement some popular design patterns in Go.\u003c/p\u003e\n\u003ch2 id=\"singleton-pattern\"\u003eSingleton pattern\u003c/h2\u003e\n\u003cp\u003eThe singleton pattern is a creational design pattern that ensures a class has only one instance and provides a global access point to it. In Go, we can implement the singleton pattern using the sync.Once type. Here\u0026rsquo;s an example:\u003c/p\u003e","title":"Implement Desing Patterns With Golang"},{"content":"NATS Messaging with Golang NATS is a high-performance, lightweight messaging system that is widely used for building distributed systems. It is designed to be simple, fast, and easy to use, making it a popular choice for many developers. In this tutorial, we will learn how to use NATS with Golang to send and receive messages.\nSetting up NATS To use NATS with Golang, we first need to install the NATS server and client libraries. The NATS server can be downloaded from the NATS website or installed using a package manager like apt or brew.\nOnce the NATS server is installed and running, we can install the NATS client library for Golang by running the following command:\ngo get github.com/nats-io/nats.go Connecting to NATS To connect to NATS, we will use the nats.Connect() function. This function takes a NATS server URL as an argument and returns a *nats.Conn type representing the connection.\nimport \u0026#34;github.com/nats-io/nats.go\u0026#34; func main() { // Connect to the NATS server nc, err := nats.Connect(\u0026#34;nats://localhost:4222\u0026#34;) if err != nil { // Handle error } // Use the connection... } Sending Messages To send a message using NATS, we can use the Publish() method of the *nats.Conn type. This method takes the subject, a message payload, and optional reply subject as arguments.\nimport \u0026#34;github.com/nats-io/nats.go\u0026#34; func main() { // Connect to the NATS server nc, err := nats.Connect(\u0026#34;nats://localhost:4222\u0026#34;) if err != nil { // Handle error } // Send a message err = nc.Publish(\u0026#34;subject\u0026#34;, []byte(\u0026#34;Hello NATS!\u0026#34;)) if err != nil { // Handle error } } Receiving Messages To receive messages from NATS, we can use the Subscribe() method of the *nats.Conn type. This method takes the subject and a callback function as arguments. The callback function will be called every time a message is received on the given subject.\nimport \u0026#34;github.com/nats-io/nats.go\u0026#34; func main() { // Connect to the NATS server nc, err := nats.Connect(\u0026#34;nats://localhost:4222\u0026#34;) if err != nil { // Handle error } // Subscribe to a subject _, err = nc.Subscribe(\u0026#34;subject\u0026#34;, func(msg *nats.Msg) { // Handle message fmt.Printf(\u0026#34;Received message: %s\\n\u0026#34;, string(msg.Data)) }) if err != nil { // Handle error } } Closing the Connection When we are finished using NATS, it is important to close the connection to release resources and avoid leaks. To close the connection, we can use the Close() method of the *nats.Conn type.\nimport \u0026#34;github.com/nats-io/nats.go\u0026#34; func main() { // Connect to the NATS server nc, err := nats.Connect(\u0026#34;nats://localhost:4222\u0026#34;) if err != nil { // Handle error } defer nc.Close() // Use NATS... } Additional Features NATS provides many additional features beyond basic message sending and receiving, such as queue subscriptions, request-response patterns, and message filtering. For more information on these features and how to use them, you can refer to the NATS documentation.\nConclusion In this tutorial, we learned how to use NATS with Golang to send and receive messages. NATS is a powerful and easy-to-use messaging system that is well-suited for building distributed systems. With the techniques covered in this tutorial, you should be well on your way to using NATS in your own projects.\n","permalink":"https://manuelfedele.github.io/posts/nats-messaging-with-golang/","summary":"\u003cp\u003eNATS Messaging with Golang\nNATS is a high-performance, lightweight messaging system that is widely used for building distributed systems. It is designed to be simple, fast, and easy to use, making it a popular choice for many developers. In this tutorial, we will learn how to use NATS with Golang to send and receive messages.\u003c/p\u003e\n\u003cp\u003eSetting up NATS\nTo use NATS with Golang, we first need to install the NATS server and client libraries. The NATS server can be downloaded from the NATS website or installed using a package manager like apt or brew.\u003c/p\u003e","title":"Nats Messaging With Golang"},{"content":"Sudoku Solver in Go: A Beginner\u0026rsquo;s Guide and Optimization Techniques The game of Sudoku has always been a popular pastime for many. Whether you\u0026rsquo;re an absolute novice or a seasoned veteran, the challenge of filling out a 9x9 grid with digits so that each column, each row, and each of the nine 3x3 subgrids contains all of the digits from 1 to 9, is an appealing task. Today, we\u0026rsquo;re going to see how we can automate this process in Golang, and how we can optimize it using memoization techniques.\nA Basic Sudoku Solver in Golang Let\u0026rsquo;s start with a basic approach. First, we need to represent our Sudoku grid in Go. A two-dimensional slice of integers fits the bill:\ntype SudokuGrid [9][9]int Backtracking Algorithm The simplest approach to solving Sudoku programmatically is to use a backtracking algorithm. Backtracking is a trial and error based methodology used for problem solving. Let\u0026rsquo;s implement a basic backtracking algorithm in Go:\nfunc solveSudoku(grid *SudokuGrid) bool { for i := 0; i \u0026lt; 9; i++ { for j := 0; j \u0026lt; 9; j++ { if grid[i][j] == 0 { for num := 1; num \u0026lt;= 9; num++ { if isValid(grid, i, j, num) { grid[i][j] = num if solveSudoku(grid) { return true } else { // Undo current cell for backtracking grid[i][j] = 0 } } } return false } } } return true } The solveSudoku function works by iterating through each cell in the grid, filling in empty cells (those that contain a zero), and checking if the number being inserted is valid for that cell. If the number is valid, the function recursively tries to solve the rest of the grid. If it can\u0026rsquo;t, it backtracks by setting the cell back to zero and tries the next number.\nThe isValid function checks if a given number is valid for a particular cell:\nfunc isValid(grid *SudokuGrid, row, col, num int) bool { // Check the line for i := 0; i \u0026lt; 9; i++ { if grid[row][i] == num { return false } } // Check the column for i := 0; i \u0026lt; 9; i++ { if grid[i][col] == num { return false } } // Check the box startRow, startCol := row-row%3, col-col%3 for i := 0; i \u0026lt; 3; i++ { for j := 0; j \u0026lt; 3; j++ { if grid[i+startRow][j+startCol] == num { return false } } } return true } Optimizing Sudoku Solver with Memoization The standard backtracking approach works fine, but it can be slow, especially for larger or more complex Sudoku puzzles. One way to speed up the backtracking algorithm is by using memoization, a technique that stores the results of expensive function calls and reusing them when the same inputs occur again.\nIn our case, we can store the validity of each number for each cell in three maps, one for the rows, one for the columns, and one for the boxes.\nLet\u0026rsquo;s define our memoization maps:\ntype SudokuGrid [9][9]int type MemoizationMap [9][9][10]bool var rowsUsed, colsUsed, boxesUsed MemoizationMap We need to initialize our maps at the beginning of our program. We\u0026rsquo;ll go through our Sudoku grid and for each cell that is not empty, we\u0026rsquo;ll set the corresponding entry in our maps:\nfunc initializeMemoizationMaps(grid *SudokuGrid) { for i := 0; i \u0026lt; 9; i++ { for j := 0; j \u0026lt; 9; j++ { num := grid[i][j] if num != 0 { idx := (i/3)*3 + j/3 rowsUsed[i][j][num] = true colsUsed[i][j][num] = true boxesUsed[idx][j][num] = true } } } } Now, we can modify our solveSudoku and isValid functions to make use of our memoization maps:\nfunc solveSudoku(grid *SudokuGrid) bool { for i := 0; i \u0026lt; 9; i++ { for j := 0; j \u0026lt; 9; j++ { if grid[i][j] == 0 { for num := 1; num \u0026lt;= 9; num++ { if isValid(i, j, num) { placeNumber(grid, i, j, num) if solveSudoku(grid) { return true } else { removeNumber(grid, i, j, num) } } } return false } } } return true } func isValid(row, col, num int) bool { idx := (row / 3) * 3 + col / 3 return !rowsUsed[row][num] \u0026amp;\u0026amp; !colsUsed[col][num] \u0026amp;\u0026amp; !boxesUsed[idx][num] } func placeNumber(grid *SudokuGrid, row, col, num int) { idx := (row / 3) * 3 + col / 3 grid[row][col] = num rowsUsed[row][num] = true colsUsed[col][num] = true boxesUsed[idx][num] = true } func removeNumber(grid *SudokuGrid, row, col, num int) { idx := (row / 3) * 3 + col / 3 grid[row][col] = 0 rowsUsed[row][num] = false colsUsed[col][num] = false boxesUsed[idx][num] = false } With this new approach, instead of repeatedly checking the validity of a number in a certain cell, we are directly accessing the stored value in constant time, leading to a substantial speed up in our program.\nWhile both these approaches solve the Sudoku puzzle, using memoization significantly speeds up the process. Remember that while memoization uses more memory, it can drastically cut down the time complexity of our algorithm.\n","permalink":"https://manuelfedele.github.io/posts/build-a-sudoku-solver-in-golang/","summary":"\u003ch1 id=\"sudoku-solver-in-go-a-beginners-guide-and-optimization-techniques\"\u003eSudoku Solver in Go: A Beginner\u0026rsquo;s Guide and Optimization Techniques\u003c/h1\u003e\n\u003cp\u003eThe game of Sudoku has always been a popular pastime for many. Whether you\u0026rsquo;re an absolute novice or a seasoned veteran, the challenge of filling out a 9x9 grid with digits so that each column, each row, and each of the nine 3x3 subgrids contains all of the digits from 1 to 9, is an appealing task. Today, we\u0026rsquo;re going to see how we can automate this process in Golang, and how we can optimize it using memoization techniques.\u003c/p\u003e","title":"Build a Sudoku Solver in Golang"},{"content":"Socket messaging allows two or more processes to communicate with each other over a network by sending and receiving messages through a socket connection. In this article, we\u0026rsquo;ll look at how to use Go to implement socket messaging between two processes.\nSetting up the Socket Server The first step in implementing socket messaging is to set up a socket server that listens for incoming connections and processes incoming messages. To do this in Go, we\u0026rsquo;ll need to use the net package to create a socket server and handle incoming connections.\nHere\u0026rsquo;s an example of how to set up a socket server in Go:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; ) func main() { // Create a socket server on port 8080 ln, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:8080\u0026#34;) if err != nil { fmt.Println(err) return } defer ln.Close() fmt.Println(\u0026#34;Socket server listening on port 8080\u0026#34;) // Accept incoming connections and handle them in a separate goroutine for { conn, err := ln.Accept() if err != nil { fmt.Println(err) continue } go handleConnection(conn) } } // handleConnection processes incoming messages from a socket connection func handleConnection(conn net.Conn) { // Read incoming messages buf := make([]byte, 1024) for { n, err := conn.Read(buf) if err != nil { fmt.Println(err) return } msg := string(buf[:n]) fmt.Printf(\u0026#34;Received message: %s\\n\u0026#34;, msg) } } This code creates a socket server on port 8080 and listens for incoming connections. When a connection is accepted, it starts a separate goroutine to handle the connection and process incoming messages. The handleConnection function reads incoming messages using the Read method of the net.Conn type, and prints them to the console.\nSending Messages from the Client Now that we have a socket server set up, we can create a client that connects to the server and sends messages. To do this in Go, we\u0026rsquo;ll need to use the net package to create a socket client and connect to the server.\nHere\u0026rsquo;s an example of how to send messages from a socket client in Go:\npackage main import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; ) func main() { // Connect to the socket server on localhost:8080 conn, err := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:8080\u0026#34;) if err != nil { fmt.Println(err) return } defer conn.Close() // Read input from the console and send it to the server reader := bufio.NewReader(os.Stdin) for { fmt.Print(\u0026#34;Enter a message: \u0026#34;) msg, _ := reader.ReadString(\u0026#39;\\n\u0026#39;) _, err = conn.Write([]byte(msg)) if err != nil { fmt.Println(err) return } } } In this example, we use the net package to connect to a socket server on localhost:8080. We then read input from the console using the bufio package, and send it to the server using the Write method of the net.Conn type.\nThis is just a simple example of socket messaging with Go. You can find more detailed documentation and examples in the Go documentation.\nI hope this helps give you an idea of how to use Go to implement socket messaging.\n","permalink":"https://manuelfedele.github.io/posts/socket-messaging-with-golang/","summary":"\u003cp\u003eSocket messaging allows two or more processes to communicate with each other over a network by sending and receiving messages through a socket connection. In this article, we\u0026rsquo;ll look at how to use Go to implement socket messaging between two processes.\u003c/p\u003e\n\u003cp\u003eSetting up the Socket Server\nThe first step in implementing socket messaging is to set up a socket server that listens for incoming connections and processes incoming messages. To do this in Go, we\u0026rsquo;ll need to use the net package to create a socket server and handle incoming connections.\u003c/p\u003e","title":"Socket Messaging With Golang"},{"content":"Tracking pixels, also known as web beacons, are small transparent images that are used to track the effectiveness of emails. They work by including a unique image in the email that is hosted on a server, and when the email is opened and the image is loaded, it sends a request to the server with information about the email opening. This information can be used to track the effectiveness of the email and see how many people have opened it.\nTo use tracking pixels with Golang, we can follow these steps:\nCreate a unique image to use as the tracking pixel. This image should be hosted on a server that you control.\nInclude the tracking pixel in the email. This can be done by using an tag in the HTML of the email and setting the src attribute to the URL of the tracking pixel image.\nSet up a server to handle the tracking pixel requests. When the email is opened and the image is loaded, it will send a request to the server with information about the email opening. The server can then log this information for later analysis.\nHere is an example of how the tracking pixel could be implemented in Golang:\nimport ( \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func main() { // Set up a handler for the tracking pixel http.HandleFunc(\u0026#34;/pixel.png\u0026#34;, func(w http.ResponseWriter, r *http.Request) { // Log the request information log.Printf(\u0026#34;Tracking pixel request from %s\u0026#34;, r.RemoteAddr) // Set the content type to image/png w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;image/png\u0026#34;) // Set the cache control headers to prevent caching w.Header().Set(\u0026#34;Cache-Control\u0026#34;, \u0026#34;no-cache, no-store, must-revalidate\u0026#34;) w.Header().Set(\u0026#34;Expires\u0026#34;, time.Now().UTC().Format(http.TimeFormat)) // Serve the transparent 1x1 PNG image http.ServeFile(w, r, \u0026#34;pixel.png\u0026#34;) } // Start the server log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } This example sets up a handler for the tracking pixel that logs the request information and serves a transparent 1x1 PNG image. It also sets the content type and cache control headers to ensure that the image is displayed correctly and not cached by the browser.\nWith this approach, we can use tracking pixels to track the effectiveness of emails in Golang.\n","permalink":"https://manuelfedele.github.io/posts/tracking-pixel-technology-email-golang/","summary":"\u003cp\u003eTracking pixels, also known as web beacons, are small transparent images that are used to track the effectiveness of emails. They work by including a unique image in the email that is hosted on a server, and when the email is opened and the image is loaded, it sends a request to the server with information about the email opening. This information can be used to track the effectiveness of the email and see how many people have opened it.\u003c/p\u003e","title":"Tracking Pixel Technology with Golang"},{"content":"Evaluating chess positions is an important part of chess strategy and can help players make more informed decisions about which moves to make. In this blog post, we\u0026rsquo;ll look at how to use the powerful Stockfish chess engine and the Go programming language to evaluate chess positions.\nInstalling Stockfish Before we can start using Stockfish, we need to install it. On Linux and macOS, you can install Stockfish using the package manager of your choice (e.g., apt-get, Homebrew, etc.). On Windows, you can download a pre-compiled executable from the Stockfish website (https://stockfishchess.org/download/).\nUsing Stockfish with Go To use Stockfish with Go, we\u0026rsquo;ll need to use a library that provides a Go interface to the engine. There are several options available, but we\u0026rsquo;ll be using the \u0026ldquo;github.com/notnil/chess\u0026rdquo; library in this example.\nTo install the library, run the following command:\ngo get github.com/notnil/chess Now we\u0026rsquo;re ready to start using Stockfish in our Go programs.\nEvaluating chess positions is an important part of chess strategy and can help players make more informed decisions about which moves to make. In this blog post, we\u0026rsquo;ll look at how to use the powerful Stockfish chess engine and the Go programming language to evaluate chess positions.\nInstalling Stockfish Before we can start using Stockfish, we need to install it. On Linux and macOS, you can install Stockfish using the package manager of your choice (e.g., apt-get, Homebrew, etc.). On Windows, you can download a pre-compiled executable from the Stockfish website (https://stockfishchess.org/download/).\nUsing Stockfish with Go To use Stockfish with Go, we\u0026rsquo;ll need to use a library that provides a Go interface to the engine. There are several options available, but we\u0026rsquo;ll be using the \u0026ldquo;github.com/notnil/chess\u0026rdquo; library in this example.\nTo install the library, run the following command:\ngo get github.com/notnil/chess Now we\u0026rsquo;re ready to start using Stockfish in our Go programs.\nEvaluating a Chess Position To get Stockfish\u0026rsquo;s evaluation of a chess position, we first need to create a new Stockfish instance:\nstockfish, err := chess.NewEngine(\u0026#34;stockfish\u0026#34;) if err != nil { panic(err) } Next, we can set the difficulty level, which controls how deep Stockfish will search for the best move. A higher difficulty level will result in more accurate evaluations, but will also take longer to compute.\nstockfish.SetDifficulty(20) Now we can set the position that we want to evaluate using standard chess notation (e.g., \u0026ldquo;rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\u0026rdquo; for the starting position).\nstockfish.SetPosition(\u0026#34;rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\u0026#34;) Finally, we can call the Evaluate() method to get Stockfish\u0026rsquo;s evaluation of the position:\nevaluation, err := stockfish.Evaluate() if err != nil { panic(err) } fmt.Println(evaluation) // Outputs: \u0026#34;0.00\u0026#34; The evaluation is returned as a string in centipawns, where a positive value indicates that White has an advantage and a negative value indicates that Black has an advantage. A value of \u0026ldquo;0.00\u0026rdquo; indicates a balanced position.\nComplete code import \u0026#34;github.com/notnil/chess\u0026#34; func main() { // Create a new Stockfish instance stockfish, err := chess.NewEngine(\u0026#34;stockfish\u0026#34;) if err != nil { panic(err) } // Set the difficulty level (this controls how deep Stockfish will search) stockfish.SetDifficulty(20) // Set the position to be evaluated stockfish.SetPosition(\u0026#34;rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\u0026#34;) // Get Stockfish\u0026#39;s evaluation of the position evaluation, err := stockfish.Evaluate() if err != nil { panic(err) } fmt.Println(evaluation) // Outputs: \u0026#34;0.00\u0026#34; } This is just a basic example, and there are many other things you can do with the library (e.g., make moves, get a list of legal moves, etc.). You can find more information about the library and its features in the documentation at https://godoc.org/github.com/notnil/chess.\nConclusion In this blog post, we\u0026rsquo;ve seen how to use the Stockfish chess engine and the Go programming language to evaluate chess positions. This can be a useful tool for chess players looking to improve their game, as well as for developers looking to build chess-related applications. With a little bit of Go code, it\u0026rsquo;s easy to get powerful evaluations of any chess position from Stockfish.\n","permalink":"https://manuelfedele.github.io/posts/evaluate-chess-position-with-golang/","summary":"\u003cp\u003eEvaluating chess positions is an important part of chess strategy and can help players make more informed decisions about which moves to make. In this blog post, we\u0026rsquo;ll look at how to use the powerful Stockfish chess engine and the Go programming language to evaluate chess positions.\u003c/p\u003e\n\u003ch2 id=\"installing-stockfish\"\u003eInstalling Stockfish\u003c/h2\u003e\n\u003cp\u003eBefore we can start using Stockfish, we need to install it. On Linux and macOS, you can install Stockfish using the package manager of your choice (e.g., apt-get, Homebrew, etc.). On Windows, you can download a pre-compiled executable from the Stockfish website (\u003ca href=\"https://stockfishchess.org/download/)\"\u003ehttps://stockfishchess.org/download/)\u003c/a\u003e.\u003c/p\u003e","title":"Evaluate Chess Position With Golang"},{"content":"Memoization is a technique that is used to speed up the execution of a function by storing the results of expensive function calls and returning the cached result when the same input occurs again. This can be particularly useful for algorithms that have a large number of recursive calls or for functions that are called multiple times with the same input.\nIn Go, it is easy to implement memoization using a simple map. For example, consider the following function that calculates the nth number in the Fibonacci sequence:\nfunc fib(n int) int { if n == 0 || n == 1 { return n } return fib(n-1) + fib(n-2) } This function has a time complexity of O(2^n), which can be very slow for large values of n. We can use memoization to speed up the function by storing the results of each recursive call in a map:\nvar cache = make(map[int]int) func fib(n int) int { if n == 0 || n == 1 { return n } if v, ok := cache[n]; ok { return v } result := fib(n-1) + fib(n-2) cache[n] = result return result } Now, the time complexity of the function is O(n), which is much faster for large values of n.\nOne thing to note is that the map used for memoization should be declared as a global variable, since it needs to be accessible from all recursive calls. It is also a good idea to clear the map after each function call, to avoid using up too much memory.\nOverall, memoization is a simple but powerful technique for optimizing the performance of recursive functions in Go. It is especially useful for functions that are called multiple times with the same input, and can significantly improve the speed of your program.\n","permalink":"https://manuelfedele.github.io/posts/the-memoization-technique/","summary":"\u003cp\u003eMemoization is a technique that is used to speed up the execution of a function by storing the results of expensive function calls and returning the cached result when the same input occurs again. This can be particularly useful for algorithms that have a large number of recursive calls or for functions that are called multiple times with the same input.\u003c/p\u003e\n\u003cp\u003eIn Go, it is easy to implement memoization using a simple map. For example, consider the following function that calculates the nth number in the Fibonacci sequence:\u003c/p\u003e","title":"The Memoization Technique"},{"content":"Alpaca is a popular platform for automated trading, offering APIs for accessing real-time market data and placing trades. In this article, we will discuss how to use Alpaca\u0026rsquo;s APIs with Golang, a popular programming language known for its simplicity, performance, and concurrency support.\nBefore we dive into the details of using Alpaca\u0026rsquo;s APIs with Golang, let\u0026rsquo;s first understand the requirements of the application.\nRequirements The application should be able to connect to the Alpaca API and authenticate the user. The application should be able to retrieve real-time market data and place trades. The application should be able to retrieve the user\u0026rsquo;s account information and trade history.\nSetting up the Alpaca API To use the Alpaca API, you will need to sign up for an Alpaca account and obtain an API key. The API key is used to authenticate your requests to the API.\nOnce you have an API key, you can use the following Go libraries to access the Alpaca API:\nalpaca-trade-api-go: A Go client library for the Alpaca Trade API. alpaca-go: A Go wrapper for the Alpaca Trade API. Both libraries provide functions for accessing various types of data and placing trades, including real-time quotes, historical data, and account information.\nFor example, the following code snippet demonstrates how to place a market order using the alpaca-trade-api-go library:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; alpaca \u0026#34;github.com/alpacahq/alpaca-trade-api-go\u0026#34; ) func main() { // Set the API key and secret alpaca.SetAPIKey(\u0026#34;YOUR_API_KEY\u0026#34;) alpaca.SetAPISecret(\u0026#34;YOUR_API_SECRET\u0026#34;) // Place a market order to buy 100 shares of the stock \u0026#34;AAPL\u0026#34; order, err := alpaca.PlaceOrder(\u0026#34;AAPL\u0026#34;, 100, alpaca.MarketOrder, alpaca.Buy, \u0026#34;day\u0026#34;) if err != nil { log.Fatal(err) } // Print the order details fmt.Printf(\u0026#34;%+v\\n\u0026#34;, order) } This will place a market order to buy 100 shares of the stock \u0026ldquo;AAPL\u0026rdquo; and print the order details, including the order ID and status.\nRetrieving Market Data In addition to placing trades, you can also use the Alpaca API to retrieve real-time market data. The following code snippet demonstrates how to retrieve the real-time quote for the stock \u0026ldquo;AAPL\u0026rdquo; using the alpaca-trade-api-go library:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; alpaca \u0026#34;github.com/alpacahq/alpaca-trade-api-go\u0026#34; ) func main() { // Set the API key and secret alpaca.SetAPIKey(\u0026#34;YOUR_API_KEY\u0026#34;) alpaca.SetAPISecret(\u0026#34;YOUR_API_SECRET\u0026#34;) // Retrieve the real-time quote for the stock \u0026#34;AAPL\u0026#34; quote, err := alpaca.GetQuote(\u0026#34;AAPL\u0026#34;) if err != nil { log.Fatal(err) } // Print the quote fmt.Printf(\u0026#34;%+v\\n\u0026#34;, quote) } This will retrieve the real-time quote for the stock \u0026ldquo;AAPL\u0026rdquo; and print the quote, including the current price, volume, and other information.\nRetrieving Account Information and Trade History In addition to retrieving market data, you can also use the Alpaca API to retrieve information about your account and trade history. The following code snippet demonstrates how to retrieve your account balance and trade history using the alpaca-trade-api-go library:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; alpaca \u0026#34;github.com/alpacahq/alpaca-trade-api-go\u0026#34; ) func main() { // Set the API key and secret alpaca.SetAPIKey(\u0026#34;YOUR_API_KEY\u0026#34;) alpaca.SetAPISecret(\u0026#34;YOUR_API_SECRET\u0026#34;) // Retrieve the account balance balance, err := alpaca.GetAccount() if err != nil { log.Fatal(err) } // Print the account balance fmt.Printf(\u0026#34;Account balance: %+v\\n\u0026#34;, balance) // Retrieve the trade history trades, err := alpaca.ListTrades() if err != nil { log.Fatal(err) } // Print the trade history fmt.Printf(\u0026#34;Trade history: %+v\\n\u0026#34;, trades) } This will retrieve your account balance and trade history and print the information.\nConclusion In this tutorial, we have discussed how to use Alpaca\u0026rsquo;s APIs with Golang to automate trading. We have seen how to connect to the Alpaca API, place trades, retrieve market data, and retrieve account information and trade history. With these tools and techniques, you can create a powerful and efficient application for automated trading.\n","permalink":"https://manuelfedele.github.io/posts/trading-with-alpaca-and-golang/","summary":"\u003cp\u003eAlpaca is a popular platform for automated trading, offering APIs for accessing real-time market data and placing trades. In this article, we will discuss how to use Alpaca\u0026rsquo;s APIs with Golang, a popular programming language known for its simplicity, performance, and concurrency support.\u003c/p\u003e\n\u003cp\u003eBefore we dive into the details of using Alpaca\u0026rsquo;s APIs with Golang, let\u0026rsquo;s first understand the requirements of the application.\u003c/p\u003e\n\u003ch3 id=\"requirements\"\u003eRequirements\u003c/h3\u003e\n\u003cp\u003eThe application should be able to connect to the Alpaca API and authenticate the user.\nThe application should be able to retrieve real-time market data and place trades.\nThe application should be able to retrieve the user\u0026rsquo;s account information and trade history.\u003c/p\u003e","title":"Trading With Alpaca and Golang"},{"content":"Introduction Elastic, also known as Elasticsearch, is a powerful search and analytics engine that can be used to index, search, and analyze large volumes of data quickly and in near real-time. It is open-source and built on top of the Lucene library. In this blog post, we will go over the basics of Elastic and how to get started using it.\nInstallation The first step to using Elastic is to install it on your system. You can download the latest version of Elastic from the official website (https://www.elastic.co/downloads/elasticsearch) and install it according to the instructions provided. Once installed, you can start the Elastic service on your machine and access it through a web interface at http://localhost:9200.\nIndexing Data In Elastic, data is stored in indices, which are similar to tables in a relational database. To index data, you need to create an index and then add documents to it. You can create an index using the Elastic API, for example by sending a PUT request to the following endpoint: http://localhost:9200/index_name. Once the index is created, you can add documents to it by sending a POST request to the same endpoint with your JSON data.\nSearching Data Once you have indexed your data, you can search for it using the Elastic API. The most basic search is a match query, which returns all documents that match a specified term. You can perform a match query by sending a GET request to the following endpoint: http://localhost:9200/index_name/_search?q=your_search_term. You can also use more advanced search options such as filtering, aggregation, and highlighting.\nAnalyzing Data Elastic also provides a variety of tools for analyzing data, such as the ability to create visualizations and dashboards using Kibana. Kibana is a separate product from Elastic but it can be easily integrated with it. With Kibana, you can create charts, tables, and maps to visualize your data and gain insights from it.\nIndexing Data Creating an Index The most basic way to create an index in Elasticsearch is to send a PUT request to the Elasticsearch API endpoint for the index you want to create. For example, to create an index named \u0026ldquo;customers\u0026rdquo;, you can use the following command: curl -XPUT http://localhost:9200/customers This will create an index with the default settings and mapping. Customizing Mappings When you create an index, Elasticsearch automatically creates a mapping for it, which defines how the data in the index should be treated. You can customize the mapping for an index by sending a PUT request with a JSON document that defines the mappings you want to use. For example, to create an index named \u0026ldquo;customers\u0026rdquo; with mappings for the fields \u0026ldquo;name\u0026rdquo;, \u0026ldquo;age\u0026rdquo;, and \u0026ldquo;address\u0026rdquo;, you can use the following command: curl -XPUT http://localhost:9200/customers -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;age\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; }, \u0026#34;address\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } }\u0026#39; Update Mappings If you have already created an index and want to update its mappings, you can use the _mapping endpoint and send a PUT request with the updated mappings. For example, to update the mappings for the \u0026ldquo;customers\u0026rdquo; index, you can use the following command: curl -XPUT http://localhost:9200/customers/_mapping -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;age\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; }, \u0026#34;address\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } }\u0026#39; Deleting an Index If you no longer need an index, you can delete it by sending a DELETE request to the Elasticsearch API endpoint for the index. For example, to delete the \u0026ldquo;customers\u0026rdquo; index, you can use the following command: curl -XDELETE http://localhost:9200/customers Searching Data Basic Search The most basic search in Elasticsearch is the match query, which returns all documents that match a specified term. For example, to search for all documents in the \u0026ldquo;customers\u0026rdquo; index that have the term \u0026ldquo;John\u0026rdquo; in the \u0026ldquo;name\u0026rdquo; field, you can use the following command: curl -XGET http://localhost:9200/customers/_search -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34; } } }\u0026#39; Advanced Search Elasticsearch also offers a wide range of search options to customize your search queries, such as filtering, aggregation, and highlighting. For example, to search for all documents in the \u0026ldquo;customers\u0026rdquo; index that have the term \u0026ldquo;John\u0026rdquo; in the \u0026ldquo;name\u0026rdquo; field and are over the age of 30, you can use the following command: curl -XGET http://localhost:9200/customers/_search -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34; } }, \u0026#34;filter\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;age\u0026#34;: { \u0026#34;gt\u0026#34;: 30 } } } } } }\u0026#39; Searching with Aggregations You can also perform data aggregations on your search results to gain insights into your data. For example, to search for all documents in the \u0026ldquo;customers\u0026rdquo; index and group them by the \u0026ldquo;age\u0026rdquo; field, you can use the following command: curl -XGET http://localhost:9200/customers/_search -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;aggs\u0026#34;: { \u0026#34;group_by_age\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;age\u0026#34; } } } }\u0026#39; Searching with Highlighting You can also use highlighting to highlight the matched search terms in the search results. For example, to search for all documents in the \u0026ldquo;customers\u0026rdquo; index that have the term \u0026ldquo;John\u0026rdquo; in the \u0026ldquo;name\u0026rdquo; field and highlight the matched term, you can use the following command: curl -XGET http://localhost:9200/customers/_search -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;name\u0026#34;: {} } } }\u0026#39; Aggregated Indexes Sometimes you need to perform aggregation and never use the \u0026ldquo;raw data\u0026rdquo;. In this cases, creating an aggregated index could improve the performance. An aggregation is a way to group, filter or calculate the statistics of your data. There are different types of aggregations in Elasticsearch, such as terms, range, date histogram, and more. For example, you can use the term aggregation to group your data by a specific field, like age, or use range aggregation to filter your data based on a specific range of values.\nCreating Aggregated Indexes You can create an index and perform aggregations on it by sending a PUT request to the Elasticsearch API endpoint for the index you want to create, and including the aggregations in the request body. For example, to create an index named \u0026ldquo;sales\u0026rdquo; with an aggregation on the \u0026ldquo;price\u0026rdquo; field, you can use the following command:\ncurl -XPUT http://localhost:9200/sales -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, \u0026#34;date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_price\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } } } }\u0026#39; Using Aggregations Once you have created an aggregated index, you can use the Elasticsearch API to perform search queries on it and retrieve the aggregated results. For example, to retrieve the average price of all documents in the \u0026ldquo;sales\u0026rdquo; index, you can use the following command:\ncurl -XGET http://localhost:9200/sales/_search -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;aggs\u0026#34;: { \u0026#34;avg_price\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } } } }\u0026#39; Combining Aggregations You can also combine multiple aggregations to gain more insights from your data. For example, you can use a terms aggregation to group your data by a specific field, like product type, and then use a range aggregation to filter your data based on a specific range of values.\ncurl -XGET http://localhost:9200/sales/_search -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;aggs\u0026#34;: { \u0026#34;product_type\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;product_type\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;price_range\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;to\u0026#34;: 10 }, { \u0026#34;from\u0026#34;: 10, \u0026#34;to\u0026#34;: 20 }, { \u0026#34;from\u0026#34;: 20 } ] } } } } } }\u0026#39; In this example, the query is grouping the documents by \u0026ldquo;product_type\u0026rdquo; field and then filters them by price range. This way you can have a better understanding of how different product types are performing based on the price range.\nAnother example could be:\ncurl -XGET http://localhost:9200/sales/_search -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;aggs\u0026#34;: { \u0026#34;by_customer_id\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;customer_id\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_price\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } } } } } }\u0026#39; This way you can have a better understanding of how different customers are performing based on the average price of their purchases.\nIn conclusion, Elastic is a powerful tool for indexing, searching, and analyzing large volumes of data. It is easy to get started with, and the possibilities are endless with the wide range of features it offers. This is just the tip of the iceberg, and there is a lot more to explore. Give Elastic a try and see how it can help you with your big data needs.\u0026quot;\n","permalink":"https://manuelfedele.github.io/posts/a-quick-overview-of-elasticsearch/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eElastic, also known as Elasticsearch, is a powerful search and analytics engine that can be used to index, search, and analyze large volumes of data quickly and in near real-time. It is open-source and built on top of the Lucene library. In this blog post, we will go over the basics of Elastic and how to get started using it.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ch3 id=\"installation\"\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eThe first step to using Elastic is to install it on your system. You can download the latest version of Elastic from the official website (\u003ca href=\"https://www.elastic.co/downloads/elasticsearch\"\u003ehttps://www.elastic.co/downloads/elasticsearch\u003c/a\u003e) and install it according to the instructions provided. Once installed, you can start the Elastic service on your machine and access it through a web interface at http://localhost:9200.\u003c/p\u003e","title":"A Quick Overview of Elasticsearch"},{"content":"Unit testing is a software testing technique in which individual units (smallest testable parts) of a software application are tested in isolation from the rest of the application. The goal of unit testing is to validate that each unit of the application is working as intended and meets the specified requirements.\nIn Go, the testing package provides support for writing unit tests. To write unit tests, you need to create a file with a name that ends in _test.go and place it in the same package as the code you want to test. The testing package also provides a testing.T type that represents a testing context and has methods for reporting test failures and logging messages.\nHere is an example of a unit test in Go:\nfunc TestSum(t *testing.T) { result := Sum(2, 3) expected := 5 if result != expected { t.Errorf(\u0026#34;Expected %d, got %d\u0026#34;, expected, result) } } In this example, we are testing a function called Sum that takes two integers as arguments and returns their sum. The test function uses the Errorf method of the testing.T type to report a test failure if the result of the Sum function does not match the expected value.\nTo run the unit tests in Go, you can use the go test command. This command will scan all the files in the current directory and its subdirectories for files with names that end in _test.go, and run the test functions in those files.\nIn Go, test functions have the following conventions:\nThey are always defined in a file with a name that ends in _test.go. They are always in the same package as the code they are testing. They have the name Test followed by the name of the function being tested, with the first letter of the function name in upper case. They take a single argument of type *testing.T, which represents the testing context. Assertion functions: The testing package provides a number of assertion functions that you can use to compare the actual result of a test with the expected result. For example, the Equal function can be used to compare two values for equality: func TestSum(t *testing.T) { result := Sum(2, 3) expected := 5 if result != expected { t.Errorf(\u0026#34;Expected %d, got %d\u0026#34;, expected, result) } } can be rewritten as:\nfunc TestSum(t *testing.T) { result := Sum(2, 3) expected := 5 if !reflect.DeepEqual(result, expected) { t.Errorf(\u0026#34;Expected %d, got %d\u0026#34;, expected, result) } } Test coverage: Go provides a tool called go test that can be used to measure the test coverage of your code. To see the test coverage of your code, you can use the -cover flag when running go test. This will output a report showing the percentage of your code that is covered by tests. It is generally considered good practice to aim for a high test coverage, as it helps ensure that all parts of the code are being tested.\nMocking dependencies: In some cases, you may need to test a function that depends on other functions or external resources, such as a database or a web service. In these cases, you can use a technique called \u0026ldquo;mocking\u0026rdquo; to isolate the function being tested from its dependencies. To mock a dependency, you can create a \u0026ldquo;fake\u0026rdquo; implementation of the dependency that the test function can use instead of the real implementation. This allows you to test the function being tested in isolation, without the need to set up the real dependencies.\nUnit testing is an important part of the software development process because it helps ensure that the application is working as intended and catches bugs early on. It is especially important in Go, where the emphasis on writing small, modular functions makes it easy to write unit tests for individual components of the application.\n","permalink":"https://manuelfedele.github.io/posts/unit-testing-in-golang/","summary":"\u003cp\u003eUnit testing is a software testing technique in which individual units (smallest testable parts) of a software application are tested in isolation from the rest of the application. The goal of unit testing is to validate that each unit of the application is working as intended and meets the specified requirements.\u003c/p\u003e\n\u003cp\u003eIn Go, the testing package provides support for writing unit tests. To write unit tests, you need to create a file with a name that ends in _test.go and place it in the same package as the code you want to test. The testing package also provides a testing.T type that represents a testing context and has methods for reporting test failures and logging messages.\u003c/p\u003e","title":"Unit Testing in Golang"},{"content":"Git is a version control system that is widely used in software development to track and manage changes to source code. Two of the most common Git commands that developers use to retrieve updates from a remote repository are git pull and git fetch. While these two commands may seem similar at first glance, they have some important differences that are worth understanding.\nThe git pull command is a combination of git fetch and git merge. When you run git pull, Git first retrieves the latest version of the repository from the remote server. It then merges the changes into your local copy of the repository. This means that git pull not only downloads the changes from the remote repository, but it also integrates them into your local copy.\nOn the other hand, the git fetch command is used to download the latest version of a repository from a remote server, but it does not integrate the changes into the local copy. Instead, it stores the changes in a separate branch, called a \u0026ldquo;remote branch\u0026rdquo;, which can be inspected but not modified. This allows you to review the changes before deciding whether to integrate them into your local copy.\nSo, the main difference between git pull and git fetch is that git pull updates your local copy with the changes from the remote repository and merges the changes into your local copy, while git fetch only downloads the changes from the remote repository and stores them in a separate branch for you to review and merge manually.\nIt\u0026rsquo;s worth noting that git fetch is often used as the first step in a workflow that involves reviewing and merging changes from a remote repository. For example, you might use git fetch to download the latest changes from a remote repository, switch to a new branch to review the changes, and then use git merge to integrate the changes into your local copy.\nIn summary, git pull and git fetch are both useful commands for retrieving updates from a remote repository, but they have different purposes. git pull is used to update your local copy with the changes from a remote repository and merge the changes into your local copy, while git fetch is used to download the changes from the remote repository and store them in a separate branch for you to review and merge manually. Understanding the differences between these two commands can help you use Git more effectively in your workflow.\n","permalink":"https://manuelfedele.github.io/posts/what-is-the-difference-between-git-pull-and-git-fetch/","summary":"\u003cp\u003eGit is a version control system that is widely used in software development to track and manage changes to source code. Two of the most common Git commands that developers use to retrieve updates from a remote repository are \u003ccode\u003egit pull\u003c/code\u003e and \u003ccode\u003egit fetch\u003c/code\u003e. While these two commands may seem similar at first glance, they have some important differences that are worth understanding.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003egit pull\u003c/code\u003e command is a combination of \u003ccode\u003egit fetch\u003c/code\u003e and \u003ccode\u003egit merge\u003c/code\u003e. When you run git pull, Git first retrieves the latest version of the repository from the remote server. It then merges the changes into your local copy of the repository. This means that git pull not only downloads the changes from the remote repository, but it also integrates them into your local copy.\u003c/p\u003e","title":"What Is the Difference Between Git Pull and Git Fetch"},{"content":"Streaming video with Go (Golang) is a popular choice for building efficient and scalable video streaming applications. In this blog post, we\u0026rsquo;ll discuss some of the key concepts and considerations for building a video streaming application with Go.\nOne of the first things to consider when building a video streaming application is the underlying video format. Go has built-in support for working with a variety of video formats, including MP4, FLV, and AVI. However, the most common format for streaming video is probably H.264, which is a popular choice due to its high compression ratio and wide compatibility with a variety of devices and platforms.\nAnother important consideration is the protocol used to deliver the video to the client. The most common protocol for streaming video is probably HTTP-based streaming, such as HLS (HTTP Live Streaming) or DASH (Dynamic Adaptive Streaming over HTTP). These protocols allow the video to be delivered over the HTTP protocol, which is widely supported by web browsers and other client devices.\nTo deliver the video using HTTP-based streaming, the server needs to be able to generate and serve the necessary video segments and playlists on-the-fly. Go has a number of libraries and frameworks available that can help with this, such as GStreamer and FFmpeg.\nIn addition to the video format and delivery protocol, there are a number of other considerations to keep in mind when building a video streaming application with Go. For example, you\u0026rsquo;ll need to think about things like scalability, reliability, and security. You may also need to consider issues like bandwidth limitations and network latency, which can have a big impact on the user experience.\nOverall, building a video streaming application with Go can be a great choice due to the language\u0026rsquo;s efficiency and scalability. By carefully considering the various factors involved and choosing the right tools and frameworks, it\u0026rsquo;s possible to build a high-quality and reliable video streaming application with Go.\nWhy is Go a good candidate?\nGo\u0026rsquo;s support for concurrency makes it a good choice for building scalable video streaming applications that can handle a large number of concurrent connections.\nGo\u0026rsquo;s built-in support for HTTP and other network protocols makes it easy to implement the server-side of a video streaming application. You can use the net/http package to build an HTTP server that serves the video segments and playlists to clients.\nTo generate the video segments and playlists on the fly, you can use a library or framework like GStreamer or FFmpeg. These tools provide a powerful set of APIs for working with video and audio streams, and can be used to transcode, mux, and demux video and audio as needed.\nWhen building a video streaming application, it\u0026rsquo;s important to consider issues like scalability, reliability, and security. You may want to use a load balancer to distribute incoming connections across a cluster of servers, and you may want to implement measures like rate limiting and authentication to protect against malicious users.\nFinally, keep in mind that video streaming applications can be resource-intensive, both in terms of CPU and bandwidth. It\u0026rsquo;s important to optimize your Go code and use tools like caching and compression to minimize the impact on your servers and network.\nHere is a simple example of how you might use Go to serve a video file over HTTP:\npackage main import ( \u0026#34;net/http\u0026#34; \u0026#34;log\u0026#34; ) func main() { http.HandleFunc(\u0026#34;/video\u0026#34;, func(w http.ResponseWriter, r *http.Request) { http.ServeFile(w, r, \u0026#34;myvideo.mp4\u0026#34;) }) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } This code sets up an HTTP server that listens on port 8080, and serves the file myvideo.mp4 whenever a client makes a request to /video. The client can then play the video by opening a URL like http://localhost:8080/video in a web browser or media player.\nKeep in mind that this is just a very basic example, and there are many additional considerations to take into account when building a production-grade video streaming application. For example, you\u0026rsquo;ll likely want to use a more advanced server framework, implement streaming protocols like HLS or DASH, and handle issues like scalability and security.\nHere is an example of how you might use the GStreamer library to generate HLS segments on-the-fly in Go:\npackage main import ( \u0026#34;github.com/ziutek/gst\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; ) func main() { gst.Init() // Create a pipeline to read the video file and encode it to H.264 pipeline := gst.ParseLaunch(\u0026#34;filesrc location=myvideo.mp4 ! qtdemux ! h264parse ! mpegtsmux ! hlssink target-duration=10 playlist-length=3\u0026#34;) // Set up an HTTP server to serve the HLS segments and playlist http.HandleFunc(\u0026#34;/video\u0026#34;, func(w http.ResponseWriter, r *http.Request) { http.ServeFile(w, r, r.URL.Path[1:]) }) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) // Run the pipeline pipeline.SetState(gst.StatePlaying) // Wait for the pipeline to finish pipeline.Wait() } This code sets up an HTTP server that listens on port 8080, and serves the HLS segments and playlist generated by the GStreamer pipeline. The pipeline reads the input video file myvideo.mp4, transcodes it to H.264, and muxes it into MPEG-TS segments using the mpegtsmux element. It then uses the hlssink element to generate an HLS playlist and segments, with a target duration of 10 seconds and a playlist length of 3 segments.\nLast but not least, here is an example of how you might use the FFmpeg library to generate HLS segments on-the-fly in Go:\npackage main import ( \u0026#34;github.com/griffithsh/go-ffmpeg-cmd\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; ) func main() { // Set up the FFmpeg command cmd := ffmpeg.Command(\u0026#34;ffmpeg\u0026#34;) // Set the input and output options cmd.Input(\u0026#34;myvideo.mp4\u0026#34;) cmd.Output(\u0026#34;video.m3u8\u0026#34;).Type(\u0026#34;hls\u0026#34;).HlsListSize(3).HlsSegmentDuration(10) // Start the command err := cmd.Start() if err != nil { log.Fatal(err) } // Set up an HTTP server to serve the HLS segments and playlist http.HandleFunc(\u0026#34;/video\u0026#34;, func(w http.ResponseWriter, r *http.Request) { http.ServeFile(w, r, r.URL.Path[1:]) }) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) // Wait for the command to finish err = cmd.Wait() if err != nil { log.Fatal(err) } } This code sets up an HTTP server that listens on port 8080, and serves the HLS segments and playlist generated by the FFmpeg command. The command reads the input video file myvideo.mp4 and transcodes it to H.264, generating an HLS playlist and segments with a target duration of 10 seconds and a playlist length of 3 segments.\nTo play the video, the client can open a URL like http://localhost:8080/video.m3u8 in a media player that supports HLS, such as VLC or QuickTime.\nAgain, keep in mind that this is just a basic example, and there are many additional options and configurations available for the GStreamer or for the FFmpeg command.\n","permalink":"https://manuelfedele.github.io/posts/streaming-video-with-golang/","summary":"\u003cp\u003eStreaming video with Go (Golang) is a popular choice for building efficient and scalable video streaming applications. In this blog post, we\u0026rsquo;ll discuss some of the key concepts and considerations for building a video streaming application with Go.\u003c/p\u003e\n\u003cp\u003eOne of the first things to consider when building a video streaming application is the underlying video format. Go has built-in support for working with a variety of video formats, including MP4, FLV, and AVI. However, the most common format for streaming video is probably H.264, which is a popular choice due to its high compression ratio and wide compatibility with a variety of devices and platforms.\u003c/p\u003e","title":"Streaming Video With Golang"},{"content":"Protocol buffers, also known as Protobuf, are a popular data serialization format used for communication between services. They are efficient, easy to use, and language-agnostic. In this article, we will look at how to use Protobuf with FastAPI, a modern, high-performance web framework for building APIs with Python.\nFirst, let\u0026rsquo;s start by installing the necessary dependencies. You will need to install fastapi, google-protobuf, and grpcio. You can do this by running the following command:\npip install fastapi google-protobuf grpcio Next, you will need to define your Protobuf message. You can do this by creating a .proto file and defining your message using the Protobuf syntax. For example:\nsyntax = \u0026#34;proto3\u0026#34;; message User { string name = 1; int32 age = 2; string email = 3; } Once you have defined your message, you will need to compile it using the protoc compiler. This will generate Python code that you can use to encode and decode your message. You can do this by running the following command:\nprotoc -I=. --python_out=. path/to/your_message.proto Now that you have compiled your message, you can use it in your FastAPI application. First, you will need to create a FastAPI application and define an endpoint that accepts a Protobuf message. You can do this by using the Body and PROTOBUF parameters from FastAPI\u0026rsquo;s pydantic module:\nfrom fastapi import FastAPI from pydantic import BaseModel, PROTOBUF from path.to.your_message_pb2 import User # import the generated Protobuf message app = FastAPI() class UserRequest(BaseModel): user: PROTOBUF(User) @app.post(\u0026#34;/users\u0026#34;) def create_user(request: UserRequest): user = request.user # do something with the user object return {\u0026#34;message\u0026#34;: \u0026#34;success\u0026#34;} Now, when you send a POST request to the /users endpoint with a Protobuf-encoded message as the request body, FastAPI will automatically decode it and pass the resulting User object to the endpoint handler.\nThe PROTOBUF parameter is used to specify that the request body is encoded using Protobuf. The User class is the compiled Protobuf message that you generated earlier using the protoc compiler.\nProtobuf is a powerful and efficient tool for communication between services, and FastAPI makes it easy to use in your Python applications. With just a few lines of code, you can add Protobuf support to your FastAPI application and start using it to efficiently transfer data between services.\n","permalink":"https://manuelfedele.github.io/posts/use-protobuf-with-fastapi/","summary":"\u003cp\u003eProtocol buffers, also known as Protobuf, are a popular data serialization format used for communication between services. They are efficient, easy to use, and language-agnostic. In this article, we will look at how to use Protobuf with FastAPI, a modern, high-performance web framework for building APIs with Python.\u003c/p\u003e\n\u003cp\u003eFirst, let\u0026rsquo;s start by installing the necessary dependencies. You will need to install fastapi, google-protobuf, and grpcio. You can do this by running the following command:\u003c/p\u003e","title":"Use Protobuf With Fastapi"},{"content":"Context in Go is a type that carries a request-scoped value across API boundaries. It is designed to be used in long-lived requests, such as an HTTP server handling multiple requests over the lifetime of a process.\nOne of the primary use cases of context is to cancel long-running operations. For example, if an HTTP server receives a request with a cancelation token, it can use that token to cancel the request if the client closes the connection. This helps to avoid resource leaks by freeing up resources that would have been used by the request.\nContext can also be used to store request-scoped values, such as a database connection or an authenticated user. This allows different parts of the codebase to share information about the request without relying on global state or passing the values through function parameters.\nTo use context in Go, you first need to import the \u0026ldquo;context\u0026rdquo; package. Then, you can create a context by calling the context.WithValue function, which takes a parent context and a key-value pair. You can then pass the context to functions that accept it as an argument.\nFor example, here\u0026rsquo;s how you might create a context with a cancelation token and pass it to an HTTP handler:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func handler(w http.ResponseWriter, r *http.Request) { ctx, cancel := context.WithTimeout(r.Context(), time.Minute) defer cancel() // Do some work with the request. // If the client closes the connection, // the cancel function is called and the work is stopped. } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } In this example, the context is created with a timeout of one minute. If the client closes the connection before the handler finishes, the cancel function is called and the work is stopped.\nIt\u0026rsquo;s important to note that context is not intended to be used as a means of communication between goroutines. While it is possible to use context to pass values between goroutines, it is generally better to use channels for this purpose.\nThe context package also provides several functions for creating and manipulating contexts. These include:\ncontext.Background: Returns a background context. This is the root of the context tree and is typically used as the parent of a context created with context.WithValue. context.WithValue: Returns a new context with the given key-value pair. The new context is derived from the parent context and carries the value across API boundaries. context.WithCancel: Returns a new context and a cancel function. The cancel function can be called to cancel the context and all contexts derived from it. context.WithTimeout: Returns a new context and a cancel function. The cancel function is called when the timeout elapses or when the parent context is canceled, whichever occurs first. context.WithDeadline: Returns a new context and a cancel function. The cancel function is called when the deadline elapses or when the parent context is canceled, whichever occurs first. Here\u0026rsquo;s an example of how you might use these functions to create a context with a cancelation token and a request-scoped value: package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { // Create a background context with a cancelation token. ctx, cancel := context.WithCancel(context.Background()) defer cancel() // Create a new context with a request-scoped value. valueCtx := context.WithValue(ctx, \u0026#34;user_id\u0026#34;, 123) // Create a new context with a deadline. deadline, _ := time.Parse(time.RFC3339, \u0026#34;2022-01-01T00:00:00Z\u0026#34;) deadlineCtx, _ := context.WithDeadline(valueCtx, deadline) // Print the values from all three contexts. fmt.Println(\u0026#34;Background:\u0026#34;, ctx.Value(\u0026#34;user_id\u0026#34;)) fmt.Println(\u0026#34;Value:\u0026#34;, valueCtx.Value(\u0026#34;user_id\u0026#34;)) fmt.Println(\u0026#34;Deadline:\u0026#34;, deadlineCtx.Value(\u0026#34;user_id\u0026#34;)) } In this example, we create a background context with a cancelation token and a request-scoped value using the context.WithCancel and context.WithValue functions. We then create a new context with a deadline using the context.WithDeadline function.\nWhen we print the values from the contexts, we can see that the values are propagated down the context tree. The background context has the cancelation token, the value context has both the cancelation token and the request-scoped value, and the deadline context has all three.\nIt\u0026rsquo;s worth noting that the context package is designed to be used with long-lived requests, such as an HTTP server handling multiple requests over the lifetime of a process. It is not intended to be used as a means of communication between goroutines or as a general-purpose synchronization mechanism.\nIn conclusion, the context package in Go is a powerful tool for carrying request-scoped values and cancelation tokens across API boundaries. It allows you to cancel long-running operations, store request-scoped values, and share information between parts of the codebase without relying on global state. If you\u0026rsquo;re building an application in Go, chances are you\u0026rsquo;ll find context to be an indispensable tool in your toolkit.\n","permalink":"https://manuelfedele.github.io/posts/the-context-package/","summary":"\u003cp\u003eContext in Go is a type that carries a request-scoped value across API boundaries. It is designed to be used in long-lived requests, such as an HTTP server handling multiple requests over the lifetime of a process.\u003c/p\u003e\n\u003cp\u003eOne of the primary use cases of context is to cancel long-running operations. For example, if an HTTP server receives a request with a cancelation token, it can use that token to cancel the request if the client closes the connection. This helps to avoid resource leaks by freeing up resources that would have been used by the request.\u003c/p\u003e","title":"The Context Package"},{"content":"Async/await is a powerful programming construct that allows you to write asynchronous code in a synchronous-like style. It was introduced in Python 3.5 as part of the asyncio module and has become a popular choice for writing concurrent and parallel code in Python.\nIn this post, we\u0026rsquo;ll take a look at what async/await is and how it works, as well as some of the benefits and drawbacks of using it. We\u0026rsquo;ll also see some examples of how to use async/await in Python to write efficient and scalable code.\nWhat is async/await? Async/await is a way of writing asynchronous code that looks and feels like synchronous code. It allows you to write code that can run concurrently with other tasks, without the overhead and complexity of using threads or processes.\nTo use async/await in Python, you need to use the async and await keywords. An async function is defined using the async def syntax, and it can contain one or more await expressions. An await expression is used to suspend the execution of the async function until the result of a call to an async function is available.\nHere\u0026rsquo;s an example of an async function that uses an await expression to wait for the result of a call to an async function:\nimport asyncio async def foo(): result = await asyncio.sleep(1) print(result) async def main(): await foo() asyncio.run(main()) In this example, the foo function uses the await keyword to suspend its execution until the asyncio.sleep function completes. The asyncio.sleep function is an async function that returns after a specified number of seconds, and it\u0026rsquo;s often used as a placeholder for more complex async operations.\nBenefits of async/await Async/await has several benefits compared to other approaches for writing concurrent and parallel code in Python:\nSimplicity: Async/await is easy to understand and use, especially for developers who are familiar with synchronous programming. It allows you to write asynchronous code in a linear, procedural style, without the need for complex control structures or callback functions.\nEfficiency: Async/await can lead to more efficient code, as it allows you to use fewer resources and avoid the overhead of using threads or processes. Async functions are implemented using coroutines, which are lightweight and can be scheduled and switched between efficiently.\nScalability: Async/await can help you write scalable code that can handle a large number of concurrent tasks. By using async/await, you can take advantage of Python\u0026rsquo;s event loop and avoid blocking the main thread, which can lead to better performance and higher throughput.\nDrawbacks of async/await Async/await is not a silver bullet and has some drawbacks that you should be aware of:\nComplexity: Async/await can make your code more complex, as it introduces new concepts and syntax that you need to learn and understand. Debugging async code can also be more challenging, as you need to deal with concurrency and the event loop. Performance: Async/await can lead to lower performance in some cases, especially when used with I/O-bound tasks that involve a lot of context switching between async functions. The overhead of scheduling and switching between coroutines can add up, leading to slower performance compared to other approaches like multithreading or multiprocessing. Not always the best choice: Async/await is not the best choice for every situation. It\u0026rsquo;s particularly well-suited for tasks that involve waiting for I/O operations, but it\u0026rsquo;s not as efficient for CPU-bound tasks that require a lot of computation. In these cases, you might be better off using multiprocessing or multithreading to take advantage of multiple CPU cores Limited support: Async/await is not supported by all libraries and frameworks in Python, and you might encounter compatibility issues when trying to use it with certain libraries. This can limit the usefulness of async/await and make it more difficult to use in some cases. When to use async/await Async/await is a powerful tool for writing concurrent and parallel code in Python, but it\u0026rsquo;s not always the best choice for every situation. Here are some cases where you might consider using async/await:\nI/O-bound tasks: Async/await is particularly well-suited for tasks that involve waiting for input/output operations, such as reading from a database, making HTTP requests, or interacting with the file system. By using async/await, you can avoid blocking the main thread and allow other tasks to run concurrently.\nHigh-concurrency tasks: Async/await can also be useful for tasks that need to handle a large number of concurrent requests or connections, such as servers or real-time apps. By using async/await, you can take advantage of Python\u0026rsquo;s event loop and scale your code more easily.\nCPU-bound tasks: Async/await is not the best choice for tasks that are CPU-bound and require a lot of computation. In these cases, you might be better off using multiprocessing or multithreading to take advantage of multiple CPU cores.\nExample: Async HTTP client To see how async/await works in practice, let\u0026rsquo;s look at an example of an async HTTP client using the aiohttp library. Here\u0026rsquo;s the code for the client:\nimport aiohttp async def fetch(session, url): async with session.get(url) as response: return await response.text() async def main(): async with aiohttp.ClientSession() as session: html = await fetch(session, \u0026#34;https://www.example.com\u0026#34;) print(html) asyncio.run(main()) In this example, the fetch function is an async function that uses the aiohttp library to make an HTTP GET request to a specified URL. It uses the await keyword to suspend its execution until the response is available. The main function is another async function that uses the aiohttp library to create a client session and then calls the fetch function to get the HTML of a website.\nYou can run this code using the asyncio.run function, which will execute the async function and wait for it to complete.\nConclusion Async/await is a powerful programming construct that allows you to write asynchronous code in a synchronous-like style. It can lead to more efficient and scalable code, especially for tasks that involve I/O operations or high concurrency. However, it\u0026rsquo;s not always the best choice for every situation, and you should consider the trade-offs and drawbacks before using it.\nI hope this post gave you a good understanding of async/await and how it works in Python. If you have any questions or comments, feel free to leave them below!\n","permalink":"https://manuelfedele.github.io/posts/understand-async-await-in-python/","summary":"\u003cp\u003eAsync/await is a powerful programming construct that allows you to write asynchronous code in a synchronous-like style. It was introduced in Python 3.5 as part of the asyncio module and has become a popular choice for writing concurrent and parallel code in Python.\u003c/p\u003e\n\u003cp\u003eIn this post, we\u0026rsquo;ll take a look at what async/await is and how it works, as well as some of the benefits and drawbacks of using it. We\u0026rsquo;ll also see some examples of how to use async/await in Python to write efficient and scalable code.\u003c/p\u003e","title":"Understand Async Await in Python"},{"content":" Install Rust Before you can start writing Rust code, you\u0026rsquo;ll need to install the Rust programming language on your computer. You can do this by following the instructions on the Rust website (https://www.rust-lang.org/tools/install).\nSet up a new Rust project Once you have Rust installed, you\u0026rsquo;ll need to create a new Rust project using the cargo command-line tool. Open a terminal and navigate to the directory where you want to store your project, then run the following command: cargo new telegram-bot \u0026ndash;bin. This will create a new Rust project called \u0026ldquo;telegram-bot\u0026rdquo; with a binary crate (executable).\nAdd dependencies In order to use the Telegram API and interact with the Telegram bot API, you\u0026rsquo;ll need to add some dependencies to your project. You can do this by adding the following to your Cargo.toml file:\n[dependencies] reqwest = \u0026#34;0.10.4\u0026#34; serde = { version = \u0026#34;1.0\u0026#34;, features = [\u0026#34;derive\u0026#34;] } serde_json = \u0026#34;1.0\u0026#34; Set up a bot In order to use the Telegram API, you\u0026rsquo;ll need to set up a bot and get an API token. To do this, follow the instructions on the Telegram API documentation (https://core.telegram.org/bots/api#authorizing-your-bot).\nCreate a function to send requests Once you have your API token, you can start interacting with the Telegram API by sending HTTP requests. To make it easier to send requests, you can create a function that takes care of all the details for you. Here\u0026rsquo;s an example of how you might do this:\nuse std::collections::HashMap; use reqwest::Client; fn send_request( client: \u0026amp;Client, api_token: \u0026amp;str, method: \u0026amp;str, params: \u0026amp;HashMap\u0026lt;\u0026amp;str, \u0026amp;str\u0026gt;, ) -\u0026gt; Result\u0026lt;serde_json::Value, reqwest::Error\u0026gt; { let mut url = String::new(); url.push_str(\u0026#34;https://api.telegram.org/bot\u0026#34;); url.push_str(api_token); url.push_str(\u0026#34;/\u0026#34;); url.push_str(method); let mut response = client.get(\u0026amp;url).query(params).send()?; let json: serde_json::Value = response.json()?; Ok(json) } This function takes a reqwest::Client, your API token, the name of the API method you want to call, and a HashMap of parameters. It constructs the URL for the API method, sends an HTTP GET request using reqwest, and returns the JSON response as a serde_json::Value.\nWrite your bot logic Now that you have the basic infrastructure in place, you can start writing the logic for your bot. There are many ways you can do this, but a common approach is to use a loop that listens for updates from the Telegram API and then responds to them appropriately. Here\u0026rsquo;s an example of how you might set this up:\nuse std::collections::HashMap; // Set up the client and API token let client = Client::new(); let api_token = \u0026#34;YOUR_API_TOKEN_HERE\u0026#34;; // Set the initial offset to 0 let mut offset = 0; // Set up a loop to listen for updates loop { // Set up the parameters for the getUpdates method let mut params = HashMap::new(); params.insert(\u0026#34;offset\u0026#34;, \u0026amp;offset.to_string()); params.insert(\u0026#34;timeout\u0026#34;, \u0026#34;30\u0026#34;); // Send the request and get the response let response = send_request(\u0026amp;client, \u0026amp;api_token, \u0026#34;getUpdates\u0026#34;, \u0026amp;params)?; // Check if there are any updates if let Some(updates) = response[\u0026#34;result\u0026#34;].as_array() { // Process each update for update in updates { // Increment the offset offset = update[\u0026#34;update_id\u0026#34;].as_u64().unwrap() + 1; // Do something with the update here... } } } This code sets up a loop that sends a request to the getUpdates method of the Telegram API every 30 seconds. If there are any updates, it processes them one by one and increments the offset variable to keep track of which updates have been processed.\nProcess the updates Inside the loop, you\u0026rsquo;ll need to write code to process each update and decide how to respond to it. Here\u0026rsquo;s an example of how you might do this:\nif let Some(message) = update[\u0026#34;message\u0026#34;].as_object() { // Get the chat ID and message text let chat_id = message[\u0026#34;chat\u0026#34;][\u0026#34;id\u0026#34;].as_i64().unwrap(); let text = message[\u0026#34;text\u0026#34;].as_str().unwrap(); // Check if the message is a command if text.starts_with(\u0026#34;/\u0026#34;) { let command = text[1..].split_whitespace().next().unwrap(); // Handle the command here... } else { // Send a message back to the chat let mut params = HashMap::new(); params.insert(\u0026#34;chat_id\u0026#34;, \u0026amp;chat_id.to_string()); params.insert(\u0026#34;text\u0026#34;, \u0026#34;You said: \u0026#34;); params.insert(\u0026#34;text\u0026#34;, text); let _response = send_request(\u0026amp;client, \u0026amp;api_token, \u0026#34;sendMessage\u0026#34;, \u0026amp;params)?; } } This code checks if the update contains a message, gets the chat ID and message text, and then checks if the message is a command (i.e., starts with a /). If it\u0026rsquo;s a command, you can handle it as you see fit. If it\u0026rsquo;s not a command, the code sends a message back to the chat with the original message text.\nI hope this gives you a good starting point for developing your own Telegram bot using Rust!\n","permalink":"https://manuelfedele.github.io/posts/create-a-telegram-bot-with-rust/","summary":"\u003col\u003e\n\u003cli\u003e\n\u003ch2 id=\"install-rust\"\u003eInstall Rust\u003c/h2\u003e\n\u003cp\u003eBefore you can start writing Rust code, you\u0026rsquo;ll need to install the Rust programming language on your computer. You can do this by following the instructions on the Rust website (\u003ca href=\"https://www.rust-lang.org/tools/install)\"\u003ehttps://www.rust-lang.org/tools/install)\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch2 id=\"set-up-a-new-rust-project\"\u003eSet up a new Rust project\u003c/h2\u003e\n\u003cp\u003eOnce you have Rust installed, you\u0026rsquo;ll need to create a new Rust project using the cargo command-line tool. Open a terminal and navigate to the directory where you want to store your project, then run the following command: cargo new telegram-bot \u0026ndash;bin. This will create a new Rust project called \u0026ldquo;telegram-bot\u0026rdquo; with a binary crate (executable).\u003c/p\u003e","title":"Create a Telegram Bot With Rust"},{"content":"FastAPI is a modern, fast, web framework for building APIs with Python 3.7 and above. It is built on top of Starlette, a lightweight ASGI framework, and uses the uvicorn ASGI server.\nHere is an example of how to create a web server with FastAPI and uvicorn:\nInstall FastAPI and uvicorn using pip: pip install fastapi uvicorn Create a file called main.py and import FastAPI: from fastapi import FastAPI app = FastAPI() Define a function that will be the endpoint for your API. This function should have a request parameter that specifies the HTTP request and a response parameter that specifies the HTTP response. You can use the @app.get decorator to define a function as a GET endpoint: @app.get(\u0026#34;/\u0026#34;) def read_root(request, response): response.status_code = 200 return {\u0026#34;Hello\u0026#34;: \u0026#34;World\u0026#34;} Run the web server using uvicorn: if __name__ == \u0026#34;__main__\u0026#34;: import uvicorn uvicorn.run(app, host=\u0026#34;0.0.0.0\u0026#34;, port=8000) This will start the web server on the specified host and port (in this case, 0.0.0.0 and 8000). You can then access the endpoint at http://0.0.0.0:8000/.\nYou can find more information about FastAPI and uvicorn in the FastAPI documentation and the Uvicorn documentation.\n","permalink":"https://manuelfedele.github.io/posts/create-webserver-with-fastapi-and-uvicorn/","summary":"\u003cp\u003eFastAPI is a modern, fast, web framework for building APIs with Python 3.7 and above. It is built on top of Starlette, a lightweight ASGI framework, and uses the uvicorn ASGI server.\u003c/p\u003e\n\u003cp\u003eHere is an example of how to create a web server with FastAPI and uvicorn:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eInstall FastAPI and uvicorn using pip:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install fastapi uvicorn\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"2\"\u003e\n\u003cli\u003eCreate a file called main.py and import FastAPI:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003efastapi\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eFastAPI\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eapp\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eFastAPI\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"3\"\u003e\n\u003cli\u003eDefine a function that will be the endpoint for your API. This function should have a request parameter that specifies the HTTP request and a response parameter that specifies the HTTP response. You can use the @app.get decorator to define a function as a GET endpoint:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nd\"\u003e@app.get\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;/\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eread_root\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erequest\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estatus_code\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e200\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;World\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"4\"\u003e\n\u003cli\u003eRun the web server using uvicorn:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"vm\"\u003e__name__\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003euvicorn\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003euvicorn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eapp\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ehost\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;0.0.0.0\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eport\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e8000\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis will start the web server on the specified host and port (in this case, 0.0.0.0 and 8000). You can then access the endpoint at http://0.0.0.0:8000/.\u003c/p\u003e","title":"Create a Webserver With Fastapi and Uvicorn"},{"content":"The keyword \u0026ldquo;yield\u0026rdquo; is a important part of the Python programming language, and it can be used in a number of different ways. In this article, we\u0026rsquo;ll take a closer look at what the yield keyword does in Python, and how it can be used to create more efficient and powerful programs.\nThe primary use of the yield keyword is to create a generator function. A generator function is a special type of function that produces a sequence of values, one at a time, when it is called. Unlike a regular function, which executes all of its code and returns a single value, a generator function can be paused at any point and resumed later, allowing it to produce a potentially infinite sequence of values.\nTo create a generator function in Python, you simply use the yield keyword in place of the return keyword. For example, here is a simple generator function that produces a sequence of numbers:\ndef count_up_to(max): count = 1 while count \u0026lt;= max: yield count count += 1 When this generator function is called, it will execute the code until it encounters the yield keyword, at which point it will pause and return the value of \u0026ldquo;count\u0026rdquo; to the caller. The generator function can then be resumed from where it left off the next time it is called, allowing it to produce the next value in the sequence.\nGenerator functions are particularly useful when you need to work with large sequences of data, as they allow you to process the data one piece at a time, rather than having to load it all into memory at once. They are also useful for creating infinite sequences, such as an infinite stream of random numbers.\nIn addition to creating generator functions, the yield keyword can also be used in the body of a regular function to produce a generator object. When a function contains a yield statement, it becomes a generator function, even if it does not use the \u0026ldquo;yield\u0026rdquo; keyword in its definition.\nHere is an example of a regular function that uses the yield keyword to produce a generator object:\ndef even_numbers(max): for num in range(max): if num % 2 == 0: yield num This function will produce a generator object that produces all of the even numbers up to the specified maximum value.\nIn summary, the yield keyword is a powerful tool in Python that allows you to create generator functions and generator objects. Generator functions and objects are useful for working with large sequences of data and for creating infinite sequences, and they can help you create more efficient and powerful programs in Python.\n","permalink":"https://manuelfedele.github.io/posts/what-does-yield-keyword-do-in-python/","summary":"\u003cp\u003eThe keyword \u0026ldquo;yield\u0026rdquo; is a important part of the Python programming language, and it can be used in a number of different ways. In this article, we\u0026rsquo;ll take a closer look at what the yield keyword does in Python, and how it can be used to create more efficient and powerful programs.\u003c/p\u003e\n\u003cp\u003eThe primary use of the yield keyword is to create a generator function. A generator function is a special type of function that produces a sequence of values, one at a time, when it is called. Unlike a regular function, which executes all of its code and returns a single value, a generator function can be paused at any point and resumed later, allowing it to produce a potentially infinite sequence of values.\u003c/p\u003e","title":"What Does Yield Keyword Do in Python"},{"content":"Protocol Buffers (Protobuf) is a language- and platform-neutral data serialization format developed by Google. It allows you to define data structures in a .proto file and then use code generation tools to generate code in various languages for working with those data structures.\nTo use Protobuf with Go, you\u0026rsquo;ll need to do the following:\nInstall the Protobuf compiler (protoc) and the Go Protobuf plugin:\n# Install protoc wget https://github.com/protocolbuffers/protobuf/releases/download/v3.14.0/protoc-3.14.0-linux-x86_64.zip unzip protoc-3.14.0-linux-x86_64.zip -d protoc3 sudo mv protoc3/bin/* /usr/local/bin/ # Install the Go Protobuf plugin go get -u github.com/golang/protobuf/protoc-gen-go Define your data structures in a .proto file. For example:\nsyntax = \u0026#34;proto3\u0026#34;; package example; message Person { string name = 1; int32 age = 2; bool is_employee = 3; } message Address { string street = 1; string city = 2; string state = 3; string zip = 4; } Use the Protobuf compiler to generate Go code from your .proto file:\nprotoc --go_out=. example.proto This will generate a Go file named example.pb.go that contains code for working with the data structures you defined in your .proto file.\nImport the generated code and use it in your Go program. For example:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;example\u0026#34; ) func main() { // Create a new Person object p := example.Person{ Name: \u0026#34;John Smith\u0026#34;, Age: 30, IsEmployee: true, Addresses: []*example.Address{ \u0026amp;example.Address{ Street: \u0026#34;123 Main Street\u0026#34;, City: \u0026#34;New York\u0026#34;, State: \u0026#34;NY\u0026#34;, Zip: \u0026#34;10001\u0026#34;, }, \u0026amp;example.Address{ Street: \u0026#34;456 Market Street\u0026#34;, City: \u0026#34;San Francisco\u0026#34;, State: \u0026#34;CA\u0026#34;, Zip: \u0026#34;94111\u0026#34;, }, }, } // Serialize the object to a Protobuf-encoded byte slice data, err := p.Marshal() if err != nil { fmt.Println(err) return } // Deserialize the byte slice back into a new Person object var p2 example.Person if err := p2.Unmarshal(data); err != nil { fmt.Println(err) return } fmt.Println(p2.Name, p2.Age, p2.IsEmployee) for _, addr := range p2.Addresses { fmt.Println(addr.Street, addr.City, addr.State, addr.Zip) } } In this example, we define a Person data structure with a name, age, and a list of addresses in a .proto file. We then use the Protobuf compiler to generate Go code for working with these data structures. In the Go code, we create a new Person object and serialize it to a Protobuf-encoded byte slice using the Marshal method. We then deserialize the byte slice back into a new Person object using the Unmarshal method. Finally, we print the deserialized object\u0026rsquo;s name, age, and list of addresses to the console.\nI hope this helps give you an idea of how to use Protobuf with Go. You can find more detailed documentation and examples in the Protobuf documentation.\n","permalink":"https://manuelfedele.github.io/posts/use-protobuf-with-golang/","summary":"\u003cp\u003eProtocol Buffers (Protobuf) is a language- and platform-neutral data serialization format developed by Google. It allows you to define data structures in a .proto file and then use code generation tools to generate code in various languages for working with those data structures.\u003c/p\u003e\n\u003cp\u003eTo use Protobuf with Go, you\u0026rsquo;ll need to do the following:\u003c/p\u003e\n\u003cp\u003eInstall the Protobuf compiler (protoc) and the Go Protobuf plugin:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Install protoc\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ewget https://github.com/protocolbuffers/protobuf/releases/download/v3.14.0/protoc-3.14.0-linux-x86_64.zip\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eunzip protoc-3.14.0-linux-x86_64.zip -d protoc3\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo mv protoc3/bin/* /usr/local/bin/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Install the Go Protobuf plugin\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ego get -u github.com/golang/protobuf/protoc-gen-go\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eDefine your data structures in a .proto file. For example:\u003c/p\u003e","title":"Use Protobuf With Golang"},{"content":"Creating a Command Line Application to Separate Files by Extension in Go If you have a folder with a large number of files and you want to organize them by file extension, you can create a command line application to do this automatically using Go. Go, also known as Golang, is a programming language developed by Google that is designed to be fast, statically-typed, and easy to learn. In this article, we\u0026rsquo;ll walk through the steps to create a Go program that separates files in a given folder into subfolders based on their extension.\nInstall Go Before you can start writing Go code, you\u0026rsquo;ll need to install the Go programming language on your machine. Go is available for Windows, macOS, and Linux, and you can download the latest release from the official Go website at https://golang.org/dl/. Once you\u0026rsquo;ve downloaded the installation package, follow the prompts to install Go on your system.\nSet Up Your Go Workspace Go uses a specific directory structure for organizing code, and it\u0026rsquo;s important to set this up correctly before you start writing your program. To set up your Go workspace, create a new directory called \u0026ldquo;go\u0026rdquo; in your home directory. Inside the \u0026ldquo;go\u0026rdquo; directory, create a subdirectory called \u0026ldquo;src\u0026rdquo; and another subdirectory called \u0026ldquo;bin\u0026rdquo;. Your Go workspace should now look like this:\n$HOME/go/ src/ bin/ Create a New Go Project Now that you have Go installed and your workspace set up, you can create a new Go project. To do this, open a terminal window and navigate to the \u0026ldquo;src\u0026rdquo; directory inside your Go workspace. From here, create a new directory for your project and navigate into it. For example, if your project is called \u0026ldquo;separate-files\u0026rdquo;, you might create a directory like this:\ncd $HOME/go/src mkdir separate-files cd separate-files Write the Go Code With your Go project set up, you can now start writing the code to separate the files by extension. Here\u0026rsquo;s a basic outline of what your program will do:\nParse the command line arguments to get the name of the folder to process. Read the list of files in the specified folder. Iterate over the list of files, creating a subfolder for each unique file extension. Move each file into the appropriate subfolder based on its extension. To start, create a new file called \u0026ldquo;main.go\u0026rdquo; in your project directory and add the following code: package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) func main() { // Parse the command line arguments args := os.Args[1:] if len(args) != 1 { fmt.Println(\u0026#34;Usage: separate-files \u0026lt;folder\u0026gt;\u0026#34;) os.Exit(1) } folder := args[0] // Read the list of files in the specified folder files, err := ioutil.ReadDir(folder) if err != nil { fmt.Printf(\u0026#34;Error reading files in folder %s: %v\\n\u0026#34;, folder, err) os.Exit(1) } // Iterate over the list of files and create a subfolder for each unique extension extensions := make(map[string]bool) for _, file := range files { if file.IsDir() { continue } extension := filepath.Ext(file.Name()) extensions[extension] = true } for extension := range extensions { // Create a subfolder for each unique extension err := os.Mkdir(filepath.Join(folder, extension), 0700) if err != nil { fmt.Printf(\u0026#34;Error creating subfolder for extension %s: %v\\n\u0026#34;, extension, err) continue } } // Move each file into the appropriate subfolder based on its extension for _, file := range files { if file.IsDir() { continue } extension := filepath.Ext(file.Name()) src := filepath.Join(folder, file.Name()) dst := filepath.Join(folder, extension, file.Name()) err := os.Rename(src, dst) if err != nil { fmt.Printf(\u0026#34;Error moving file %s to subfolder %s: %v\\n\u0026#34;, src, extension, err) continue } } } This code reads the list of files in the specified folder, creates a subfolder for each unique file extension, and then moves each file into the appropriate subfolder based on its extension.\nTo use this program, save the file and run it from the command line with the name of the folder you want to process as an argument. For example:\ngo run main.go /path/to/folder This will separate the files in the specified folder into subfolders based on their extension.\nTo package and distribute your Go code, you have a few options:\nCompile the code into a standalone executable: You can use the go build command to compile your Go code into a standalone executable that you can distribute to users. For example, to build an executable called \u0026ldquo;separate-files\u0026rdquo; from the code above, you can run the following command: go build -o separate-files main.go This will create an executable file called \u0026ldquo;separate-files\u0026rdquo; in the current directory that you can distribute to users.\nUse a package manager: There are several package managers available for Go that you can use to distribute your code, including dep, glide, and godep. These tools allow you to manage the dependencies for your project and build a single package that includes all of the necessary code and libraries.\nUse a containerization tool: You can also use a containerization tool like Docker to package your code and its dependencies into a container image that can be easily distributed and run on any machine. This is a good option if you want to ensure that your code runs consistently across different environments.\nOnce you have packaged your code, you can distribute it to users in a variety of ways, such as uploading it to a package repository, hosting it on a website, or sharing it via email or file sharing services.\n","permalink":"https://manuelfedele.github.io/posts/create-command-line-application-with-golang/","summary":"\u003ch2 id=\"creating-a-command-line-application-to-separate-files-by-extension-in-go\"\u003eCreating a Command Line Application to Separate Files by Extension in Go\u003c/h2\u003e\n\u003cp\u003eIf you have a folder with a large number of files and you want to organize them by file extension, you can create a command line application to do this automatically using Go. Go, also known as Golang, is a programming language developed by Google that is designed to be fast, statically-typed, and easy to learn. In this article, we\u0026rsquo;ll walk through the steps to create a Go program that separates files in a given folder into subfolders based on their extension.\u003c/p\u003e","title":"Create Command Line Application With Golang"},{"content":"GitHub Pages is a free service that allows you to host your website directly from a GitHub repository. It is a great platform for developers to showcase their portfolio, document their projects, or create a personal blog. In this article, we will walk through the steps of creating a static website using GitHub Pages and Gatsby, a popular static site generator.\nInstall Gatsby First, you will need to install Gatsby by running the following command in your terminal: npm install -g gatsby-cli Create a new Gatsby project gatsby new my-website This will create a new directory called my-website with the necessary files and dependencies for your Gatsby project.\nTest your Gatsby project To test your Gatsby project, navigate to the project directory and run the following command: gatsby develop This will start a development server and you should see a message indicating that your site is running at http://localhost:8000. Open this URL in your browser to see your Gatsby site.\nCreate a repository on GitHub Next, you will need to create a new repository on GitHub to host your website. To do this, log in to your GitHub account and click on the \u0026ldquo;New\u0026rdquo; button in the top right corner. Give your repository a name (e.g. my-website) and click \u0026ldquo;Create repository\u0026rdquo;.\nConnect your local project to the GitHub repository Next, you will need to connect your local Gatsby project to the GitHub repository you just created. To do this, open a terminal window and navigate to your Gatsby project directory. Then, run the following commands:\ngit init git remote add origin https://github.com/YOUR_USERNAME/my-website.git Replace YOUR_USERNAME with your actual GitHub username.\nDeploy your Gatsby site to GitHub Pages To deploy your Gatsby site to GitHub Pages, you will need to build the site and push the built files to the gh-pages branch of your repository. To do this, run the following commands: gatsby build git add -A git commit -m \u0026#34;Initial commit\u0026#34; git push origin master:gh-pages This will build your Gatsby site and push the built files to the gh-pages branch of your repository.\nEnable GitHub Pages Finally, you will need to enable GitHub Pages for your repository. To do this, go to the settings page for your repository and scroll down to the \u0026ldquo;GitHub Pages\u0026rdquo; section. Select the gh-pages branch as the source and click \u0026ldquo;Save\u0026rdquo;. Your Gatsby site should now be live on GitHub Pages at the URL https://YOUR_USERNAME.github.io/my-website.\nThat\u0026rsquo;s it! You have successfully created a static website using GitHub Pages and Gatsby. You can now customize your site by editing the files in your Gatsby project and deploying the updates to GitHub Pages.\n","permalink":"https://manuelfedele.github.io/posts/create-website-with-gatsby-and-github-pages/","summary":"\u003cp\u003eGitHub Pages is a free service that allows you to host your website directly from a GitHub repository. It is a great platform for developers to showcase their portfolio, document their projects, or create a personal blog. In this article, we will walk through the steps of creating a static website using GitHub Pages and Gatsby, a popular static site generator.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eInstall Gatsby\nFirst, you will need to install Gatsby by running the following command in your terminal:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003enpm install -g gatsby-cli\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"2\"\u003e\n\u003cli\u003eCreate a new Gatsby project\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egatsby new my-website\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis will create a new directory called my-website with the necessary files and dependencies for your Gatsby project.\u003c/p\u003e","title":"Create Website With Gatsby and Github Pages"},{"content":"Using Golang and Redis Redis is an in-memory data structure store that can be used as a database, cache, and message broker. It is known for its speed, simplicity, and flexibility. In this article, we will discuss how to use Redis with Golang, a popular programming language known for its simplicity, performance, and concurrency support.\nBefore we dive into the details of using Redis with Golang, let\u0026rsquo;s first understand the requirements of the application.\nRequirements The application should be able to connect to a Redis server. The application should be able to store and retrieve data from Redis. The application should be able to perform basic operations on Redis data structures, such as strings, lists, sets, and hashes. Setting up Redis To use Redis with Golang, you will need to have a Redis server installed and running on your system. You can download the latest version of Redis from the official website and follow the instructions to install and run the server.\nOnce the Redis server is running, you can use the following Go libraries to connect to the server and perform various operations:\nredis: A low-level Redis client library for Go. radix: A higher-level Redis client library for Go that provides a more convenient API and additional features. Connecting to Redis To connect to the Redis server using the redis library, you can use the Dial function:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/gomodule/redigo/redis\u0026#34; ) func main() { // Connect to the Redis server conn, err := redis.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:6379\u0026#34;) if err != nil { log.Fatal(err) } defer conn.Close() // Print the Redis version version, err := redis.String(conn.Do(\u0026#34;INFO\u0026#34;, \u0026#34;server\u0026#34;)) if err != nil { log.Fatal(err) } fmt.Println(version) } This will connect to the Redis server running on the local host at the default port (6379) and print the Redis version.\nTo connect to the Redis server using the radix library, you can use the Dial function:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/mediocregopher/radix/v3\u0026#34; ) func main() { // Connect to the Redis server conn, err := radix.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:6379\u0026#34;) if err != nil { log.Fatal(err) } defer conn.Close() // Print the Redis version var version string err = conn.Do(radix.Cmd(\u0026amp;version, \u0026#34;INFO\u0026#34;, \u0026#34;server\u0026#34;)) if err != nil { log.Fatal(err) } fmt.Println(version) } This will also connect to the Redis server running on the local host at the default port (6379).\nStoring and Retrieving Data Once you have connected to the Redis server, you can use various Redis commands to store and retrieve data.\nStrings To store and retrieve a string value using the redis library, you can use the Do function and the redis.String function:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/gomodule/redigo/redis\u0026#34; ) func main() { // Connect to the Redis server conn, err := redis.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:6379\u0026#34;) if err != nil { log.Fatal(err) } defer conn.Close() // Set the key \u0026#34;message\u0026#34; to the value \u0026#34;Hello, Redis!\u0026#34; _, err = conn.Do(\u0026#34;SET\u0026#34;, \u0026#34;message\u0026#34;, \u0026#34;Hello, Redis!\u0026#34;) if err != nil { log.Fatal(err) } // Retrieve the value of the key \u0026#34;message\u0026#34; value, err := redis.String(conn.Do(\u0026#34;GET\u0026#34;, \u0026#34;message\u0026#34;)) if err != nil { log.Fatal(err) } // Print the value fmt.Println(value) } This will set the key \u0026ldquo;message\u0026rdquo; to the value \u0026ldquo;Hello, Redis!\u0026rdquo; and retrieve the value of the key \u0026ldquo;message\u0026rdquo;, printing \u0026ldquo;Hello, Redis!\u0026rdquo;.\nTo store and retrieve a string value using the radix library, you can use the Do function and the radix.String function:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/mediocregopher/radix/v3\u0026#34; ) func main() { // Connect to the Redis server conn, err := radix.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:6379\u0026#34;) if err != nil { log.Fatal(err) } defer conn.Close() // Set the key \u0026#34;message\u0026#34; to the value \u0026#34;Hello, Redis!\u0026#34; err = conn.Do(radix.FlatCmd(nil, \u0026#34;SET\u0026#34;, \u0026#34;message\u0026#34;, \u0026#34;Hello, Redis!\u0026#34;)) if err != nil { log.Fatal(err) } // Retrieve the value of the key \u0026#34;message\u0026#34; var value string err = conn.Do(radix.Cmd(\u0026amp;value, \u0026#34;GET\u0026#34;, \u0026#34;message\u0026#34;)) if err != nil { log.Fatal(err) } // Print the value fmt.Println(value) } This will also set the key \u0026ldquo;message\u0026rdquo; to the value \u0026ldquo;Hello, Redis!\u0026rdquo; and retrieve the value of the key \u0026ldquo;message\u0026rdquo;, printing \u0026ldquo;Hello, Redis!\u0026rdquo;.\nLists To store and retrieve a list using the redis library, you can use the Do function and the redis.Strings function:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/gomodule/redigo/redis\u0026#34; ) func main() { // Connect to the Redis server conn, err := redis.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:6379\u0026#34;) if err != nil { log.Fatal(err) } defer conn.Close() // Store the list [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;] in the key \u0026#34;fruits\u0026#34; _, err = conn.Do(\u0026#34;RPUSH\u0026#34;, \u0026#34;fruits\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;) if err != nil { log.Fatal(err) } // Retrieve the list from the key \u0026#34;fruits\u0026#34; values, err := redis.Strings(conn.Do(\u0026#34;LRANGE\u0026#34;, \u0026#34;fruits\u0026#34;, 0, -1)) if err != nil { log.Fatal(err) } // Print the list fmt.Println(values) } This will store the list [\u0026ldquo;apple\u0026rdquo;, \u0026ldquo;banana\u0026rdquo;, \u0026ldquo;cherry\u0026rdquo;] in the key \u0026ldquo;fruits\u0026rdquo; and retrieve the list from the key \u0026ldquo;fruits\u0026rdquo;, printing [\u0026ldquo;apple\u0026rdquo;, \u0026ldquo;banana\u0026rdquo;, \u0026ldquo;cherry\u0026rdquo;].\nTo store and retrieve a list using the radix library, you can use the Do function and the radix.Strings function:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/mediocregopher/radix/v3\u0026#34; ) func main() { // Connect to the Redis server conn, err := radix.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:6379\u0026#34;) if err != nil { log.Fatal(err) } defer conn.Close() // Store the list [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;] in the key \u0026#34;fruits\u0026#34; err = conn.Do(radix.FlatCmd(nil, \u0026#34;RPUSH\u0026#34;, \u0026#34;fruits\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;)) if err != nil { log.Fatal(err) } // Retrieve the list from the key \u0026#34;fruits\u0026#34; var values []string err = conn.Do(radix.Cmd(\u0026amp;values, \u0026#34;LRANGE\u0026#34;, \u0026#34;fruits\u0026#34;, 0, -1)) if err != nil { log.Fatal(err) } // Print the list fmt.Println(values) } This will also store the list [\u0026ldquo;apple\u0026rdquo;, \u0026ldquo;banana\u0026rdquo;, \u0026ldquo;cherry\u0026rdquo;] in the key \u0026ldquo;fruits\u0026rdquo; and retrieve the list from the key \u0026ldquo;fruits\u0026rdquo;, printing [\u0026ldquo;apple\u0026rdquo;, \u0026ldquo;banana\u0026rdquo;, \u0026ldquo;cherry\u0026rdquo;].\nSets To store and retrieve a set using the redis library, you can use the Do function and the redis.Strings function:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/gomodule/redigo/redis\u0026#34; ) func main() { // Connect to the Redis server conn, err := redis.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:6379\u0026#34;) if err != nil { log.Fatal(err) } defer conn.Close() // Store the set {\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;} in the key \u0026#34;fruits\u0026#34; _, err = conn.Do(\u0026#34;SADD\u0026#34;, \u0026#34;fruits\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;) if err != nil { log.Fatal(err) } // Retrieve the set from the key \u0026#34;fruits\u0026#34; values, err := redis.Strings(conn.Do(\u0026#34;SMEMBERS\u0026#34;, \u0026#34;fruits\u0026#34;)) if err != nil { log.Fatal(err) } // Print the set fmt.Println(values) } This will store the set {\u0026ldquo;apple\u0026rdquo;, \u0026ldquo;banana\u0026rdquo;, \u0026ldquo;cherry\u0026rdquo;} in the key \u0026ldquo;fruits\u0026rdquo; and retrieve the set from the key \u0026ldquo;fruits\u0026rdquo;, printing {\u0026ldquo;apple\u0026rdquo;, \u0026ldquo;banana\u0026rdquo;, \u0026ldquo;cherry\u0026rdquo;}.\nTo store and retrieve a set using the radix library, you can use the Do function and the radix.Strings function:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/mediocregopher/radix/v3\u0026#34; ) func main() { // Connect to the Redis server conn, err := radix.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:6379\u0026#34;) if err != nil { log.Fatal(err) } defer conn.Close() // Store the set {\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;} in the key \u0026#34;fruits\u0026#34; err = conn.Do(radix.FlatCmd(nil, \u0026#34;SADD\u0026#34;, \u0026#34;fruits\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;)) if err != nil { log.Fatal(err) } // Retrieve the set from the key \u0026#34;fruits\u0026#34; var values []string err = conn.Do(radix.Cmd(\u0026amp;values, \u0026#34;SMEMBERS\u0026#34;, \u0026#34;fruits\u0026#34;)) if err != nil { log.Fatal(err) } // Print the set fmt.Println(values) } This will also store the set {\u0026ldquo;apple\u0026rdquo;, \u0026ldquo;banana\u0026rdquo;, \u0026ldquo;cherry\u0026rdquo;} in the key \u0026ldquo;fruits\u0026rdquo; and retrieve the set from the key \u0026ldquo;fruits\u0026rdquo;, printing {\u0026ldquo;apple\u0026rdquo;, \u0026ldquo;banana\u0026rdquo;, \u0026ldquo;cherry\u0026rdquo;}.\nHashes To store and retrieve a hash using the redis library, you can use the Do function and the redis.Strings function:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/gomodule/redigo/redis\u0026#34; ) func main() { // Connect to the Redis server conn, err := redis.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:6379\u0026#34;) if err != nil { log.Fatal(err) } defer conn.Close() // Store the hash {\u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;30\u0026#34;} in the key \u0026#34;person\u0026#34; _, err = conn.Do(\u0026#34;HMSET\u0026#34;, \u0026#34;person\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;Alice\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;30\u0026#34;) if err != nil { log.Fatal(err) } // Retrieve the hash from the key \u0026#34;person\u0026#34; values, err := redis.Strings(conn.Do(\u0026#34;HGETALL\u0026#34;, \u0026#34;person\u0026#34;)) if err != nil { log.Fatal(err) } // Print the hash fmt.Println(values) } This will store the hash {\u0026ldquo;name\u0026rdquo;: \u0026ldquo;Alice\u0026rdquo;, \u0026ldquo;age\u0026rdquo;: \u0026ldquo;30\u0026rdquo;} in the key \u0026ldquo;person\u0026rdquo; and retrieve the hash from the key \u0026ldquo;person\u0026rdquo;, printing {\u0026ldquo;name\u0026rdquo;, \u0026ldquo;Alice\u0026rdquo;, \u0026ldquo;age\u0026rdquo;, \u0026ldquo;30\u0026rdquo;}.\nTo store and retrieve a hash using the radix library, you can use the Do function and the radix.Map function:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/mediocregopher/radix/v3\u0026#34; ) func main() { // Connect to the Redis server conn, err := radix.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:6379\u0026#34;) if err != nil { log.Fatal(err) } defer conn.Close() // Store the hash {\u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;30\u0026#34;} in the key \u0026#34;person\u0026#34; err = conn.Do(radix.FlatCmd(nil, \u0026#34;HMSET\u0026#34;, \u0026#34;person\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;Alice\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;30\u0026#34;)) if err != nil { log.Fatal(err) } // Retrieve the hash from the key \u0026#34;person\u0026#34; var values map[string]string err = conn.Do(radix.Cmd(\u0026amp;values, \u0026#34;HGETALL\u0026#34;, \u0026#34;person\u0026#34;)) if err != nil { log.Fatal(err) } // Print the hash fmt.Println(values) } Conclusion In this article, we have learned how to use the redis and radix libraries to connect to a Redis server and store and retrieve various data types in Go. Redis is a powerful and fast in-memory data store that can be used for a wide range of applications, and the redis and radix libraries make it easy to use Redis in Go.\n","permalink":"https://manuelfedele.github.io/posts/use-redis-with-golang/","summary":"\u003ch2 id=\"using-golang-and-redis\"\u003eUsing Golang and Redis\u003c/h2\u003e\n\u003cp\u003eRedis is an in-memory data structure store that can be used as a database, cache, and message broker. It is known for its speed, simplicity, and flexibility. In this article, we will discuss how to use Redis with Golang, a popular programming language known for its simplicity, performance, and concurrency support.\u003c/p\u003e\n\u003cp\u003eBefore we dive into the details of using Redis with Golang, let\u0026rsquo;s first understand the requirements of the application.\u003c/p\u003e","title":"Use Redis With Golang"},{"content":"Golang, also known as Go, is a popular programming language known for its simplicity, performance, and concurrency support. Fyne is an open-source cross-platform UI toolkit written in Go, which makes it an excellent choice for building desktop applications. In this article, we will discuss how to build a desktop application for stock market data using Golang and Fyne.\nBefore we dive into the details of building the application, let\u0026rsquo;s first understand the requirements of the application.\nRequirements The application should be able to fetch real-time stock market data from a reliable source. The user should be able to search for a specific stock by its ticker symbol. The application should display the following data for a selected stock: Current price Price change (in percentage and absolute terms) 52-week high and low Market capitalization Volume P/E ratio The application should also display a chart showing the price history of the selected stock. The application should be able to run on Windows, Mac, and Linux. Fetching Stock Market Data There are several APIs available that provide real-time stock market data. Some popular ones include Yahoo Finance, Alpha Vantage, and IEX Cloud. In this tutorial, we will use the IEX Cloud API, which provides a wide range of financial data, including stock market data.\nTo use the IEX Cloud API, you will need to sign up for an account and obtain an API key. The API key is used to authenticate your requests to the API.\nOnce you have an API key, you can use the following Go libraries to fetch the stock market data:\niexcloud-go: A Go client library for the IEX Cloud API. go-iex: A Go wrapper for the IEX Cloud API. Both libraries provide functions to fetch various types of data, including real-time stock quotes, historical prices, and company information. For example, the following code snippet fetches the real-time quote for a stock with the ticker symbol \u0026ldquo;AAPL\u0026rdquo; using the iexcloud-go library:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/santegoeds/iexcloud-go\u0026#34; ) func main() { // Set the API key iexcloud.SetAPIKey(\u0026#34;YOUR_API_KEY\u0026#34;) // Fetch the real-time quote for the stock \u0026#34;AAPL\u0026#34; quote, err := iexcloud.Quote(\u0026#34;AAPL\u0026#34;) if err != nil { log.Fatal(err) } // Print the quote fmt.Printf(\u0026#34;%+v\\n\u0026#34;, quote) } This will print the real-time quote for the stock \u0026ldquo;AAPL\u0026rdquo;, including the current price, price change, and volume.\nBuilding the User Interface Now that we have the ability to fetch the stock market data, let\u0026rsquo;s build the user interface for the application using Fyne. Fyne provides a wide range of UI elements, such as buttons, labels, text boxes, and tables, which can be used to build the layout and behavior of the application.\nHere is a rough outline of the steps involved in building the user interface:\nCreate a Go function to initialize the UI elements and layout of the application. This function will create the main window of the application and add the necessary UI elements, such as a search bar, stock data display, and chart.\nUse Fyne\u0026rsquo;s NewContainer function to create a container for the UI elements and specify the layout using Fyne\u0026rsquo;s layout management functions, such as NewHBox and NewVBox.\nFyne\u0026rsquo;s NewTextField and NewButton functions to create a search bar and bind the search functionality to the button\u0026rsquo;s OnTapped event.\nFyne\u0026rsquo;s NewLabel and NewTable functions to create a table for displaying the stock data. You can bind the data fetched from the IEX Cloud API to the table using Fyne\u0026rsquo;s SetCell function.\nFyne\u0026rsquo;s NewCanvas function to create a canvas for rendering the chart. You can use a JavaScript charting library such as Highcharts or D3.js to render the chart on the canvas.\nFyne\u0026rsquo;s Run function to launch the application and display the main window.\nPackaging the Application Once you have completed the development of the application, you can use the fyne-packager tool to package the application for distribution on Windows, Mac, and Linux. The fyne-packager tool will create a self-contained executable for each platform, which can be easily installed and run on any system.\nConclusion In this tutorial, we have discussed how to build a desktop application for stock market data using Golang and Fyne. We have seen how to fetch real-time stock market data from the IEX Cloud API and how to build the user interface using Fyne\u0026rsquo;s UI elements and layout management functions. With these tools and techniques, you can create a powerful and efficient application for tracking and analyzing stock market data.\n","permalink":"https://manuelfedele.github.io/posts/create-desktop-application-stock-market-data-golang/","summary":"\u003cp\u003eGolang, also known as Go, is a popular programming language known for its simplicity, performance, and concurrency support. Fyne is an open-source cross-platform UI toolkit written in Go, which makes it an excellent choice for building desktop applications. In this article, we will discuss how to build a desktop application for stock market data using Golang and Fyne.\u003c/p\u003e\n\u003cp\u003eBefore we dive into the details of building the application, let\u0026rsquo;s first understand the requirements of the application.\u003c/p\u003e","title":"Create Desktop Application to fetch Stock Market Data  with Golang"},{"content":"In PostgreSQL, the json data type can be used to store JSON data. You can use the -\u0026gt; operator to access elements of a JSON object, and the -\u0026raquo; operator to access values of a JSON object as text.\nFor example, consider the following table with a JSON field called data:\nCREATE TABLE documents ( id serial PRIMARY KEY, data json ); To insert a JSON object into the data field, you can use the json_build_object function:\nINSERT INTO documents (data) VALUES (json_build_object(\u0026#39;key1\u0026#39;, \u0026#39;value1\u0026#39;, \u0026#39;key2\u0026#39;, \u0026#39;value2\u0026#39;)); To query the values of a JSON object, you can use the -\u0026gt; operator:\nSELECT data-\u0026gt;\u0026#39;key1\u0026#39; FROM documents; This will return the value of the key1 field in the JSON object.\nYou can also use the -\u0026raquo; operator to access values as text:\nSELECT data-\u0026gt;\u0026gt;\u0026#39;key1\u0026#39; FROM documents; This will return the value of the key1 field as text.\nYou can also use the jsonb_each function to iterate over the key/value pairs of a JSON object:\nSELECT key, value FROM jsonb_each(data) WHERE key=\u0026#39;key1\u0026#39;; This will return a row for each key/value pair where the key is \u0026lsquo;key1\u0026rsquo;.\nYou can find more information about working with JSON data in PostgreSQL in the PostgreSQL documentation.\n","permalink":"https://manuelfedele.github.io/posts/work-with-json-postgresql/","summary":"\u003cp\u003eIn PostgreSQL, the json data type can be used to store JSON data. You can use the -\u0026gt; operator to access elements of a JSON object, and the -\u0026raquo; operator to access values of a JSON object as text.\u003c/p\u003e\n\u003cp\u003eFor example, consider the following table with a JSON field called data:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-SQL\" data-lang=\"SQL\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eCREATE\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003edocuments\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nb\"\u003eserial\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003ePRIMARY\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003eKEY\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003edata\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ejson\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTo insert a JSON object into the data field, you can use the json_build_object function:\u003c/p\u003e","title":"Work with Json field on Postgresql"},{"content":"To create a web server in Go, you can use the http package provided by the standard library. This package includes the http.Server type, which represents an HTTP server, and the http.ListenAndServe() function, which listens for incoming HTTP requests on a specified port and serves responses to those requests.\nHere is an example of how you might create a simple web server in Go:\n// Import the http package import \u0026#34;net/http\u0026#34; // Define a function that will be called to handle incoming HTTP requests func handler(w http.ResponseWriter, r *http.Request) { // Write a response message to the client fmt.Fprintf(w, \u0026#34;Hello, World!\u0026#34;) } func main() { // Set up a route that will call the handler function for any requests to the root URL http.HandleFunc(\u0026#34;/\u0026#34;, handler) // Start the web server and listen for incoming requests on port 8080 http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } In this example, the handler() function is defined to handle incoming HTTP requests. This function receives a http.ResponseWriter and a *http.Request as arguments, which are used to write the response message and access information about the incoming request, respectively. The main() function sets up a route that will call the handler() function for any requests to the root URL (\u0026quot;/\u0026quot;) and then starts the web server using the http.ListenAndServe() function. This web server will listen for incoming requests on port 8080 and serve responses using the handler() function.\nThis is a very basic example of how to create a web server in Go. You can customize the handler() function and add additional routes to handle different URL paths and HTTP methods as needed. You can also configure the http.Server object to customize the behavior of the web server, such as setting timeouts or enabling TLS.\nMore often, if you are building microservices, you probably want to handle JSON input/output.\nTo handle JSON input and output in a web server in Go, you can use the encoding/json package to encode and decode JSON data. This package provides the json.Marshal() and json.Unmarshal() functions, which can be used to convert between JSON data and Go data types.\nHere is an example of how you might handle JSON input and output in a web server in Go:\n// Import the encoding/json and net/http packages import ( \u0026#34;encoding/json\u0026#34; \u0026#34;net/http\u0026#34; ) // Define a struct that will be used to hold the JSON data type Message struct { Text string `json:\u0026#34;text\u0026#34;` } // Define a function that will be called to handle incoming HTTP requests func handler(w http.ResponseWriter, r *http.Request) { // Check the HTTP method of the request switch r.Method { case \u0026#34;POST\u0026#34;: // If the request is a POST request, read the JSON data from the request body var m Message if err := json.NewDecoder(r.Body).Decode(\u0026amp;m); err != nil { // If there is an error decoding the JSON data, respond with an error message http.Error(w, err.Error(), http.StatusBadRequest) return } // Do something with the JSON data (in this case, just print it to the console) fmt.Println(m.Text) // Respond with a JSON message json.NewEncoder(w).Encode(\u0026amp;Message{Text: \u0026#34;Hello from the server!\u0026#34;}) default: // If the request is not a POST request, respond with an error message http.Error(w, \u0026#34;Invalid request method\u0026#34;, http.StatusMethodNotAllowed) } } func main() { // Set up a route that will call the handler function for any requests to the root URL http.HandleFunc(\u0026#34;/\u0026#34;, handler) // Start the web server and listen for incoming requests on port 8080 http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } In this example, the handler() function is defined to handle incoming HTTP requests. This function checks the HTTP method of the request and, if it is a POST request, it uses the json.NewDecoder() function to read the JSON data from the request body and decode it into a Message struct. The Message struct includes a json:\u0026ldquo;text\u0026rdquo; tag, which specifies the field name to use in the JSON data. This tag is used by the json package to map the JSON data to the corresponding fields in the Message struct.\nIf the request is a POST request, the handler() function does something with the JSON data (in this case, just printing it to the console) and then responds with a JSON message using the json.NewEncoder() function. This function encodes the Message struct as JSON and writes it to the http.ResponseWriter object, which sends the response to the client.\nThis is a simple example of how to handle JSON input and output in a web server in Go. You can customize the handling of the JSON data and the response message as needed, depending on the requirements of your application.\n","permalink":"https://manuelfedele.github.io/posts/how-to-create-a-webserver-in-golang/","summary":"\u003cp\u003eTo create a web server in Go, you can use the http package provided by the standard library. This package includes the http.Server type, which represents an HTTP server, and the http.ListenAndServe() function, which listens for incoming HTTP requests on a specified port and serves responses to those requests.\u003c/p\u003e\n\u003cp\u003eHere is an example of how you might create a simple web server in Go:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-Go\" data-lang=\"Go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Import the http package\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;net/http\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Define a function that will be called to handle incoming HTTP requests\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"nf\"\u003ehandler\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ew\u003c/span\u003e \u003cspan class=\"nx\"\u003ehttp\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eResponseWriter\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003er\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"nx\"\u003ehttp\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eRequest\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e// Write a response message to the client\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nx\"\u003efmt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eFprintf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ew\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;Hello, World!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e// Set up a route that will call the handler function for any requests to the root URL\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nx\"\u003ehttp\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eHandleFunc\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;/\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003ehandler\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e// Start the web server and listen for incoming requests on port 8080\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nx\"\u003ehttp\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eListenAndServe\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;:8080\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"kc\"\u003enil\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn this example, the handler() function is defined to handle incoming HTTP requests. This function receives a http.ResponseWriter and a *http.Request as arguments, which are used to write the response message and access information about the incoming request, respectively. The main() function sets up a route that will call the handler() function for any requests to the root URL (\u0026quot;/\u0026quot;) and then starts the web server using the http.ListenAndServe() function. This web server will listen for incoming requests on port 8080 and serve responses using the handler() function.\u003c/p\u003e","title":"How to Create a Webserver in Golang"},{"content":"In Go, an interface is a type that defines a set of methods that a struct must implement in order to implement the interface. An interface defines the behavior or capabilities of a struct without specifying the implementation details. This allows different structs to implement the same interface in different ways, promoting loose coupling and flexibility in your code.\nHere is an example of an interface in Go:\n// Define an interface named Animal type Animal interface { // Define a method named Speak that takes no arguments and returns a string Speak() string } In this example, the Animal interface defines a single method named Speak(), which takes no arguments and returns a string. Any struct that wants to implement the Animal interface must implement this Speak() method.\nHere is an example of how a struct can implement the Animal interface:\n// Define a struct named Dog type Dog struct{} // Implement the Speak method for the Dog struct func (d *Dog) Speak() string { return \u0026#34;Woof!\u0026#34; } In this example, the Dog struct implements the Speak() method required by the Animal interface. This allows the Dog struct to be used wherever the Animal interface is expected.\nInterfaces in Go are similar to abstract classes in other programming languages. They define a common contract that structs must adhere to in order to implement the interface, but they do not provide any implementation details. This allows multiple structs to implement the same interface in different ways, providing flexibility and reuse in your code.\n","permalink":"https://manuelfedele.github.io/posts/how-to-use-interfaces-in-golang/","summary":"\u003cp\u003eIn Go, an interface is a type that defines a set of methods that a struct must implement in order to implement the interface. An interface defines the behavior or capabilities of a struct without specifying the implementation details. This allows different structs to implement the same interface in different ways, promoting loose coupling and flexibility in your code.\u003c/p\u003e\n\u003cp\u003eHere is an example of an interface in Go:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-Go\" data-lang=\"Go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Define an interface named Animal\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003etype\u003c/span\u003e \u003cspan class=\"nx\"\u003eAnimal\u003c/span\u003e \u003cspan class=\"kd\"\u003einterface\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e// Define a method named Speak that takes no arguments and returns a string\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nf\"\u003eSpeak\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"kt\"\u003estring\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn this example, the Animal interface defines a single method named Speak(), which takes no arguments and returns a string. Any struct that wants to implement the Animal interface must implement this Speak() method.\u003c/p\u003e","title":"How to Use Interfaces in Golang"},{"content":"To implement the factory design pattern in Go, you can create a factory function that returns an object that implements a common interface. This function can take parameters to specify the type of object to be created. Here is an example of how you might implement the factory design pattern in Go:\n// Define an interface that the factory will create objects for type Animal interface { Speak() string } // Define a factory function that returns an object that implements the Animal interface func NewAnimal(animalType string) Animal { switch animalType { case \u0026#34;dog\u0026#34;: return \u0026amp;Dog{} case \u0026#34;cat\u0026#34;: return \u0026amp;Cat{} default: return nil } } // Define a struct for a dog that implements the Animal interface type Dog struct{} func (d *Dog) Speak() string { return \u0026#34;Woof!\u0026#34; } // Define a struct for a cat that implements the Animal interface type Cat struct{} func (c *Cat) Speak() string { return \u0026#34;Meow!\u0026#34; } // Use the factory function to create new Animal objects dog := NewAnimal(\u0026#34;dog\u0026#34;) cat := NewAnimal(\u0026#34;cat\u0026#34;) fmt.Println(dog.Speak()) // \u0026#34;Woof!\u0026#34; fmt.Println(cat.Speak()) // \u0026#34;Meow!\u0026#34; In this example, the NewAnimal() function is the factory function that returns objects of different types (Dog or Cat in this case) that implement the Animal interface. The factory function takes a string parameter that specifies the type of object to be created. The Dog and Cat structs both implement the Animal interface by implementing the Speak() method. The NewAnimal() function uses a switch statement to determine which type of object to return based on the animalType parameter.\nThis implementation of the factory design pattern allows you to easily create new objects of different types without having to specify the exact type of each object. It also promotes loose coupling between the different types of objects, as they only need to implement a common interface.\n","permalink":"https://manuelfedele.github.io/posts/implement-factory-design-pattern-golang/","summary":"\u003cp\u003eTo implement the factory design pattern in Go, you can create a factory function that returns an object that implements a common interface. This function can take parameters to specify the type of object to be created. Here is an example of how you might implement the factory design pattern in Go:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Define an interface that the factory will create objects for\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003etype\u003c/span\u003e \u003cspan class=\"nx\"\u003eAnimal\u003c/span\u003e \u003cspan class=\"kd\"\u003einterface\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nf\"\u003eSpeak\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"kt\"\u003estring\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Define a factory function that returns an object that implements the Animal interface\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"nf\"\u003eNewAnimal\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eanimalType\u003c/span\u003e \u003cspan class=\"kt\"\u003estring\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"nx\"\u003eAnimal\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eswitch\u003c/span\u003e \u003cspan class=\"nx\"\u003eanimalType\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ecase\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;dog\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"nx\"\u003eDog\u003c/span\u003e\u003cspan class=\"p\"\u003e{}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ecase\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;cat\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"nx\"\u003eCat\u003c/span\u003e\u003cspan class=\"p\"\u003e{}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edefault\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"kc\"\u003enil\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Define a struct for a dog that implements the Animal interface\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003etype\u003c/span\u003e \u003cspan class=\"nx\"\u003eDog\u003c/span\u003e \u003cspan class=\"kd\"\u003estruct\u003c/span\u003e\u003cspan class=\"p\"\u003e{}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ed\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"nx\"\u003eDog\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"nf\"\u003eSpeak\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"kt\"\u003estring\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;Woof!\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Define a struct for a cat that implements the Animal interface\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003etype\u003c/span\u003e \u003cspan class=\"nx\"\u003eCat\u003c/span\u003e \u003cspan class=\"kd\"\u003estruct\u003c/span\u003e\u003cspan class=\"p\"\u003e{}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ec\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"nx\"\u003eCat\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"nf\"\u003eSpeak\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"kt\"\u003estring\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;Meow!\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Use the factory function to create new Animal objects\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003edog\u003c/span\u003e \u003cspan class=\"o\"\u003e:=\u003c/span\u003e \u003cspan class=\"nf\"\u003eNewAnimal\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;dog\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003ecat\u003c/span\u003e \u003cspan class=\"o\"\u003e:=\u003c/span\u003e \u003cspan class=\"nf\"\u003eNewAnimal\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;cat\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003efmt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ePrintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003edog\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eSpeak\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e \u003cspan class=\"c1\"\u003e// \u0026#34;Woof!\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003efmt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ePrintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ecat\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eSpeak\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e \u003cspan class=\"c1\"\u003e// \u0026#34;Meow!\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn this example, the NewAnimal() function is the factory function that returns objects of different types (Dog or Cat in this case) that implement the Animal interface. The factory function takes a string parameter that specifies the type of object to be created. The Dog and Cat structs both implement the Animal interface by implementing the Speak() method. The NewAnimal() function uses a switch statement to determine which type of object to return based on the animalType parameter.\u003c/p\u003e","title":"Implement Factory Design Pattern in Golang"},{"content":"Introduction This tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I\u0026rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won\u0026rsquo;t cover using CSS to style your theme.\nWe\u0026rsquo;ll start with creating a new site with a very basic template. Then we\u0026rsquo;ll add in a few pages and posts. With small variations on that, you will be able to create many different types of web sites.\nIn this tutorial, commands that you enter will start with the \u0026ldquo;$\u0026rdquo; prompt. The output will follow. Lines that start with \u0026ldquo;#\u0026rdquo; are comments that I\u0026rsquo;ve added to explain a point. When I show updates to a file, the \u0026ldquo;:wq\u0026rdquo; on the last line means to save the file.\nHere\u0026rsquo;s an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## this is a comment $ echo this is a command this is a command ## edit the file $ vi foo.md +++ date = \u0026#34;2014-09-28\u0026#34; title = \u0026#34;creating a new theme\u0026#34; +++ bah and humbug :wq ## show it $ cat foo.md +++ date = \u0026#34;2014-09-28\u0026#34; title = \u0026#34;creating a new theme\u0026#34; +++ bah and humbug $ Some Definitions There are a few concepts that you need to understand before creating a theme.\nSkins Skins are the files responsible for the look and feel of your site. It’s the CSS that controls colors and fonts, it’s the Javascript that determines actions and reactions. It’s also the rules that Hugo uses to transform your content into the HTML that the site will serve to visitors.\nYou have two ways to create a skin. The simplest way is to create it in the layouts/ directory. If you do, then you don’t have to worry about configuring Hugo to recognize it. The first place that Hugo will look for rules and files is in the layouts/ directory so it will always find the skin.\nYour second choice is to create it in a sub-directory of the themes/ directory. If you do, then you must always tell Hugo where to search for the skin. It’s extra work, though, so why bother with it?\nThe difference between creating a skin in layouts/ and creating it in themes/ is very subtle. A skin in layouts/ can’t be customized without updating the templates and static files that it is built from. A skin created in themes/, on the other hand, can be and that makes it easier for other people to use it.\nThe rest of this tutorial will call a skin created in the themes/ directory a theme.\nNote that you can use this tutorial to create a skin in the layouts/ directory if you wish to. The main difference will be that you won’t need to update the site’s configuration file to use a theme.\nThe Home Page The home page, or landing page, is the first page that many visitors to a site see. It is the index.html file in the root directory of the web site. Since Hugo writes files to the public/ directory, our home page is public/index.html.\nSite Configuration File When Hugo runs, it looks for a configuration file that contains settings that override default values for the entire site. The file can use TOML, YAML, or JSON. I prefer to use TOML for my configuration files. If you prefer to use JSON or YAML, you’ll need to translate my examples. You’ll also need to change the name of the file since Hugo uses the extension to determine how to process it.\nHugo translates Markdown files into HTML. By default, Hugo expects to find Markdown files in your content/ directory and template files in your themes/ directory. It will create HTML files in your public/ directory. You can change this by specifying alternate locations in the configuration file.\nContent Content is stored in text files that contain two sections. The first section is the “front matter,” which is the meta-information on the content. The second section contains Markdown that will be converted to HTML.\nFront Matter The front matter is information about the content. Like the configuration file, it can be written in TOML, YAML, or JSON. Unlike the configuration file, Hugo doesn’t use the file’s extension to know the format. It looks for markers to signal the type. TOML is surrounded by “+++”, YAML by “---”, and JSON is enclosed in curly braces. I prefer to use TOML, so you’ll need to translate my examples if you prefer YAML or JSON.\nThe information in the front matter is passed into the template before the content is rendered into HTML.\nMarkdown Content is written in Markdown which makes it easier to create the content. Hugo runs the content through a Markdown engine to create the HTML which will be written to the output file.\nTemplate Files Hugo uses template files to render content into HTML. Template files are a bridge between the content and presentation. Rules in the template define what content is published, where it\u0026rsquo;s published to, and how it will rendered to the HTML file. The template guides the presentation by specifying the style to use.\nThere are three types of templates: single, list, and partial. Each type takes a bit of content as input and transforms it based on the commands in the template.\nHugo uses its knowledge of the content to find the template file used to render the content. If it can’t find a template that is an exact match for the content, it will shift up a level and search from there. It will continue to do so until it finds a matching template or runs out of templates to try. If it can’t find a template, it will use the default template for the site.\nPlease note that you can use the front matter to influence Hugo’s choice of templates.\nSingle Template A single template is used to render a single piece of content. For example, an article or post would be a single piece of content and use a single template.\nList Template A list template renders a group of related content. That could be a summary of recent postings or all articles in a category. List templates can contain multiple groups.\nThe homepage template is a special type of list template. Hugo assumes that the home page of your site will act as the portal for the rest of the content in the site.\nPartial Template A partial template is a template that can be included in other templates. Partial templates must be called using the “partial” template command. They are very handy for rolling up common behavior. For example, your site may have a banner that all pages use. Instead of copying the text of the banner into every single and list template, you could create a partial with the banner in it. That way if you decide to change the banner, you only have to change the partial template.\nCreate a New Site Let\u0026rsquo;s use Hugo to create a new web site. I\u0026rsquo;m a Mac user, so I\u0026rsquo;ll create mine in my home directory, in the Sites folder. If you\u0026rsquo;re using Linux, you might have to create the folder first.\nThe \u0026ldquo;new site\u0026rdquo; command will create a skeleton of a site. It will give you the basic directory structure and a useable configuration file.\n1 2 3 4 5 6 7 8 9 10 11 12 $ hugo new site ~/Sites/zafta $ cd ~/Sites/zafta $ ls -l total 8 drwxr-xr-x 7 quoha staff 238 Sep 29 16:49 . drwxr-xr-x 3 quoha staff 102 Sep 29 16:49 .. drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static $ Take a look in the content/ directory to confirm that it is empty.\nThe other directories (archetypes/, layouts/, and static/) are used when customizing a theme. That\u0026rsquo;s a topic for a different tutorial, so please ignore them for now.\nGenerate the HTML For the New Site Running the hugo command with no options will read all the available content and generate the HTML files. It will also copy all static files (that\u0026rsquo;s everything that\u0026rsquo;s not content). Since we have an empty site, it won\u0026rsquo;t do much, but it will do it very quickly.\n1 2 3 4 5 6 7 8 9 10 11 12 $ hugo --verbose INFO: 2014/09/29 Using config file: config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ The \u0026ldquo;--verbose\u0026rdquo; flag gives extra information that will be helpful when we build the template. Every line of the output that starts with \u0026ldquo;INFO:\u0026rdquo; or \u0026ldquo;WARN:\u0026rdquo; is present because we used that flag. The lines that start with \u0026ldquo;WARN:\u0026rdquo; are warning messages. We\u0026rsquo;ll go over them later.\nWe can verify that the command worked by looking at the directory again.\n1 2 3 4 5 6 7 8 9 $ ls -l total 8 drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static $ See that new public/ directory? Hugo placed all generated content there. When you\u0026rsquo;re ready to publish your web site, that\u0026rsquo;s the place to start. For now, though, let\u0026rsquo;s just confirm that we have what we\u0026rsquo;d expect from a site with no content.\n1 2 3 4 5 $ ls -l public total 16 -rw-r--r-- 1 quoha staff 416 Sep 29 17:02 index.xml -rw-r--r-- 1 quoha staff 262 Sep 29 17:02 sitemap.xml $ Hugo created two XML files, which is standard, but there are no HTML files.\nTest the New Site Verify that you can run the built-in web server. It will dramatically shorten your development cycle if you do. Start it by running the \u0026ldquo;server\u0026rdquo; command. If it is successful, you will see output similar to the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ hugo server --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms Serving pages from /Users/quoha/Sites/zafta/public Web Server is available at http://localhost:1313 Press Ctrl+C to stop Connect to the listed URL (it\u0026rsquo;s on the line that starts with \u0026ldquo;Web Server\u0026rdquo;). If everything is working correctly, you should get a page that shows the following:\n1 2 index.xml sitemap.xml That\u0026rsquo;s a listing of your public/ directory. Hugo didn\u0026rsquo;t create a home page because our site has no content. When there\u0026rsquo;s no index.html file in a directory, the server lists the files in the directory, which is what you should see in your browser.\nLet’s go back and look at those warnings again.\n1 2 WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] That second warning is easier to explain. We haven’t created a template to be used to generate “page not found errors.” The 404 message is a topic for a separate tutorial.\nNow for the first warning. It is for the home page. You can tell because the first layout that it looked for was “index.html.” That’s only used by the home page.\nI like that the verbose flag causes Hugo to list the files that it\u0026rsquo;s searching for. For the home page, they are index.html, _default/list.html, and _default/single.html. There are some rules that we\u0026rsquo;ll cover later that explain the names and paths. For now, just remember that Hugo couldn\u0026rsquo;t find a template for the home page and it told you so.\nAt this point, you\u0026rsquo;ve got a working installation and site that we can build upon. All that’s left is to add some content and a theme to display it.\nCreate a New Theme Hugo doesn\u0026rsquo;t ship with a default theme. There are a few available (I counted a dozen when I first installed Hugo) and Hugo comes with a command to create new themes.\nWe\u0026rsquo;re going to create a new theme called \u0026ldquo;zafta.\u0026rdquo; Since the goal of this tutorial is to show you how to fill out the files to pull in your content, the theme will not contain any CSS. In other words, ugly but functional.\nAll themes have opinions on content and layout. For example, Zafta uses \u0026ldquo;post\u0026rdquo; over \u0026ldquo;blog\u0026rdquo;. Strong opinions make for simpler templates but differing opinions make it tougher to use themes. When you build a theme, consider using the terms that other themes do.\nCreate a Skeleton Use the hugo \u0026ldquo;new\u0026rdquo; command to create the skeleton of a theme. This creates the directory structure and places empty files for you to fill out.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ hugo new theme zafta $ ls -l total 8 drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static drwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes $ find themes -type f | xargs ls -l -rw-r--r-- 1 quoha staff 1081 Sep 29 17:31 themes/zafta/LICENSE.md -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/archetypes/default.md -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html -rw-r--r-- 1 quoha staff 93 Sep 29 17:31 themes/zafta/theme.toml $ The skeleton includes templates (the files ending in .html), license file, a description of your theme (the theme.toml file), and an empty archetype.\nPlease take a minute to fill out the theme.toml and LICENSE.md files. They\u0026rsquo;re optional, but if you\u0026rsquo;re going to be distributing your theme, it tells the world who to praise (or blame). It\u0026rsquo;s also nice to declare the license so that people will know how they can use the theme.\n1 2 3 4 5 6 7 8 9 10 11 $ vi themes/zafta/theme.toml author = \u0026#34;michael d henderson\u0026#34; description = \u0026#34;a minimal working template\u0026#34; license = \u0026#34;MIT\u0026#34; name = \u0026#34;zafta\u0026#34; source_repo = \u0026#34;\u0026#34; tags = [\u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34;] :wq ## also edit themes/zafta/LICENSE.md and change ## the bit that says \u0026#34;YOUR_NAME_HERE\u0026#34; Note that the the skeleton\u0026rsquo;s template files are empty. Don\u0026rsquo;t worry, we\u0026rsquo;ll be changing that shortly.\n1 2 3 4 5 6 7 $ find themes/zafta -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html $ Update the Configuration File to Use the Theme Now that we\u0026rsquo;ve got a theme to work with, it\u0026rsquo;s a good idea to add the theme name to the configuration file. This is optional, because you can always add \u0026ldquo;-t zafta\u0026rdquo; on all your commands. I like to put it the configuration file because I like shorter command lines. If you don\u0026rsquo;t put it in the configuration file or specify it on the command line, you won\u0026rsquo;t use the template that you\u0026rsquo;re expecting to.\nEdit the file to add the theme, add a title for the site, and specify that all of our content will use the TOML format.\n1 2 3 4 5 6 7 8 9 $ vi config.toml theme = \u0026#34;zafta\u0026#34; baseurl = \u0026#34;\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;zafta - totally refreshing\u0026#34; MetaDataFormat = \u0026#34;toml\u0026#34; :wq $ Generate the Site Now that we have an empty theme, let\u0026rsquo;s generate the site again.\n1 2 3 4 5 6 7 8 9 10 11 12 $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ Did you notice that the output is different? The warning message for the home page has disappeared and we have an additional information line saying that Hugo is syncing from the theme\u0026rsquo;s directory.\nLet\u0026rsquo;s check the public/ directory to see what Hugo\u0026rsquo;s created.\n1 2 3 4 5 6 7 8 $ ls -l public total 16 drwxr-xr-x 2 quoha staff 68 Sep 29 17:56 css -rw-r--r-- 1 quoha staff 0 Sep 29 17:56 index.html -rw-r--r-- 1 quoha staff 407 Sep 29 17:56 index.xml drwxr-xr-x 2 quoha staff 68 Sep 29 17:56 js -rw-r--r-- 1 quoha staff 243 Sep 29 17:56 sitemap.xml $ Notice four things:\nHugo created a home page. This is the file public/index.html. Hugo created a css/ directory. Hugo created a js/ directory. Hugo claimed that it created 0 pages. It created a file and copied over static files, but didn\u0026rsquo;t create any pages. That\u0026rsquo;s because it considers a \u0026ldquo;page\u0026rdquo; to be a file created directly from a content file. It doesn\u0026rsquo;t count things like the index.html files that it creates automatically. The Home Page Hugo supports many different types of templates. The home page is special because it gets its own type of template and its own template file. The file, layouts/index.html, is used to generate the HTML for the home page. The Hugo documentation says that this is the only required template, but that depends. Hugo\u0026rsquo;s warning message shows that it looks for three different templates:\n1 WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] If it can\u0026rsquo;t find any of these, it completely skips creating the home page. We noticed that when we built the site without having a theme installed.\nWhen Hugo created our theme, it created an empty home page template. Now, when we build the site, Hugo finds the template and uses it to generate the HTML for the home page. Since the template file is empty, the HTML file is empty, too. If the template had any rules in it, then Hugo would have used them to generate the home page.\n1 2 3 4 $ find . -name index.html | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 20:21 ./public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 ./themes/zafta/layouts/index.html $ The Magic of Static Hugo does two things when generating the site. It uses templates to transform content into HTML and it copies static files into the site. Unlike content, static files are not transformed. They are copied exactly as they are.\nHugo assumes that your site will use both CSS and JavaScript, so it creates directories in your theme to hold them. Remember opinions? Well, Hugo\u0026rsquo;s opinion is that you\u0026rsquo;ll store your CSS in a directory named css/ and your JavaScript in a directory named js/. If you don\u0026rsquo;t like that, you can change the directory names in your theme directory or even delete them completely. Hugo\u0026rsquo;s nice enough to offer its opinion, then behave nicely if you disagree.\n1 2 3 4 5 6 7 8 9 10 $ find themes/zafta -type d | xargs ls -ld drwxr-xr-x 7 quoha staff 238 Sep 29 17:38 themes/zafta drwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes/zafta/archetypes drwxr-xr-x 5 quoha staff 170 Sep 29 17:31 themes/zafta/layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/_default drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/partials drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/static drwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/css drwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/js $ The Theme Development Cycle When you\u0026rsquo;re working on a theme, you will make changes in the theme\u0026rsquo;s directory, rebuild the site, and check your changes in the browser. Hugo makes this very easy:\nPurge the public/ directory. Run the built in web server in watch mode. Open your site in a browser. Update the theme. Glance at your browser window to see changes. Return to step 4. I’ll throw in one more opinion: never work on a theme on a live site. Always work on a copy of your site. Make changes to your theme, test them, then copy them up to your site. For added safety, use a tool like Git to keep a revision history of your content and your theme. Believe me when I say that it is too easy to lose both your mind and your changes.\nCheck the main Hugo site for information on using Git with Hugo.\nPurge the public/ Directory When generating the site, Hugo will create new files and update existing ones in the public/ directory. It will not delete files that are no longer used. For example, files that were created in the wrong directory or with the wrong title will remain. If you leave them, you might get confused by them later. I recommend cleaning out your site prior to generating it.\nNote: If you\u0026rsquo;re building on an SSD, you should ignore this. Churning on a SSD can be costly.\nHugo\u0026rsquo;s Watch Option Hugo\u0026rsquo;s \u0026ldquo;--watch\u0026rdquo; option will monitor the content/ and your theme directories for changes and rebuild the site automatically.\nLive Reload Hugo\u0026rsquo;s built in web server supports live reload. As pages are saved on the server, the browser is told to refresh the page. Usually, this happens faster than you can say, \u0026ldquo;Wow, that\u0026rsquo;s totally amazing.\u0026rdquo;\nDevelopment Commands Use the following commands as the basis for your workflow.\n1 2 3 4 5 6 7 ## purge old files. hugo will recreate the public directory. ## $ rm -rf public ## ## run hugo in watch mode ## $ hugo server --watch --verbose Here\u0026rsquo;s sample output showing Hugo detecting a change to the template for the home page. Once generated, the web browser automatically reloaded the page. I\u0026rsquo;ve said this before, it\u0026rsquo;s amazing.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ rm -rf public $ hugo server --watch --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms Watching for changes in /Users/quoha/Sites/zafta/content Serving pages from /Users/quoha/Sites/zafta/public Web Server is available at http://localhost:1313 Press Ctrl+C to stop INFO: 2014/09/29 File System Event: [\u0026#34;/Users/quoha/Sites/zafta/themes/zafta/layouts/index.html\u0026#34;: MODIFY|ATTRIB] Change detected, rebuilding site WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 1 ms Update the Home Page Template The home page is one of a few special pages that Hugo creates automatically. As mentioned earlier, it looks for one of three files in the theme\u0026rsquo;s layout/ directory:\nindex.html _default/list.html _default/single.html We could update one of the default templates, but a good design decision is to update the most specific template available. That\u0026rsquo;s not a hard and fast rule (in fact, we\u0026rsquo;ll break it a few times in this tutorial), but it is a good generalization.\nMake a Static Home Page Right now, that page is empty because we don\u0026rsquo;t have any content and we don\u0026rsquo;t have any logic in the template. Let\u0026rsquo;s change that by adding some text to the template.\n1 2 3 4 5 6 7 8 9 10 $ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq $ Build the web site and then verify the results.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 78 Sep 29 21:26 public/index.html $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;/html\u0026gt; Live Reload Note: If you\u0026rsquo;re running the server with the --watch option, you\u0026rsquo;ll see different content in the file:\n1 2 3 4 5 6 7 8 9 10 $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;script\u0026gt;document.write(\u0026#39;\u0026lt;script src=\u0026#34;http://\u0026#39; + (location.host || \u0026#39;localhost\u0026#39;).split(\u0026#39;:\u0026#39;)[0] + \u0026#39;:1313/livereload.js?mindelay=10\u0026#34;\u0026gt;\u0026lt;/\u0026#39; + \u0026#39;script\u0026gt;\u0026#39;)\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; When you use --watch, the Live Reload script is added by Hugo. Look for live reload in the documentation to see what it does and how to disable it.\nBuild a \u0026ldquo;Dynamic\u0026rdquo; Home Page \u0026ldquo;Dynamic home page?\u0026rdquo; Hugo\u0026rsquo;s a static web site generator, so this seems an odd thing to say. I mean let\u0026rsquo;s have the home page automatically reflect the content in the site every time Hugo builds it. We\u0026rsquo;ll use iteration in the template to do that.\nCreate New Posts Now that we have the home page generating static content, let\u0026rsquo;s add some content to the site. We\u0026rsquo;ll display these posts as a list on the home page and on their own page, too.\nHugo has a command to generate a skeleton post, just like it does for sites and themes.\n1 2 3 4 5 6 7 $ hugo --verbose new post/first.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/first.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/default.md ERROR: 2014/09/29 Unable to Cast \u0026lt;nil\u0026gt; to map[string]interface{} $ That wasn\u0026rsquo;t very nice, was it?\nThe \u0026ldquo;new\u0026rdquo; command uses an archetype to create the post file. Hugo created an empty default archetype file, but that causes an error when there\u0026rsquo;s a theme. For me, the workaround was to create an archetypes file specifically for the post type.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 $ vi themes/zafta/archetypes/post.md +++ Description = \u0026#34;\u0026#34; Tags = [] Categories = [] +++ :wq $ find themes/zafta/archetypes -type f | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 21:53 themes/zafta/archetypes/default.md -rw-r--r-- 1 quoha staff 51 Sep 29 21:54 themes/zafta/archetypes/post.md $ hugo --verbose new post/first.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/first.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md INFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/first.md /Users/quoha/Sites/zafta/content/post/first.md created $ hugo --verbose new post/second.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/second.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md INFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/second.md /Users/quoha/Sites/zafta/content/post/second.md created $ ls -l content/post total 16 -rw-r--r-- 1 quoha staff 104 Sep 29 21:54 first.md -rw-r--r-- 1 quoha staff 105 Sep 29 21:57 second.md $ cat content/post/first.md +++ Categories = [] Description = \u0026#34;\u0026#34; Tags = [] date = \u0026#34;2014-09-29T21:54:53-05:00\u0026#34; title = \u0026#34;first\u0026#34; +++ my first post $ cat content/post/second.md +++ Categories = [] Description = \u0026#34;\u0026#34; Tags = [] date = \u0026#34;2014-09-29T21:57:09-05:00\u0026#34; title = \u0026#34;second\u0026#34; +++ my second post $ Build the web site and then verify the results.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;, \u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ The output says that it created 2 pages. Those are our new posts:\n1 2 3 4 5 6 $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 78 Sep 29 22:13 public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/second/index.html $ The new files are empty because because the templates used to generate the content are empty. The homepage doesn\u0026rsquo;t show the new content, either. We have to update the templates to add the posts.\nList and Single Templates In Hugo, we have three major kinds of templates. There\u0026rsquo;s the home page template that we updated previously. It is used only by the home page. We also have \u0026ldquo;single\u0026rdquo; templates which are used to generate output for a single content file. We also have \u0026ldquo;list\u0026rdquo; templates that are used to group multiple pieces of content before generating output.\nGenerally speaking, list templates are named \u0026ldquo;list.html\u0026rdquo; and single templates are named \u0026ldquo;single.html.\u0026rdquo;\nThere are three other types of templates: partials, content views, and terms. We will not go into much detail on these.\nAdd Content to the Homepage The home page will contain a list of posts. Let\u0026rsquo;s update its template to add the posts that we just created. The logic in the template will run every time we build the site.\n1 2 3 4 5 6 7 8 9 10 11 12 $ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; {{ range first 10 .Data.Pages }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq $ Hugo uses the Go template engine. That engine scans the template files for commands which are enclosed between \u0026ldquo;{{\u0026rdquo; and \u0026ldquo;}}\u0026rdquo;. In our template, the commands are:\nrange .Title end The \u0026ldquo;range\u0026rdquo; command is an iterator. We\u0026rsquo;re going to use it to go through the first ten pages. Every HTML file that Hugo creates is treated as a page, so looping through the list of pages will look at every file that will be created.\nThe \u0026ldquo;.Title\u0026rdquo; command prints the value of the \u0026ldquo;title\u0026rdquo; variable. Hugo pulls it from the front matter in the Markdown file.\nThe \u0026ldquo;end\u0026rdquo; command signals the end of the range iterator. The engine loops back to the top of the iteration when it finds \u0026ldquo;end.\u0026rdquo; Everything between the \u0026ldquo;range\u0026rdquo; and \u0026ldquo;end\u0026rdquo; is evaluated every time the engine goes through the iteration. In this file, that would cause the title from the first ten pages to be output as heading level one.\nIt\u0026rsquo;s helpful to remember that some variables, like .Data, are created before any output files. Hugo loads every content file into the variable and then gives the template a chance to process before creating the HTML files.\nBuild the web site and then verify the results.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 $ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:23 public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/second/index.html $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;second\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;first\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ Congratulations, the home page shows the title of the two posts. The posts themselves are still empty, but let\u0026rsquo;s take a moment to appreciate what we\u0026rsquo;ve done. Your template now generates output dynamically. Believe it or not, by inserting the range command inside of those curly braces, you\u0026rsquo;ve learned everything you need to know to build a theme. All that\u0026rsquo;s really left is understanding which template will be used to generate each content file and becoming familiar with the commands for the template engine.\nAnd, if that were entirely true, this tutorial would be much shorter. There are a few things to know that will make creating a new template much easier. Don\u0026rsquo;t worry, though, that\u0026rsquo;s all to come.\nAdd Content to the Posts We\u0026rsquo;re working with posts, which are in the content/post/ directory. That means that their section is \u0026ldquo;post\u0026rdquo; (and if we don\u0026rsquo;t do something weird, their type is also \u0026ldquo;post\u0026rdquo;).\nHugo uses the section and type to find the template file for every piece of content. Hugo will first look for a template file that matches the section or type name. If it can\u0026rsquo;t find one, then it will look in the _default/ directory. There are some twists that we\u0026rsquo;ll cover when we get to categories and tags, but for now we can assume that Hugo will try post/single.html, then _default/single.html.\nNow that we know the search rule, let\u0026rsquo;s see what we actually have available:\n1 2 $ find themes/zafta -name single.html | xargs ls -l -rw-r--r-- 1 quoha staff 132 Sep 29 17:31 themes/zafta/layouts/_default/single.html We could create a new template, post/single.html, or change the default. Since we don\u0026rsquo;t know of any other content types, let\u0026rsquo;s start with updating the default.\nRemember, any content that we haven\u0026rsquo;t created a template for will end up using this template. That can be good or bad. Bad because I know that we\u0026rsquo;re going to be adding different types of content and we\u0026rsquo;re going to end up undoing some of the changes we\u0026rsquo;ve made. It\u0026rsquo;s good because we\u0026rsquo;ll be able to see immediate results. It\u0026rsquo;s also good to start here because we can start to build the basic layout for the site. As we add more content types, we\u0026rsquo;ll refactor this file and move logic around. Hugo makes that fairly painless, so we\u0026rsquo;ll accept the cost and proceed.\nPlease see the Hugo documentation on template rendering for all the details on determining which template to use. And, as the docs mention, if you\u0026rsquo;re building a single page application (SPA) web site, you can delete all of the other templates and work with just the default single page. That\u0026rsquo;s a refreshing amount of joy right there.\nUpdate the Template File 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ vi themes/zafta/layouts/_default/single.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq $ Build the web site and verify the results.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 $ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:40 public/index.html -rw-r--r-- 1 quoha staff 125 Sep 29 22:40 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:40 public/post/index.html -rw-r--r-- 1 quoha staff 128 Sep 29 22:40 public/post/second/index.html $ cat public/post/first/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;first\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;first\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;my first post\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ cat public/post/second/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;second\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;second\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;my second post\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ Notice that the posts now have content. You can go to localhost:1313/post/first to verify.\nLinking to Content The posts are on the home page. Let\u0026rsquo;s add a link from there to the post. Since this is the home page, we\u0026rsquo;ll update its template.\n1 2 3 4 5 6 7 8 9 $ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; {{ range first 10 .Data.Pages }} \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Build the web site and verify the results.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 149 Sep 29 22:44 public/index.html -rw-r--r-- 1 quoha staff 125 Sep 29 22:44 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:44 public/post/index.html -rw-r--r-- 1 quoha staff 128 Sep 29 22:44 public/post/second/index.html $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;/post/second/\u0026#34;\u0026gt;second\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;/post/first/\u0026#34;\u0026gt;first\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ Create a Post Listing We have the posts displaying on the home page and on their own page. We also have a file public/post/index.html that is empty. Let\u0026rsquo;s make it show a list of all posts (not just the first ten).\nWe need to decide which template to update. This will be a listing, so it should be a list template. Let\u0026rsquo;s take a quick look and see which list templates are available.\n1 2 $ find themes/zafta -name list.html | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html As with the single post, we have to decide to update _default/list.html or create post/list.html. We still don\u0026rsquo;t have multiple content types, so let\u0026rsquo;s stay consistent and update the default list template.\nCreating Top Level Pages Let\u0026rsquo;s add an \u0026ldquo;about\u0026rdquo; page and display it at the top level (as opposed to a sub-level like we did with posts).\nThe default in Hugo is to use the directory structure of the content/ directory to guide the location of the generated html in the public/ directory. Let\u0026rsquo;s verify that by creating an \u0026ldquo;about\u0026rdquo; page at the top level:\n1 2 3 4 5 6 7 8 9 10 11 12 $ vi content/about.md +++ title = \u0026#34;about\u0026#34; description = \u0026#34;about this site\u0026#34; date = \u0026#34;2014-09-27\u0026#34; slug = \u0026#34;about time\u0026#34; +++ ## about us i\u0026#39;m speechless :wq Generate the web site and verify the results.\n1 2 3 4 5 6 $ find public -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-rw-r-- 1 mdhender staff 334 Sep 27 15:08 public/about-time/index.html -rw-rw-r-- 1 mdhender staff 527 Sep 27 15:08 public/index.html -rw-rw-r-- 1 mdhender staff 358 Sep 27 15:08 public/post/first-post/index.html -rw-rw-r-- 1 mdhender staff 0 Sep 27 15:08 public/post/index.html -rw-rw-r-- 1 mdhender staff 342 Sep 27 15:08 public/post/second-post/index.html Notice that the page wasn\u0026rsquo;t created at the top level. It was created in a sub-directory named \u0026lsquo;about-time/\u0026rsquo;. That name came from our slug. Hugo will use the slug to name the generated content. It\u0026rsquo;s a reasonable default, by the way, but we can learn a few things by fighting it for this file.\nOne other thing. Take a look at the home page.\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/post/theme/\u0026#34;\u0026gt;creating a new theme\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/about-time/\u0026#34;\u0026gt;about\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/post/second-post/\u0026#34;\u0026gt;second\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/post/first-post/\u0026#34;\u0026gt;first\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;script\u0026gt;document.write(\u0026#39;\u0026lt;script src=\u0026#34;http://\u0026#39; + (location.host || \u0026#39;localhost\u0026#39;).split(\u0026#39;:\u0026#39;)[0] + \u0026#39;:1313/livereload.js?mindelay=10\u0026#34;\u0026gt;\u0026lt;/\u0026#39; + \u0026#39;script\u0026gt;\u0026#39;)\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Notice that the \u0026ldquo;about\u0026rdquo; link is listed with the posts? That\u0026rsquo;s not desirable, so let\u0026rsquo;s change that first.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;posts\u0026lt;/h1\u0026gt; {{ range first 10 .Data.Pages }} {{ if eq .Type \u0026#34;post\u0026#34;}} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;h1\u0026gt;pages\u0026lt;/h1\u0026gt; {{ range .Data.Pages }} {{ if eq .Type \u0026#34;page\u0026#34; }} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq Generate the web site and verify the results. The home page has two sections, posts and pages, and each section has the right set of headings and links in it.\nBut, that about page still renders to about-time/index.html.\n1 2 3 4 5 6 $ find public -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-rw-r-- 1 mdhender staff 334 Sep 27 15:33 public/about-time/index.html -rw-rw-r-- 1 mdhender staff 645 Sep 27 15:33 public/index.html -rw-rw-r-- 1 mdhender staff 358 Sep 27 15:33 public/post/first-post/index.html -rw-rw-r-- 1 mdhender staff 0 Sep 27 15:33 public/post/index.html -rw-rw-r-- 1 mdhender staff 342 Sep 27 15:33 public/post/second-post/index.html Knowing that hugo is using the slug to generate the file name, the simplest solution is to change the slug. Let\u0026rsquo;s do it the hard way and change the permalink in the configuration file.\n1 2 3 4 $ vi config.toml [permalinks] page = \u0026#34;/:title/\u0026#34; about = \u0026#34;/:filename/\u0026#34; Generate the web site and verify that this didn\u0026rsquo;t work. Hugo lets \u0026ldquo;slug\u0026rdquo; or \u0026ldquo;URL\u0026rdquo; override the permalinks setting in the configuration file. Go ahead and comment out the slug in content/about.md, then generate the web site to get it to be created in the right place.\nSharing Templates If you\u0026rsquo;ve been following along, you probably noticed that posts have titles in the browser and the home page doesn\u0026rsquo;t. That\u0026rsquo;s because we didn\u0026rsquo;t put the title in the home page\u0026rsquo;s template (layouts/index.html). That\u0026rsquo;s an easy thing to do, but let\u0026rsquo;s look at a different option.\nWe can put the common bits into a shared template that\u0026rsquo;s stored in the themes/zafta/layouts/partials/ directory.\nCreate the Header and Footer Partials In Hugo, a partial is a sugar-coated template. Normally a template reference has a path specified. Partials are different. Hugo searches for them along a TODO defined search path. This makes it easier for end-users to override the theme\u0026rsquo;s presentation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ vi themes/zafta/layouts/partials/header.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; :wq $ vi themes/zafta/layouts/partials/footer.html \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq Update the Home Page Template to Use the Partials The most noticeable difference between a template call and a partials call is the lack of path:\n{{ template \u0026#34;theme/partials/header.html\u0026#34; . }} versus\n{{ partial \u0026#34;header.html\u0026#34; . }} Both pass in the context.\nLet\u0026rsquo;s change the home page template to use these new partials.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ vi themes/zafta/layouts/index.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;posts\u0026lt;/h1\u0026gt; {{ range first 10 .Data.Pages }} {{ if eq .Type \u0026#34;post\u0026#34;}} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;h1\u0026gt;pages\u0026lt;/h1\u0026gt; {{ range .Data.Pages }} {{ if or (eq .Type \u0026#34;page\u0026#34;) (eq .Type \u0026#34;about\u0026#34;) }} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Type }} - {{ .Title }} - {{ .RelPermalink }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Generate the web site and verify the results. The title on the home page is now \u0026ldquo;your title here\u0026rdquo;, which comes from the \u0026ldquo;title\u0026rdquo; variable in the config.toml file.\nUpdate the Default Single Template to Use the Partials 1 2 3 4 5 6 7 8 $ vi themes/zafta/layouts/_default/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Generate the web site and verify the results. The title on the posts and the about page should both reflect the value in the markdown file.\nAdd “Date Published” to Posts It\u0026rsquo;s common to have posts display the date that they were written or published, so let\u0026rsquo;s add that. The front matter of our posts has a variable named \u0026ldquo;date.\u0026rdquo; It\u0026rsquo;s usually the date the content was created, but let\u0026rsquo;s pretend that\u0026rsquo;s the value we want to display.\nAdd “Date Published” to the Template We\u0026rsquo;ll start by updating the template used to render the posts. The template code will look like:\n{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }} Posts use the default single template, so we\u0026rsquo;ll change that file.\n1 2 3 4 5 6 7 8 9 $ vi themes/zafta/layouts/_default/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }}\u0026lt;/h2\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Generate the web site and verify the results. The posts now have the date displayed in them. There\u0026rsquo;s a problem, though. The \u0026ldquo;about\u0026rdquo; page also has the date displayed.\nAs usual, there are a couple of ways to make the date display only on posts. We could do an \u0026ldquo;if\u0026rdquo; statement like we did on the home page. Another way would be to create a separate template for posts.\nThe \u0026ldquo;if\u0026rdquo; solution works for sites that have just a couple of content types. It aligns with the principle of \u0026ldquo;code for today,\u0026rdquo; too.\nLet\u0026rsquo;s assume, though, that we\u0026rsquo;ve made our site so complex that we feel we have to create a new template type. In Hugo-speak, we\u0026rsquo;re going to create a section template.\nLet\u0026rsquo;s restore the default single template before we forget.\n1 2 3 4 5 6 7 8 9 $ mkdir themes/zafta/layouts/post $ vi themes/zafta/layouts/_default/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Now we\u0026rsquo;ll update the post\u0026rsquo;s version of the single template. If you remember Hugo\u0026rsquo;s rules, the template engine will use this version over the default.\n1 2 3 4 5 6 7 8 9 $ vi themes/zafta/layouts/post/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }}\u0026lt;/h2\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Note that we removed the date logic from the default template and put it in the post template. Generate the web site and verify the results. Posts have dates and the about page doesn\u0026rsquo;t.\nDon\u0026rsquo;t Repeat Yourself DRY is a good design goal and Hugo does a great job supporting it. Part of the art of a good template is knowing when to add a new template and when to update an existing one. While you\u0026rsquo;re figuring that out, accept that you\u0026rsquo;ll be doing some refactoring. Hugo makes that easy and fast, so it\u0026rsquo;s okay to delay splitting up a template.\n","permalink":"https://manuelfedele.github.io/posts/create-template-hugo/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I\u0026rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won\u0026rsquo;t cover using CSS to style your theme.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll start with creating a new site with a very basic template. Then we\u0026rsquo;ll add in a few pages and posts. With small variations on that, you will be able to create many different types of web sites.\u003c/p\u003e","title":"Create a template for Hugo"},{"content":"For my last project, I used poetry to manage dependencies and package my code. Poetry comes with all the tools you might need to manage your projects in a deterministic way.\nAnyway, after running poetry init in an existing project, I tried to let poetry create a virtuale environment, by adding the first dependecy, but when I did poetry add twisted it suddenly hang out, printing:\nNo module named \u0026#39;virtualenv.seed.via_app_data\u0026#39; Why is that? Because poetry was using the wrong virtualenv instance, so to solve this:\nRemove all virtualenv istances sudo apt remove --purge python3-virtualenv virtualenv Remove unused dependencies sudo apt autoremove Install virtualenv pip3 install virtualenv Add dependencies with poetry poetry add \u0026lt;dependecy_name\u0026gt; Now it should work.\n","permalink":"https://manuelfedele.github.io/posts/poetry-no-module-seed-via-app-data/","summary":"\u003cp\u003eFor my last project, I used \u003cstrong\u003epoetry\u003c/strong\u003e to manage dependencies and package my code.\nPoetry comes with all the tools you might need to manage your projects in a deterministic way.\u003c/p\u003e\n\u003cp\u003eAnyway, after running poetry init in an existing project, I tried to let poetry create a virtuale environment, by adding the first dependecy, but when I did poetry add twisted it suddenly hang out, printing:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eNo module named \u0026#39;virtualenv.seed.via_app_data\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eWhy is that? Because \u003cstrong\u003epoetry\u003c/strong\u003e was using the wrong virtualenv instance, so to solve this:\u003c/p\u003e","title":"Poetry No Module Seed via App Data"},{"content":"TLDR; I just unpaired the devices, then started a YouTube video on my Ubuntu machine and then repeated the steps below (pairing) while the audio was playing. At that point I’ve been able to send audio to my Echo Plus.\nYesterday, I tried to use my Amazon Echo Plus as bluetooth speaker on my media center Running Ubuntu 20.04.\nI paired the Echo Plus using Alexa app on my phone following these steps:\nOpen Alexa app on your phone and go to devices tab, then Echo \u0026amp; Alexa. Choose your Echo Plus from list and then the gear icon on top left. Under Wireless tab, choose Bluetooth Devices and then Pair new device.` Now the Echo Plus is in Pairing Mode. On Ubuntu, discover devices with Bluetooth and then pair Echo-s5u. Following these steps, paired was successful but I couldn’t send any audio to Echo Plus.\nWhy? Because the Echo was recognized as microphone instead of a speaker.\nThe only workaround I found at the moment to avoid this, is to pair the Echo Plus while you’re playing some audio on your Ubuntu machine.\nWhen you pair an Echo Plus to a device, the device and the Echo Plus need to establish a connection using the A2DP (Advanced Audio Distribution Profile) protocol. This protocol is used to transmit high-quality audio over Bluetooth.\nWhen you play audio on the machine before pairing the Echo Plus, it initiates the A2DP connection between the two devices. This allows the Echo Plus to establish a connection with the machine as an audio sink, which enables it to receive and play audio.\nIt\u0026rsquo;s possible that the Echo Plus is not able to initiate the A2DP connection on its own, so playing audio on the machine before pairing helps establish the connection. Once the connection is established, the Echo Plus should be able to play audio.\nI just unpaired the devices, then started a YouTube video on my Ubuntu machine and then repeated the steps above. A that point I’ve been able to send audio to my Echo Plus.\n","permalink":"https://manuelfedele.github.io/posts/ubuntu-echo-plus-alexa-workaround-bluetooth-speaker/","summary":"\u003ch3 id=\"tldr\"\u003eTLDR;\u003c/h3\u003e\n\u003cp\u003eI just unpaired the devices, then started a YouTube video on my Ubuntu machine and then repeated the steps below (pairing) while the audio was playing. At that point I’ve been able to send audio to my Echo Plus.\u003c/p\u003e\n\u003cp\u003eYesterday, I tried to use my Amazon Echo Plus as bluetooth speaker on my media center Running Ubuntu 20.04.\u003c/p\u003e\n\u003cp\u003eI paired the Echo Plus using Alexa app on my phone following these steps:\u003c/p\u003e","title":"Amazon Echo Plus as bluetooth speaker on Ubuntu 20.04"}]